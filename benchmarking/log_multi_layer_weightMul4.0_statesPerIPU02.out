2022-09-12 17:10:06.334395: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.01
DENSE_SIZES:  [2312, 1466, 1466, 10]
SPARSE_SIZES:  [32, 14, 14, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  2
SECOND_THRESHOLD:  0.9

60000
9984
[24746 53578 39551 ... 47355 38660 33188]
2022-09-12 17:10:15.206291: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 17:10:16.068815: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 7, 2, 1
Build_allocator: 10, 3, 1
Build_allocator: 8, 2, 2
Build_allocator: 11, 3, 2
Build_allocator: 3, 1, 0
Build_allocator: 4, 1, 1
Build_allocator: 5, 1, 2
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 12, 4, 0
Build_allocator: 13, 4, 1
Build_allocator: 14, 4, 2
Build_allocator: 15, 5, 0
Build_allocator: 16, 5, 1
Build_allocator: 17, 5, 2
Build_allocator: 6, 2, 0
Build_allocator: 9, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0xa0f1170
1: &init_state[ilay]0xa0f1178
2: &init_state[ilay]0xa0f1180
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 734, 1467, }
end_tiles: {734, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{4, 4, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 5, 6, 8, 10, 11, 13, 15, 17, 18, 20, 22, 23, 25, 27, 28, 30, 32, 34, 35, 37, 39, 40, 42, 44, 46, 47, 49, 51, 52, 54, 56, 57, 59, 61, 63, 64, 66, 68, 69, 71, 73, 74, 76, 78, 80, 81, 83, 85, 86, 88, 90, 92, 93, 95, 97, 98, 100, 102, 103, 105, 107, 109, 110, 112, 114, 115, 117, 119, 120, 122, 124, 126, 127, 129, 131, 132, 134, 136, 138, 139, 141, 143, 144, 146, 148, 149, 151, 153, 155, 156, 158, 160, 161, 163, 165, 166, 168, 170, 172, 173, 175, 177, 178, 180, 182, 184, 185, 187, 189, 190, 192, 194, 195, 197, 199, 201, 202, 204, 206, 207, 209, 211, 212, 214, 216, 218, 219, 221, 223, 224, 226, 228, 230, 231, 233, 235, 236, 238, 240, 241, 243, 245, 247, 248, 250, 252, 253, 255, 257, 258, 260, 262, 264, 265, 267, 269, 270, 272, 274, 276, 277, 279, 281, 282, 284, 286, 287, 289, 291, 293, 294, 296, 298, 299, 301, 303, 304, 306, 308, 310, 311, 313, 315, 316, 318, 320, 322, 323, 325, 327, 328, 330, 332, 333, 335, 337, 339, 340, 342, 344, 345, 347, 349, 350, 352, 354, 356, 357, 359, 361, 362, 364, 366, 368, 369, 371, 373, 374, 376, 378, 379, 381, 383, 385, 386, 388, 390, 391, 393, 395, 396, 398, 400, 402, 403, 405, 407, 408, 410, 412, 414, 415, 417, 419, 420, 422, 424, 425, 427, 429, 431, 432, 434, 436, 437, 439, 441, 442, 444, 446, 448, 449, 451, 453, 454, 456, 458, 460, 461, 463, 465, 466, 468, 470, 471, 473, 475, 477, 478, 480, 482, 483, 485, 487, 488, 490, 492, 494, 495, 497, 499, 500, 502, 504, 506, 507, 509, 511, 512, 514, 516, 517, 519, 521, 523, 524, 526, 528, 529, 531, 533, 534, 536, 538, 540, 541, 543, 545, 546, 548, 550, 552, 553, 555, 557, 558, 560, 562, 563, 565, 567, 569, 570, 572, 574, 575, 577, 579, 580, 582, 584, 586, 587, 589, 591, 592, 594, 596, 598, 599, 601, 603, 604, 606, 608, 609, 611, 613, 615, 616, 618, 620, 621, 623, 625, 626, 628, 630, 632, 633, 635, 637, 638, 640, 642, 644, 645, 647, 649, 650, 652, }
{0, 13, 27, 40, 54, 68, 81, 95, 109, 122, 136, 149, 163, 177, 190, 204, 218, 231, 245, 258, 272, 286, 299, 313, 327, 340, 354, 368, 381, 395, 408, 422, 436, 449, 463, 477, 490, 504, 517, 531, 545, 558, 572, 586, 599, 613, 626, 640, }
{654, 655, 657, 659, 661, 662, 664, 666, 667, 669, 671, 672, 674, 676, 678, 679, 681, 683, 684, 686, 688, 690, 691, 693, 695, 696, 698, 700, 701, 703, 705, 707, 708, 710, 712, 713, 715, 717, 718, 720, 722, 724, 725, 727, 729, 730, 732, 734, 736, 737, 739, 741, 742, 744, 746, 747, 749, 751, 753, 754, 756, 758, 759, 761, 763, 764, 766, 768, 770, 771, 773, 775, 776, 778, 780, 782, 783, 785, 787, 788, 790, 792, 793, 795, 797, 799, 800, 802, 804, 805, 807, 809, 810, 812, 814, 816, 817, 819, 821, 822, 824, 826, 828, 829, 831, 833, 834, 836, 838, 839, 841, 843, 845, 846, 848, 850, 851, 853, 855, 856, 858, 860, 862, 863, 865, 867, 868, 870, 872, 874, 875, 877, 879, 880, 882, 884, 885, 887, 889, 891, 892, 894, 896, 897, 899, 901, 902, 904, 906, 908, 909, 911, 913, 914, 916, 918, 920, 921, 923, 925, 926, 928, 930, 931, 933, 935, 937, 938, 940, 942, 943, 945, 947, 948, 950, 952, 954, 955, 957, 959, 960, 962, 964, 966, 967, 969, 971, 972, 974, 976, 977, 979, 981, 983, 984, 986, 988, 989, 991, 993, 994, 996, 998, 1000, 1001, 1003, 1005, 1006, 1008, 1010, 1012, 1013, 1015, 1017, 1018, 1020, 1022, 1023, 1025, 1027, 1029, 1030, 1032, 1034, 1035, 1037, 1039, 1040, 1042, 1044, 1046, 1047, 1049, 1051, 1052, 1054, 1056, 1058, 1059, 1061, 1063, 1064, 1066, 1068, 1069, 1071, 1073, 1075, 1076, 1078, 1080, 1081, 1083, 1085, 1086, 1088, 1090, 1092, 1093, 1095, 1097, 1098, 1100, 1102, 1104, 1105, 1107, 1109, 1110, 1112, 1114, 1115, 1117, 1119, 1121, 1122, 1124, 1126, 1127, 1129, 1131, 1132, 1134, 1136, 1138, 1139, 1141, 1143, 1144, 1146, 1148, 1150, 1151, 1153, 1155, 1156, 1158, 1160, 1161, 1163, 1165, 1167, 1168, 1170, 1172, 1173, 1175, 1177, 1178, 1180, 1182, 1184, 1185, 1187, 1189, 1190, 1192, 1194, 1196, 1197, 1199, 1201, 1202, 1204, 1206, 1207, 1209, 1211, 1213, 1214, 1216, 1218, 1219, 1221, 1223, 1224, 1226, 1228, 1230, 1231, 1233, 1235, 1236, 1238, 1240, 1242, 1243, 1245, 1247, 1248, 1250, 1252, 1253, 1255, 1257, 1259, 1260, 1262, 1264, 1265, 1267, 1269, 1270, 1272, 1274, 1276, 1277, 1279, 1281, 1282, 1284, 1286, 1288, 1289, 1291, 1293, 1294, 1296, 1298, 1299, 1301, 1303, 1305, 1306, }
{654, 667, 681, 695, 708, 722, 736, 749, 763, 776, 790, 804, 817, 831, 845, 858, 872, 885, 899, 913, 926, 940, 954, 967, 981, 994, 1008, 1022, 1035, 1049, 1063, 1076, 1090, 1104, 1117, 1131, 1144, 1158, 1172, 1185, 1199, 1213, 1226, 1240, 1253, 1267, 1281, 1294, }
{1308, 1310, 1311, 1313, 1315, 1316, 1318, 1320, 1322, 1323, 1325, 1327, 1328, 1330, 1332, 1334, 1335, 1337, 1339, 1340, 1342, 1344, 1345, 1347, 1349, 1351, 1352, 1354, 1356, 1357, 1359, 1361, 1362, 1364, 1366, 1368, 1369, 1371, 1373, 1374, 1376, 1378, 1380, 1381, 1383, 1385, 1386, 1388, 1390, 1391, 1393, 1395, 1397, 1398, 1400, 1402, 1403, 1405, 1407, 1408, 1410, 1412, 1414, 1415, 1417, 1419, 1420, 1422, 1424, 1426, 1427, 1429, 1431, 1432, 1434, 1436, 1437, 1439, 1441, 1443, 1444, 1446, 1448, 1449, 1451, 1453, 1454, 1456, 1458, 1460, 1461, 1463, 1465, 1466, 1468, 1470, }
{1308, 1311, 1315, 1318, 1322, 1325, 1328, 1332, 1335, 1339, 1342, 1345, 1349, 1352, 1356, 1359, 1362, 1366, 1369, 1373, 1376, 1380, 1383, 1386, 1390, 1393, 1397, 1400, 1403, 1407, 1410, 1414, 1417, 1420, 1424, 1427, 1431, 1434, 1437, 1441, 1444, 1448, 1451, 1454, 1458, 1461, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 4
num_workers: 24
max_num_states_per_worker: 62
sparse_size_to_use: 14
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 4
num_workers: 24
max_num_states_per_worker: 62
sparse_size_to_use: 14
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,4,14}
 repeated_sparse_spike_ids.shapeToString(): {48,2,4,14}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,14}
slicedInpSpikeIds.shapeToString(): {48,14}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,14}
slicedInpSpikeIds.shapeToString(): {48,14}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{1444, 26, }
start_tile: 1
num_tiles_per_layer[ilay]: 1444
start_tile + num_tiles_per_layer[ilay]: 1445
num_occupied_tiles: 733
batchsize: 48
sparse_size: 14
dLdx.shapeToString(): {733,48,14}
replicated_numGrads.shapeToString(): {733,48}
exchangeGroupSize: 24
numExchangeGroups: 30
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
start_id: 456, end_id: 480
start_id: 480, end_id: 504
start_id: 504, end_id: 528
start_id: 528, end_id: 552
start_id: 552, end_id: 576
start_id: 576, end_id: 600
start_id: 600, end_id: 624
start_id: 624, end_id: 648
start_id: 648, end_id: 672
start_id: 672, end_id: 696
lastExchangeGroupLarger
start_id: 696, end_id: 733
exchangeGroupSizeLast: 37
exchangeBatchSizeLast: 2
start_tile: 1445
num_tiles_per_layer[ilay]: 26
start_tile + num_tiles_per_layer[ilay]: 1471
num_occupied_tiles: 5
batchsize: 48
sparse_size: 14
dLdx.shapeToString(): {5,48,14}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 17:25:51.725930: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[1466 1466   10]
0 733.0
1 733.0
2 5.0
[TileMapping(start_tile=1, end_tile=734), TileMapping(start_tile=734, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=734), TileMapping(start_tile=734, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.2558990251065398
limit=0.2558990251065398
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 1466) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 1466) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_14_14_2_48_1_734_1467_734_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 14), (100 5562034     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][2
                                                                 keras_multi_lif_layer_sparse[0][5
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 5,562,034
Trainable params: 5,553,208
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_14_14_2_48_1_734_1467_734_1467_1472
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_14_14_2_48_1_734_1467_734_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.187852/52 [==============================] - 938s 18s/step - loss: 2.1878
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.099952/52 [==============================] - 0s 7ms/step - loss: 2.0999
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 2.058752/52 [==============================] - 0s 6ms/step - loss: 2.0587
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 1.981652/52 [==============================] - 0s 6ms/step - loss: 1.9816
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 1.677152/52 [==============================] - 0s 6ms/step - loss: 1.6771
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 1.576252/52 [==============================] - 0s 6ms/step - loss: 1.5762
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 1.550452/52 [==============================] - 0s 6ms/step - loss: 1.5504
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 1.541552/52 [==============================] - 0s 6ms/step - loss: 1.5415
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 1.486052/52 [==============================] - 0s 6ms/step - loss: 1.4860
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 1.499052/52 [==============================] - 0s 6ms/step - loss: 1.4990
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 1.512952/52 [==============================] - 0s 6ms/step - loss: 1.5129
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 1.508452/52 [==============================] - 0s 6ms/step - loss: 1.5084
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 1.462352/52 [==============================] - 0s 6ms/step - loss: 1.4623
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 1.475352/52 [==============================] - 0s 6ms/step - loss: 1.4753
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 1.501552/52 [==============================] - 0s 6ms/step - loss: 1.5015
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 1.491952/52 [==============================] - 0s 6ms/step - loss: 1.4919
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 1.456252/52 [==============================] - 0s 7ms/step - loss: 1.4562
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 1.476852/52 [==============================] - 0s 6ms/step - loss: 1.4768
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 1.500752/52 [==============================] - 0s 6ms/step - loss: 1.5007
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 1.498352/52 [==============================] - 0s 6ms/step - loss: 1.4983
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 1.462052/52 [==============================] - 1s 17ms/step - loss: 1.4620
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 1.455152/52 [==============================] - 1s 14ms/step - loss: 1.4551
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 1.435152/52 [==============================] - 0s 6ms/step - loss: 1.4351
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 1.380252/52 [==============================] - 0s 6ms/step - loss: 1.3802
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 1.349452/52 [==============================] - 0s 6ms/step - loss: 1.3494

Final time:  947.0303273200989
2022-09-12 17:26:04.194909: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.01
DENSE_SIZES:  [2312, 978, 978, 976, 10]
SPARSE_SIZES:  [32, 8, 8, 8, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  3
SECOND_THRESHOLD:  0.9

60000
9984
[12640  5548  2342 ... 12599 40233 58553]
2022-09-12 17:26:13.028662: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 17:26:13.927549: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 9, 2, 1
Build_allocator: 13, 3, 1
Build_allocator: 10, 2, 2
Build_allocator: 14, 3, 2
Build_allocator: 11, 2, 3
Build_allocator: 15, 3, 3
Build_allocator: 4, 1, 0
Build_allocator: 5, 1, 1
Build_allocator: 6, 1, 2
Build_allocator: 7, 1, 3
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 16, 4, 0
Build_allocator: 17, 4, 1
Build_allocator: 18, 4, 2
Build_allocator: 19, 4, 3
Build_allocator: 20, 5, 0
Build_allocator: 21, 5, 1
Build_allocator: 22, 5, 2
Build_allocator: 23, 5, 3
Build_allocator: 8, 2, 0
Build_allocator: 12, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0x23e65f90
1: &init_state[ilay]0x23e65f98
2: &init_state[ilay]0x23e65fa0
3: &init_state[ilay]0x23e65fa8
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 490, 979, 1467, }
end_tiles: {490, 979, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{3, 3, 3, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 4, 6, 7, 9, 10, 12, 13, 15, 16, 18, 19, 21, 23, 24, 26, 27, 29, 30, 32, 33, 35, 36, 38, 39, 41, 42, 44, 46, 47, 49, 50, 52, 53, 55, 56, 58, 59, 61, 62, 64, 65, 67, 69, 70, 72, 73, 75, 76, 78, 79, 81, 82, 84, 85, 87, 88, 90, 92, 93, 95, 96, 98, 99, 101, 102, 104, 105, 107, 108, 110, 111, 113, 115, 116, 118, 119, 121, 122, 124, 125, 127, 128, 130, 131, 133, 134, 136, 138, 139, 141, 142, 144, 145, 147, 148, 150, 151, 153, 154, 156, 157, 159, 161, 162, 164, 165, 167, 168, 170, 171, 173, 174, 176, 177, 179, 180, 182, 184, 185, 187, 188, 190, 191, 193, 194, 196, 197, 199, 200, 202, 203, 205, 207, 208, 210, 211, 213, 214, 216, 217, 219, 220, 222, 223, 225, 226, 228, 230, 231, 233, 234, 236, 237, 239, 240, 242, 243, 245, 246, 248, 249, 251, 253, 254, 256, 257, 259, 260, 262, 263, 265, 266, 268, 269, 271, 272, 274, 276, 277, 279, 280, 282, 283, 285, 286, 288, 289, 291, 292, 294, 295, 297, 299, 300, 302, 303, 305, 306, 308, 309, 311, 312, 314, 315, 317, 318, 320, 322, 323, 325, 326, 328, 329, 331, 332, 334, 335, 337, 338, 340, 341, 343, 345, 346, 348, 349, 351, 352, 354, 355, 357, 358, 360, 361, 363, 364, 366, 368, 369, 371, 372, 374, 375, 377, 378, 380, 381, 383, 384, 386, 387, 389, 391, 392, 394, 395, 397, 398, 400, 401, 403, 404, 406, 407, 409, 410, 412, 414, 415, 417, 418, 420, 421, 423, 424, 426, 427, 429, 430, 432, 433, 435, 437, 438, 440, }
{0, 9, 18, 27, 36, 46, 55, 64, 73, 82, 92, 101, 110, 119, 128, 138, 147, 156, 165, 174, 184, 193, 202, 211, 220, 230, 239, 248, 257, 266, 276, 285, 294, 303, 312, 322, 331, 340, 349, 358, 368, 377, 386, 395, 404, 414, 423, 432, }
{441, 443, 444, 446, 447, 449, 450, 452, 453, 455, 456, 458, 460, 461, 463, 464, 466, 467, 469, 470, 472, 473, 475, 476, 478, 479, 481, 483, 484, 486, 487, 489, 490, 492, 493, 495, 496, 498, 499, 501, 502, 504, 506, 507, 509, 510, 512, 513, 515, 516, 518, 519, 521, 522, 524, 525, 527, 529, 530, 532, 533, 535, 536, 538, 539, 541, 542, 544, 545, 547, 548, 550, 552, 553, 555, 556, 558, 559, 561, 562, 564, 565, 567, 568, 570, 571, 573, 575, 576, 578, 579, 581, 582, 584, 585, 587, 588, 590, 591, 593, 594, 596, 598, 599, 601, 602, 604, 605, 607, 608, 610, 611, 613, 614, 616, 617, 619, 621, 622, 624, 625, 627, 628, 630, 631, 633, 634, 636, 637, 639, 640, 642, 644, 645, 647, 648, 650, 651, 653, 654, 656, 657, 659, 660, 662, 663, 665, 667, 668, 670, 671, 673, 674, 676, 677, 679, 680, 682, 683, 685, 686, 688, 690, 691, 693, 694, 696, 697, 699, 700, 702, 703, 705, 706, 708, 709, 711, 713, 714, 716, 717, 719, 720, 722, 723, 725, 726, 728, 729, 731, 732, 734, 736, 737, 739, 740, 742, 743, 745, 746, 748, 749, 751, 752, 754, 755, 757, 759, 760, 762, 763, 765, 766, 768, 769, 771, 772, 774, 775, 777, 778, 780, 782, 783, 785, 786, 788, 789, 791, 792, 794, 795, 797, 798, 800, 801, 803, 805, 806, 808, 809, 811, 812, 814, 815, 817, 818, 820, 821, 823, 824, 826, 828, 829, 831, 832, 834, 835, 837, 838, 840, 841, 843, 844, 846, 847, 849, 851, 852, 854, 855, 857, 858, 860, 861, 863, 864, 866, 867, 869, 870, 872, 874, 875, 877, 878, 880, 881, }
{441, 450, 460, 469, 478, 487, 496, 506, 515, 524, 533, 542, 552, 561, 570, 579, 588, 598, 607, 616, 625, 634, 644, 653, 662, 671, 680, 690, 699, 708, 717, 726, 736, 745, 754, 763, 772, 782, 791, 800, 809, 818, 828, 837, 846, 855, 864, 874, }
{883, 884, 886, 887, 889, 890, 892, 893, 895, 897, 898, 900, 901, 903, 904, 906, 907, 909, 910, 912, 913, 915, 916, 918, 920, 921, 923, 924, 926, 927, 929, 930, 932, 933, 935, 936, 938, 939, 941, 943, 944, 946, 947, 949, 950, 952, 953, 955, 956, 958, 959, 961, 962, 964, 966, 967, 969, 970, 972, 973, 975, 976, 978, 979, 981, 982, 984, 985, 987, 989, 990, 992, 993, 995, 996, 998, 999, 1001, 1002, 1004, 1005, 1007, 1008, 1010, 1012, 1013, 1015, 1016, 1018, 1019, 1021, 1022, 1024, 1025, 1027, 1028, 1030, 1031, 1033, 1035, 1036, 1038, 1039, 1041, 1042, 1044, 1045, 1047, 1048, 1050, 1051, 1053, 1054, 1056, 1058, 1059, 1061, 1062, 1064, 1065, 1067, 1068, 1070, 1071, 1073, 1074, 1076, 1077, 1079, 1081, 1082, 1084, 1085, 1087, 1088, 1090, 1091, 1093, 1094, 1096, 1097, 1099, 1100, 1102, 1104, 1105, 1107, 1108, 1110, 1111, 1113, 1114, 1116, 1117, 1119, 1120, 1122, 1123, 1125, 1127, 1128, 1130, 1131, 1133, 1134, 1136, 1137, 1139, 1140, 1142, 1143, 1145, 1146, 1148, 1150, 1151, 1153, 1154, 1156, 1157, 1159, 1160, 1162, 1163, 1165, 1166, 1168, 1169, 1171, 1173, 1174, 1176, 1177, 1179, 1180, 1182, 1183, 1185, 1186, 1188, 1189, 1191, 1192, 1194, 1196, 1197, 1199, 1200, 1202, 1203, 1205, 1206, 1208, 1209, 1211, 1212, 1214, 1215, 1217, 1219, 1220, 1222, 1223, 1225, 1226, 1228, 1229, 1231, 1232, 1234, 1235, 1237, 1238, 1240, 1242, 1243, 1245, 1246, 1248, 1249, 1251, 1252, 1254, 1255, 1257, 1258, 1260, 1261, 1263, 1265, 1266, 1268, 1269, 1271, 1272, 1274, 1275, 1277, 1278, 1280, 1281, 1283, 1284, 1286, 1288, 1289, 1291, 1292, 1294, 1295, 1297, 1298, 1300, 1301, 1303, 1304, 1306, 1307, 1309, 1311, 1312, 1314, 1315, 1317, 1318, 1320, 1321, 1323, }
{883, 892, 901, 910, 920, 929, 938, 947, 956, 966, 975, 984, 993, 1002, 1012, 1021, 1030, 1039, 1048, 1058, 1067, 1076, 1085, 1094, 1104, 1113, 1122, 1131, 1140, 1150, 1159, 1168, 1177, 1186, 1196, 1205, 1214, 1223, 1232, 1242, 1251, 1260, 1269, 1278, 1288, 1297, 1306, 1315, }
{1324, 1326, 1327, 1329, 1330, 1332, 1334, 1335, 1337, 1338, 1340, 1341, 1343, 1344, 1346, 1347, 1349, 1350, 1352, 1353, 1355, 1357, 1358, 1360, 1361, 1363, 1364, 1366, 1367, 1369, 1370, 1372, 1373, 1375, 1376, 1378, 1380, 1381, 1383, 1384, 1386, 1387, 1389, 1390, 1392, 1393, 1395, 1396, 1398, 1399, 1401, 1403, 1404, 1406, 1407, 1409, 1410, 1412, 1413, 1415, 1416, 1418, 1419, 1421, 1422, 1424, 1426, 1427, 1429, 1430, 1432, 1433, 1435, 1436, 1438, 1439, 1441, 1442, 1444, 1445, 1447, 1449, 1450, 1452, 1453, 1455, 1456, 1458, 1459, 1461, 1462, 1464, 1465, 1467, 1468, 1470, }
{1324, 1327, 1330, 1334, 1337, 1340, 1343, 1346, 1349, 1352, 1355, 1358, 1361, 1364, 1367, 1370, 1373, 1376, 1380, 1383, 1386, 1389, 1392, 1395, 1398, 1401, 1404, 1407, 1410, 1413, 1416, 1419, 1422, 1426, 1429, 1432, 1435, 1438, 1441, 1444, 1447, 1450, 1453, 1456, 1459, 1462, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 8
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 8
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 8
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,8}
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,8}
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,8}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,8}
slicedInpSpikeIds.shapeToString(): {48,8}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,8}
slicedInpSpikeIds.shapeToString(): {48,8}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,8}
slicedInpSpikeIds.shapeToString(): {48,8}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {488,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{726, 725, 18, }
start_tile: 1
num_tiles_per_layer[ilay]: 726
start_tile + num_tiles_per_layer[ilay]: 727
num_occupied_tiles: 489
batchsize: 48
sparse_size: 8
dLdx.shapeToString(): {489,48,8}
replicated_numGrads.shapeToString(): {489,48}
exchangeGroupSize: 24
numExchangeGroups: 20
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
lastExchangeGroupLarger
start_id: 456, end_id: 489
exchangeGroupSizeLast: 33
exchangeBatchSizeLast: 2
start_tile: 727
num_tiles_per_layer[ilay]: 725
start_tile + num_tiles_per_layer[ilay]: 1452
num_occupied_tiles: 488
batchsize: 48
sparse_size: 8
dLdx.shapeToString(): {488,48,8}
replicated_numGrads.shapeToString(): {488,48}
exchangeGroupSize: 24
numExchangeGroups: 20
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
lastExchangeGroupLarger
start_id: 456, end_id: 488
exchangeGroupSizeLast: 32
exchangeBatchSizeLast: 2
start_tile: 1452
num_tiles_per_layer[ilay]: 18
start_tile + num_tiles_per_layer[ilay]: 1470
num_occupied_tiles: 5
batchsize: 48
sparse_size: 8
dLdx.shapeToString(): {5,48,8}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {488,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 17:38:15.245090: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[978 978 976  10]
0 489.0
1 489.0
2 488.0
3 5.0
[TileMapping(start_tile=1, end_tile=490), TileMapping(start_tile=490, end_tile=979), TileMapping(start_tile=979, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=490), TileMapping(start_tile=490, end_tile=979), TileMapping(start_tile=979, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.31330417999518295
limit=0.31330417999518295
limit=0.31362502409359
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 978) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 978) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 976) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_8_8_8_2_48_1_490_979_1467_490_979_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 8), (100, 4190734     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][3
                                                                 keras_multi_lif_layer_sparse[0][7
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 4,190,734
Trainable params: 4,181,908
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_8_8_8_2_48_1_490_979_1467_490_979_1467_1472
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_8_8_8_2_48_1_490_979_1467_490_979_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.091952/52 [==============================] - 723s 14s/step - loss: 2.0919
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 1.640152/52 [==============================] - 0s 6ms/step - loss: 1.6401
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 1.292752/52 [==============================] - 0s 5ms/step - loss: 1.2927
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 1.192652/52 [==============================] - 0s 5ms/step - loss: 1.1926
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 1.092052/52 [==============================] - 0s 5ms/step - loss: 1.0920
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 0.975352/52 [==============================] - 0s 5ms/step - loss: 0.9753
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 0.863352/52 [==============================] - 0s 5ms/step - loss: 0.8633
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 0.819652/52 [==============================] - 0s 5ms/step - loss: 0.8196
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 0.723152/52 [==============================] - 0s 5ms/step - loss: 0.7231
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 0.677852/52 [==============================] - 0s 5ms/step - loss: 0.6778
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 0.608752/52 [==============================] - 0s 5ms/step - loss: 0.6087
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 0.662552/52 [==============================] - 0s 6ms/step - loss: 0.6625
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 0.576252/52 [==============================] - 1s 16ms/step - loss: 0.5762
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 0.576952/52 [==============================] - 0s 8ms/step - loss: 0.5769
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 0.558852/52 [==============================] - 0s 5ms/step - loss: 0.5588
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 0.605352/52 [==============================] - 0s 5ms/step - loss: 0.6053
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 0.534452/52 [==============================] - 0s 5ms/step - loss: 0.5344
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 0.546852/52 [==============================] - 0s 5ms/step - loss: 0.5468
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 0.512952/52 [==============================] - 0s 5ms/step - loss: 0.5129
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 0.589252/52 [==============================] - 0s 5ms/step - loss: 0.5892
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 0.504552/52 [==============================] - 0s 5ms/step - loss: 0.5045
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 0.531152/52 [==============================] - 0s 5ms/step - loss: 0.5311
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 0.516552/52 [==============================] - 0s 5ms/step - loss: 0.5165
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 0.602152/52 [==============================] - 0s 5ms/step - loss: 0.6021
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 0.526452/52 [==============================] - 0s 5ms/step - loss: 0.5264

Final time:  730.5918292999268
2022-09-12 17:38:25.375344: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.01
DENSE_SIZES:  [2312, 734, 734, 732, 732, 10]
SPARSE_SIZES:  [32, 6, 6, 6, 6, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  4
SECOND_THRESHOLD:  0.9

60000
9984
[ 9518 38183  5355 ...   776  7244 29087]
2022-09-12 17:38:33.840875: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 17:38:34.706783: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 11, 2, 1
Build_allocator: 16, 3, 1
Build_allocator: 12, 2, 2
Build_allocator: 17, 3, 2
Build_allocator: 13, 2, 3
Build_allocator: 18, 3, 3
Build_allocator: 14, 2, 4
Build_allocator: 19, 3, 4
Build_allocator: 5, 1, 0
Build_allocator: 6, 1, 1
Build_allocator: 7, 1, 2
Build_allocator: 8, 1, 3
Build_allocator: 9, 1, 4
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 4, 0, 4
Build_allocator: 20, 4, 0
Build_allocator: 21, 4, 1
Build_allocator: 22, 4, 2
Build_allocator: 23, 4, 3
Build_allocator: 24, 4, 4
Build_allocator: 25, 5, 0
Build_allocator: 26, 5, 1
Build_allocator: 27, 5, 2
Build_allocator: 28, 5, 3
Build_allocator: 29, 5, 4
Build_allocator: 10, 2, 0
Build_allocator: 15, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0x1c1430d0
1: &init_state[ilay]0x1c1430d8
2: &init_state[ilay]0x1c1430e0
3: &init_state[ilay]0x1c1430e8
4: &init_state[ilay]0x1c1430f0
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0
3: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
3: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 368, 735, 1101, 1467, }
end_tiles: {368, 735, 1101, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{2, 2, 2, 2, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 5, 6, 8, 10, 11, 13, 15, 17, 18, 20, 22, 23, 25, 27, 28, 30, 32, 34, 35, 37, 39, 40, 42, 44, 46, 47, 49, 51, 52, 54, 56, 57, 59, 61, 63, 64, 66, 68, 69, 71, 73, 74, 76, 78, 80, 81, 83, 85, 86, 88, 90, 92, 93, 95, 97, 98, 100, 102, 103, 105, 107, 109, 110, 112, 114, 115, 117, 119, 120, 122, 124, 126, 127, 129, 131, 132, 134, 136, 138, 139, 141, 143, 144, 146, 148, 149, 151, 153, 155, 156, 158, 160, 161, 163, 165, 166, 168, 170, 172, 173, 175, 177, 178, 180, 182, 184, 185, 187, 189, 190, 192, 194, 195, 197, 199, 201, 202, 204, 206, 207, 209, 211, 212, 214, 216, 218, 219, 221, 223, 224, 226, 228, 230, 231, 233, 235, 236, 238, 240, 241, 243, 245, 247, 248, 250, 252, 253, 255, 257, 258, 260, 262, 264, 265, 267, 269, 270, 272, 274, 276, 277, 279, 281, 282, 284, 286, 287, 289, 291, 293, 294, 296, 298, 299, 301, 303, 304, 306, 308, 310, 311, 313, 315, 316, 318, 320, 322, 323, 325, }
{0, 6, 13, 20, 27, 34, 40, 47, 54, 61, 68, 74, 81, 88, 95, 102, 109, 115, 122, 129, 136, 143, 149, 156, 163, 170, 177, 184, 190, 197, 204, 211, 218, 224, 231, 238, 245, 252, 258, 265, 272, 279, 286, 293, 299, 306, 313, 320, }
{327, 328, 330, 332, 333, 335, 337, 339, 340, 342, 344, 345, 347, 349, 350, 352, 354, 356, 357, 359, 361, 362, 364, 366, 368, 369, 371, 373, 374, 376, 378, 379, 381, 383, 385, 386, 388, 390, 391, 393, 395, 396, 398, 400, 402, 403, 405, 407, 408, 410, 412, 414, 415, 417, 419, 420, 422, 424, 425, 427, 429, 431, 432, 434, 436, 437, 439, 441, 442, 444, 446, 448, 449, 451, 453, 454, 456, 458, 460, 461, 463, 465, 466, 468, 470, 471, 473, 475, 477, 478, 480, 482, 483, 485, 487, 488, 490, 492, 494, 495, 497, 499, 500, 502, 504, 506, 507, 509, 511, 512, 514, 516, 517, 519, 521, 523, 524, 526, 528, 529, 531, 533, 534, 536, 538, 540, 541, 543, 545, 546, 548, 550, 552, 553, 555, 557, 558, 560, 562, 563, 565, 567, 569, 570, 572, 574, 575, 577, 579, 580, 582, 584, 586, 587, 589, 591, 592, 594, 596, 598, 599, 601, 603, 604, 606, 608, 609, 611, 613, 615, 616, 618, 620, 621, 623, 625, 626, 628, 630, 632, 633, 635, 637, 638, 640, 642, 644, 645, 647, 649, 650, 652, }
{327, 333, 340, 347, 354, 361, 368, 374, 381, 388, 395, 402, 408, 415, 422, 429, 436, 442, 449, 456, 463, 470, 477, 483, 490, 497, 504, 511, 517, 524, 531, 538, 545, 552, 558, 565, 572, 579, 586, 592, 599, 606, 613, 620, 626, 633, 640, 647, }
{654, 655, 657, 659, 661, 662, 664, 666, 667, 669, 671, 672, 674, 676, 678, 679, 681, 683, 684, 686, 688, 690, 691, 693, 695, 696, 698, 700, 701, 703, 705, 707, 708, 710, 712, 713, 715, 717, 718, 720, 722, 724, 725, 727, 729, 730, 732, 734, 736, 737, 739, 741, 742, 744, 746, 747, 749, 751, 753, 754, 756, 758, 759, 761, 763, 764, 766, 768, 770, 771, 773, 775, 776, 778, 780, 782, 783, 785, 787, 788, 790, 792, 793, 795, 797, 799, 800, 802, 804, 805, 807, 809, 810, 812, 814, 816, 817, 819, 821, 822, 824, 826, 828, 829, 831, 833, 834, 836, 838, 839, 841, 843, 845, 846, 848, 850, 851, 853, 855, 856, 858, 860, 862, 863, 865, 867, 868, 870, 872, 874, 875, 877, 879, 880, 882, 884, 885, 887, 889, 891, 892, 894, 896, 897, 899, 901, 902, 904, 906, 908, 909, 911, 913, 914, 916, 918, 920, 921, 923, 925, 926, 928, 930, 931, 933, 935, 937, 938, 940, 942, 943, 945, 947, 948, 950, 952, 954, 955, 957, 959, 960, 962, 964, 966, 967, 969, 971, 972, 974, 976, 977, 979, }
{654, 661, 667, 674, 681, 688, 695, 701, 708, 715, 722, 729, 736, 742, 749, 756, 763, 770, 776, 783, 790, 797, 804, 810, 817, 824, 831, 838, 845, 851, 858, 865, 872, 879, 885, 892, 899, 906, 913, 920, 926, 933, 940, 947, 954, 960, 967, 974, }
{981, 983, 984, 986, 988, 989, 991, 993, 994, 996, 998, 1000, 1001, 1003, 1005, 1006, 1008, 1010, 1012, 1013, 1015, 1017, 1018, 1020, 1022, 1023, 1025, 1027, 1029, 1030, 1032, 1034, 1035, 1037, 1039, 1040, 1042, 1044, 1046, 1047, 1049, 1051, 1052, 1054, 1056, 1058, 1059, 1061, 1063, 1064, 1066, 1068, 1069, 1071, 1073, 1075, 1076, 1078, 1080, 1081, 1083, 1085, 1086, 1088, 1090, 1092, 1093, 1095, 1097, 1098, 1100, 1102, 1104, 1105, 1107, 1109, 1110, 1112, 1114, 1115, 1117, 1119, 1121, 1122, 1124, 1126, 1127, 1129, 1131, 1132, 1134, 1136, 1138, 1139, 1141, 1143, 1144, 1146, 1148, 1150, 1151, 1153, 1155, 1156, 1158, 1160, 1161, 1163, 1165, 1167, 1168, 1170, 1172, 1173, 1175, 1177, 1178, 1180, 1182, 1184, 1185, 1187, 1189, 1190, 1192, 1194, 1196, 1197, 1199, 1201, 1202, 1204, 1206, 1207, 1209, 1211, 1213, 1214, 1216, 1218, 1219, 1221, 1223, 1224, 1226, 1228, 1230, 1231, 1233, 1235, 1236, 1238, 1240, 1242, 1243, 1245, 1247, 1248, 1250, 1252, 1253, 1255, 1257, 1259, 1260, 1262, 1264, 1265, 1267, 1269, 1270, 1272, 1274, 1276, 1277, 1279, 1281, 1282, 1284, 1286, 1288, 1289, 1291, 1293, 1294, 1296, 1298, 1299, 1301, 1303, 1305, 1306, }
{981, 988, 994, 1001, 1008, 1015, 1022, 1029, 1035, 1042, 1049, 1056, 1063, 1069, 1076, 1083, 1090, 1097, 1104, 1110, 1117, 1124, 1131, 1138, 1144, 1151, 1158, 1165, 1172, 1178, 1185, 1192, 1199, 1206, 1213, 1219, 1226, 1233, 1240, 1247, 1253, 1260, 1267, 1274, 1281, 1288, 1294, 1301, }
{1308, 1310, 1311, 1313, 1315, 1316, 1318, 1320, 1322, 1323, 1325, 1327, 1328, 1330, 1332, 1334, 1335, 1337, 1339, 1340, 1342, 1344, 1345, 1347, 1349, 1351, 1352, 1354, 1356, 1357, 1359, 1361, 1362, 1364, 1366, 1368, 1369, 1371, 1373, 1374, 1376, 1378, 1380, 1381, 1383, 1385, 1386, 1388, 1390, 1391, 1393, 1395, 1397, 1398, 1400, 1402, 1403, 1405, 1407, 1408, 1410, 1412, 1414, 1415, 1417, 1419, 1420, 1422, 1424, 1426, 1427, 1429, 1431, 1432, 1434, 1436, 1437, 1439, 1441, 1443, 1444, 1446, 1448, 1449, 1451, 1453, 1454, 1456, 1458, 1460, 1461, 1463, 1465, 1466, 1468, 1470, }
{1308, 1311, 1315, 1318, 1322, 1325, 1328, 1332, 1335, 1339, 1342, 1345, 1349, 1352, 1356, 1359, 1362, 1366, 1369, 1373, 1376, 1380, 1383, 1386, 1390, 1393, 1397, 1400, 1403, 1407, 1410, 1414, 1417, 1420, 1424, 1427, 1431, 1434, 1437, 1441, 1444, 1448, 1451, 1454, 1458, 1461, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 62
sparse_size_to_use: 6
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 62
sparse_size_to_use: 6
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 61
sparse_size_to_use: 6
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 61
sparse_size_to_use: 6
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,6}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,6}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,6}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,6}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 4
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,6}
slicedInpSpikeIds.shapeToString(): {48,6}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,6}
slicedInpSpikeIds.shapeToString(): {48,6}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,6}
slicedInpSpikeIds.shapeToString(): {48,6}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

3
out_spike_ids[ilay].shapeToString(): {100,48,6}
slicedInpSpikeIds.shapeToString(): {48,6}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,2}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{485, 484, 484, 15, }
start_tile: 1
num_tiles_per_layer[ilay]: 485
start_tile + num_tiles_per_layer[ilay]: 486
num_occupied_tiles: 367
batchsize: 48
sparse_size: 6
dLdx.shapeToString(): {367,48,6}
replicated_numGrads.shapeToString(): {367,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 367
exchangeGroupSizeLast: 31
exchangeBatchSizeLast: 2
start_tile: 486
num_tiles_per_layer[ilay]: 484
start_tile + num_tiles_per_layer[ilay]: 970
num_occupied_tiles: 366
batchsize: 48
sparse_size: 6
dLdx.shapeToString(): {366,48,6}
replicated_numGrads.shapeToString(): {366,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 366
exchangeGroupSizeLast: 30
exchangeBatchSizeLast: 2
start_tile: 970
num_tiles_per_layer[ilay]: 484
start_tile + num_tiles_per_layer[ilay]: 1454
num_occupied_tiles: 366
batchsize: 48
sparse_size: 6
dLdx.shapeToString(): {366,48,6}
replicated_numGrads.shapeToString(): {366,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 366
exchangeGroupSizeLast: 30
exchangeBatchSizeLast: 2
start_tile: 1454
num_tiles_per_layer[ilay]: 15
start_tile + num_tiles_per_layer[ilay]: 1469
num_occupied_tiles: 5
batchsize: 48
sparse_size: 6
dLdx.shapeToString(): {5,48,6}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,1}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 17:48:40.056133: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[734 734 732 732  10]
0 367.0
1 367.0
2 366.0
3 366.0
4 5.0
[TileMapping(start_tile=1, end_tile=368), TileMapping(start_tile=368, end_tile=735), TileMapping(start_tile=735, end_tile=1101), TileMapping(start_tile=1101, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=368), TileMapping(start_tile=368, end_tile=735), TileMapping(start_tile=735, end_tile=1101), TileMapping(start_tile=1101, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.3616492648073473
limit=0.3616492648073473
limit=0.3621429841700741
limit=0.3621429841700741
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 734) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 734) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 732) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 732) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_5:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_6_6_6_6_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 6), (100, 3325022     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][4
                                                                 keras_multi_lif_layer_sparse[0][9
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 3,325,022
Trainable params: 3,316,196
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_6_6_6_6_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_6_6_6_6_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.306452/52 [==============================] - 607s 12s/step - loss: 2.3064
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 1.964052/52 [==============================] - 0s 5ms/step - loss: 1.9640
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 1.938352/52 [==============================] - 0s 5ms/step - loss: 1.9383
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 1.916052/52 [==============================] - 0s 5ms/step - loss: 1.9160
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 1.942252/52 [==============================] - 0s 5ms/step - loss: 1.9422
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 1.918752/52 [==============================] - 0s 5ms/step - loss: 1.9187
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 1.931652/52 [==============================] - 0s 5ms/step - loss: 1.9316
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 1.948252/52 [==============================] - 0s 5ms/step - loss: 1.9482
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 1.951552/52 [==============================] - 0s 5ms/step - loss: 1.9515
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 1.928552/52 [==============================] - 0s 5ms/step - loss: 1.9285
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 1.938652/52 [==============================] - 0s 5ms/step - loss: 1.9386
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 1.944552/52 [==============================] - 0s 5ms/step - loss: 1.9445
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 1.940752/52 [==============================] - 0s 5ms/step - loss: 1.9407
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 1.910352/52 [==============================] - 0s 5ms/step - loss: 1.9103
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 1.933252/52 [==============================] - 0s 5ms/step - loss: 1.9332
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 1.932452/52 [==============================] - 0s 5ms/step - loss: 1.9324
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 1.937352/52 [==============================] - 0s 5ms/step - loss: 1.9373
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 1.909552/52 [==============================] - 0s 5ms/step - loss: 1.9095
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 1.923952/52 [==============================] - 0s 5ms/step - loss: 1.9239
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 1.927852/52 [==============================] - 0s 5ms/step - loss: 1.9278
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 1.936152/52 [==============================] - 0s 5ms/step - loss: 1.9361
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 1.905552/52 [==============================] - 0s 5ms/step - loss: 1.9055
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 1.917852/52 [==============================] - 0s 5ms/step - loss: 1.9178
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 1.925452/52 [==============================] - 0s 5ms/step - loss: 1.9254
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 1.923852/52 [==============================] - 0s 5ms/step - loss: 1.9238

Final time:  613.4743690490723
2022-09-12 17:48:49.024016: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.01
DENSE_SIZES:  [2312, 588, 586, 586, 586, 586, 10]
SPARSE_SIZES:  [32, 4, 4, 4, 4, 4, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  5
SECOND_THRESHOLD:  0.9

60000
9984
[50525 14711 20328 ... 13610 34239  4788]
2022-09-12 17:48:57.474891: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 17:48:58.342596: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 13, 2, 1
Build_allocator: 19, 3, 1
Build_allocator: 14, 2, 2
Build_allocator: 20, 3, 2
Build_allocator: 15, 2, 3
Build_allocator: 21, 3, 3
Build_allocator: 16, 2, 4
Build_allocator: 22, 3, 4
Build_allocator: 17, 2, 5
Build_allocator: 23, 3, 5
Build_allocator: 6, 1, 0
Build_allocator: 7, 1, 1
Build_allocator: 8, 1, 2
Build_allocator: 9, 1, 3
Build_allocator: 10, 1, 4
Build_allocator: 11, 1, 5
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 4, 0, 4
Build_allocator: 5, 0, 5
Build_allocator: 24, 4, 0
Build_allocator: 25, 4, 1
Build_allocator: 26, 4, 2
Build_allocator: 27, 4, 3
Build_allocator: 28, 4, 4
Build_allocator: 29, 4, 5
Build_allocator: 30, 5, 0
Build_allocator: 31, 5, 1
Build_allocator: 32, 5, 2
Build_allocator: 33, 5, 3
Build_allocator: 34, 5, 4
Build_allocator: 35, 5, 5
Build_allocator: 12, 2, 0
Build_allocator: 18, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0x1ca748d0
1: &init_state[ilay]0x1ca748d8
2: &init_state[ilay]0x1ca748e0
3: &init_state[ilay]0x1ca748e8
4: &init_state[ilay]0x1ca748f0
5: &init_state[ilay]0x1ca748f8
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0
3: init_state[ilay]==init_state[ilay+1]: 0
4: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
3: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
4: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 295, 588, 881, 1174, 1467, }
end_tiles: {295, 588, 881, 1174, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{2, 2, 2, 2, 2, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 2, 4, 5, 6, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 25, 26, 27, 29, 30, 32, 33, 34, 36, 37, 39, 40, 41, 43, 44, 46, 47, 48, 50, 51, 52, 54, 55, 57, 58, 59, 61, 62, 64, 65, 66, 68, 69, 71, 72, 73, 75, 76, 78, 79, 80, 82, 83, 85, 86, 87, 89, 90, 92, 93, 94, 96, 97, 98, 100, 101, 103, 104, 105, 107, 108, 110, 111, 112, 114, 115, 117, 118, 119, 121, 122, 124, 125, 126, 128, 129, 131, 132, 133, 135, 136, 138, 139, 140, 142, 143, 144, 146, 147, 149, 150, 151, 153, 154, 156, 157, 158, 160, 161, 163, 164, 165, 167, 168, 170, 171, 172, 174, 175, 177, 178, 179, 181, 182, 184, 185, 186, 188, 189, 190, 192, 193, 195, 196, 197, 199, 200, 202, 203, 204, 206, 207, 209, 210, 211, 213, 214, 216, 217, 218, 220, 221, 223, 224, 225, 227, 228, 230, 231, 232, 234, 235, 236, 238, 239, 241, 242, 243, 245, 246, 248, 249, 250, 252, 253, 255, 256, 257, 259, 260, 262, 263, 264, 266, }
{0, 5, 11, 16, 22, 27, 33, 39, 44, 50, 55, 61, 66, 72, 78, 83, 89, 94, 100, 105, 111, 117, 122, 128, 133, 139, 144, 150, 156, 161, 167, 172, 178, 184, 189, 195, 200, 206, 211, 217, 223, 228, 234, 239, 245, 250, 256, 262, }
{267, 269, 270, 271, 273, 274, 276, 277, 278, 280, 281, 282, 284, 285, 287, 288, 289, 291, 292, 294, 295, 296, 298, 299, 301, 302, 303, 305, 306, 308, 309, 310, 312, 313, 315, 316, 317, 319, 320, 322, 323, 324, 326, 327, 328, 330, 331, 333, 334, 335, 337, 338, 340, 341, 342, 344, 345, 347, 348, 349, 351, 352, 354, 355, 356, 358, 359, 361, 362, 363, 365, 366, 368, 369, 370, 372, 373, 374, 376, 377, 379, 380, 381, 383, 384, 386, 387, 388, 390, 391, 393, 394, 395, 397, 398, 400, 401, 402, 404, 405, 407, 408, 409, 411, 412, 414, 415, 416, 418, 419, 420, 422, 423, 425, 426, 427, 429, 430, 432, 433, 434, 436, 437, 439, 440, 441, 443, 444, 446, 447, 448, 450, 451, 453, 454, 455, 457, 458, 460, 461, 462, 464, 465, 466, 468, 469, 471, 472, 473, 475, 476, 478, 479, 480, 482, 483, 485, 486, 487, 489, 490, 492, 493, 494, 496, 497, 499, 500, 501, 503, 504, 506, 507, 508, 510, 511, 512, 514, 515, 517, 518, 519, 521, 522, 524, 525, 526, 528, 529, 531, 532, 533, }
{267, 273, 278, 284, 289, 295, 301, 306, 312, 317, 323, 328, 334, 340, 345, 351, 356, 362, 368, 373, 379, 384, 390, 395, 401, 407, 412, 418, 423, 429, 434, 440, 446, 451, 457, 462, 468, 473, 479, 485, 490, 496, 501, 507, 512, 518, 524, 529, }
{535, 536, 538, 539, 540, 542, 543, 545, 546, 547, 549, 550, 552, 553, 554, 556, 557, 558, 560, 561, 563, 564, 565, 567, 568, 570, 571, 572, 574, 575, 577, 578, 579, 581, 582, 584, 585, 586, 588, 589, 591, 592, 593, 595, 596, 598, 599, 600, 602, 603, 604, 606, 607, 609, 610, 611, 613, 614, 616, 617, 618, 620, 621, 623, 624, 625, 627, 628, 630, 631, 632, 634, 635, 637, 638, 639, 641, 642, 644, 645, 646, 648, 649, 650, 652, 653, 655, 656, 657, 659, 660, 662, 663, 664, 666, 667, 669, 670, 671, 673, 674, 676, 677, 678, 680, 681, 683, 684, 685, 687, 688, 690, 691, 692, 694, 695, 696, 698, 699, 701, 702, 703, 705, 706, 708, 709, 710, 712, 713, 715, 716, 717, 719, 720, 722, 723, 724, 726, 727, 729, 730, 731, 733, 734, 736, 737, 738, 740, 741, 742, 744, 745, 747, 748, 749, 751, 752, 754, 755, 756, 758, 759, 761, 762, 763, 765, 766, 768, 769, 770, 772, 773, 775, 776, 777, 779, 780, 782, 783, 784, 786, 787, 788, 790, 791, 793, 794, 795, 797, 798, 800, 801, }
{535, 540, 546, 552, 557, 563, 568, 574, 579, 585, 591, 596, 602, 607, 613, 618, 624, 630, 635, 641, 646, 652, 657, 663, 669, 674, 680, 685, 691, 696, 702, 708, 713, 719, 724, 730, 736, 741, 747, 752, 758, 763, 769, 775, 780, 786, 791, 797, }
{802, 804, 805, 807, 808, 809, 811, 812, 814, 815, 816, 818, 819, 821, 822, 823, 825, 826, 828, 829, 830, 832, 833, 834, 836, 837, 839, 840, 841, 843, 844, 846, 847, 848, 850, 851, 853, 854, 855, 857, 858, 860, 861, 862, 864, 865, 867, 868, 869, 871, 872, 874, 875, 876, 878, 879, 880, 882, 883, 885, 886, 887, 889, 890, 892, 893, 894, 896, 897, 899, 900, 901, 903, 904, 906, 907, 908, 910, 911, 913, 914, 915, 917, 918, 920, 921, 922, 924, 925, 926, 928, 929, 931, 932, 933, 935, 936, 938, 939, 940, 942, 943, 945, 946, 947, 949, 950, 952, 953, 954, 956, 957, 959, 960, 961, 963, 964, 966, 967, 968, 970, 971, 972, 974, 975, 977, 978, 979, 981, 982, 984, 985, 986, 988, 989, 991, 992, 993, 995, 996, 998, 999, 1000, 1002, 1003, 1005, 1006, 1007, 1009, 1010, 1012, 1013, 1014, 1016, 1017, 1018, 1020, 1021, 1023, 1024, 1025, 1027, 1028, 1030, 1031, 1032, 1034, 1035, 1037, 1038, 1039, 1041, 1042, 1044, 1045, 1046, 1048, 1049, 1051, 1052, 1053, 1055, 1056, 1058, 1059, 1060, 1062, 1063, 1064, 1066, 1067, 1069, }
{802, 808, 814, 819, 825, 830, 836, 841, 847, 853, 858, 864, 869, 875, 880, 886, 892, 897, 903, 908, 914, 920, 925, 931, 936, 942, 947, 953, 959, 964, 970, 975, 981, 986, 992, 998, 1003, 1009, 1014, 1020, 1025, 1031, 1037, 1042, 1048, 1053, 1059, 1064, }
{1070, 1071, 1073, 1074, 1076, 1077, 1078, 1080, 1081, 1083, 1084, 1085, 1087, 1088, 1090, 1091, 1092, 1094, 1095, 1097, 1098, 1099, 1101, 1102, 1104, 1105, 1106, 1108, 1109, 1110, 1112, 1113, 1115, 1116, 1117, 1119, 1120, 1122, 1123, 1124, 1126, 1127, 1129, 1130, 1131, 1133, 1134, 1136, 1137, 1138, 1140, 1141, 1143, 1144, 1145, 1147, 1148, 1150, 1151, 1152, 1154, 1155, 1156, 1158, 1159, 1161, 1162, 1163, 1165, 1166, 1168, 1169, 1170, 1172, 1173, 1175, 1176, 1177, 1179, 1180, 1182, 1183, 1184, 1186, 1187, 1189, 1190, 1191, 1193, 1194, 1196, 1197, 1198, 1200, 1201, 1202, 1204, 1205, 1207, 1208, 1209, 1211, 1212, 1214, 1215, 1216, 1218, 1219, 1221, 1222, 1223, 1225, 1226, 1228, 1229, 1230, 1232, 1233, 1235, 1236, 1237, 1239, 1240, 1242, 1243, 1244, 1246, 1247, 1248, 1250, 1251, 1253, 1254, 1255, 1257, 1258, 1260, 1261, 1262, 1264, 1265, 1267, 1268, 1269, 1271, 1272, 1274, 1275, 1276, 1278, 1279, 1281, 1282, 1283, 1285, 1286, 1288, 1289, 1290, 1292, 1293, 1294, 1296, 1297, 1299, 1300, 1301, 1303, 1304, 1306, 1307, 1308, 1310, 1311, 1313, 1314, 1315, 1317, 1318, 1320, 1321, 1322, 1324, 1325, 1327, 1328, 1329, 1331, 1332, 1334, 1335, 1336, }
{1070, 1076, 1081, 1087, 1092, 1098, 1104, 1109, 1115, 1120, 1126, 1131, 1137, 1143, 1148, 1154, 1159, 1165, 1170, 1176, 1182, 1187, 1193, 1198, 1204, 1209, 1215, 1221, 1226, 1232, 1237, 1243, 1248, 1254, 1260, 1265, 1271, 1276, 1282, 1288, 1293, 1299, 1304, 1310, 1315, 1321, 1327, 1332, }
{1338, 1339, 1340, 1342, 1343, 1345, 1346, 1347, 1349, 1350, 1352, 1353, 1354, 1356, 1357, 1359, 1360, 1361, 1363, 1364, 1366, 1367, 1368, 1370, 1371, 1373, 1374, 1375, 1377, 1378, 1380, 1381, 1382, 1384, 1385, 1386, 1388, 1389, 1391, 1392, 1393, 1395, 1396, 1398, 1399, 1400, 1402, 1403, 1405, 1406, 1407, 1409, 1410, 1412, 1413, 1414, 1416, 1417, 1419, 1420, 1421, 1423, 1424, 1426, 1427, 1428, 1430, 1431, 1432, 1434, 1435, 1437, 1438, 1439, 1441, 1442, 1444, 1445, 1446, 1448, 1449, 1451, 1452, 1453, 1455, 1456, 1458, 1459, 1460, 1462, 1463, 1465, 1466, 1467, 1469, 1470, }
{1338, 1340, 1343, 1346, 1349, 1352, 1354, 1357, 1360, 1363, 1366, 1368, 1371, 1374, 1377, 1380, 1382, 1385, 1388, 1391, 1393, 1396, 1399, 1402, 1405, 1407, 1410, 1413, 1416, 1419, 1421, 1424, 1427, 1430, 1432, 1435, 1438, 1441, 1444, 1446, 1449, 1452, 1455, 1458, 1460, 1463, 1466, 1469, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 4
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 4
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 4
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 4
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 4
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,4}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,4}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,4}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,4}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,4}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 4
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 5
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,4}
slicedInpSpikeIds.shapeToString(): {48,4}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,4}
slicedInpSpikeIds.shapeToString(): {48,4}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,4}
slicedInpSpikeIds.shapeToString(): {48,4}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

3
out_spike_ids[ilay].shapeToString(): {100,48,4}
slicedInpSpikeIds.shapeToString(): {48,4}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

4
out_spike_ids[ilay].shapeToString(): {100,48,4}
slicedInpSpikeIds.shapeToString(): {48,4}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {294,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 5
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{364, 364, 364, 364, 14, }
start_tile: 1
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 365
num_occupied_tiles: 293
batchsize: 48
sparse_size: 4
dLdx.shapeToString(): {293,48,4}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 365
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 729
num_occupied_tiles: 293
batchsize: 48
sparse_size: 4
dLdx.shapeToString(): {293,48,4}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 729
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 1093
num_occupied_tiles: 293
batchsize: 48
sparse_size: 4
dLdx.shapeToString(): {293,48,4}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 1093
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 1457
num_occupied_tiles: 293
batchsize: 48
sparse_size: 4
dLdx.shapeToString(): {293,48,4}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 1457
num_tiles_per_layer[ilay]: 14
start_tile + num_tiles_per_layer[ilay]: 1471
num_occupied_tiles: 5
batchsize: 48
sparse_size: 4
dLdx.shapeToString(): {5,48,4}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {294,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 5
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 17:56:59.597819: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[588 586 586 586 586  10]
0 294.0
1 293.0
2 293.0
3 293.0
4 293.0
5 5.0
[TileMapping(start_tile=1, end_tile=295), TileMapping(start_tile=295, end_tile=588), TileMapping(start_tile=588, end_tile=881), TileMapping(start_tile=881, end_tile=1174), TileMapping(start_tile=1174, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=295), TileMapping(start_tile=295, end_tile=588), TileMapping(start_tile=588, end_tile=881), TileMapping(start_tile=881, end_tile=1174), TileMapping(start_tile=1174, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.40406101782088427
limit=0.4047499544129309
limit=0.4047499544129309
limit=0.4047499544129309
limit=0.4047499544129309
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 588) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 586) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 586) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 586) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_5:0' shape=(2, 586) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_6:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_4_4_4_4_4_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 4), (100, 2748898     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][5
                                                                 keras_multi_lif_layer_sparse[0][1
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 2,748,898
Trainable params: 2,740,072
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_4_4_4_4_4_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_4_4_4_4_4_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.375052/52 [==============================] - 483s 9s/step - loss: 2.3750
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.278752/52 [==============================] - 0s 5ms/step - loss: 2.2787
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 2.163252/52 [==============================] - 0s 4ms/step - loss: 2.1632
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 2.123152/52 [==============================] - 0s 4ms/step - loss: 2.1231
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 2.128252/52 [==============================] - 0s 4ms/step - loss: 2.1282
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 2.136152/52 [==============================] - 0s 4ms/step - loss: 2.1361
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 2.143752/52 [==============================] - 0s 4ms/step - loss: 2.1437
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 2.151852/52 [==============================] - 0s 4ms/step - loss: 2.1518
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 2.171952/52 [==============================] - 0s 4ms/step - loss: 2.1719
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 2.166952/52 [==============================] - 0s 4ms/step - loss: 2.1669
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 2.139152/52 [==============================] - 0s 4ms/step - loss: 2.1391
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 2.126852/52 [==============================] - 0s 4ms/step - loss: 2.1268
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 1.973152/52 [==============================] - 0s 4ms/step - loss: 1.9731
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 1.947552/52 [==============================] - 0s 4ms/step - loss: 1.9475
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 1.941752/52 [==============================] - 0s 4ms/step - loss: 1.9417
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 1.924952/52 [==============================] - 0s 4ms/step - loss: 1.9249
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 1.959952/52 [==============================] - 0s 4ms/step - loss: 1.9599
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 1.942652/52 [==============================] - 0s 4ms/step - loss: 1.9426
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 1.918552/52 [==============================] - 0s 4ms/step - loss: 1.9185
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 1.922852/52 [==============================] - 0s 4ms/step - loss: 1.9228
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 1.947952/52 [==============================] - 0s 4ms/step - loss: 1.9479
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 1.934652/52 [==============================] - 0s 4ms/step - loss: 1.9346
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 1.917052/52 [==============================] - 0s 5ms/step - loss: 1.9170
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 1.912352/52 [==============================] - 0s 5ms/step - loss: 1.9123
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 1.941152/52 [==============================] - 0s 4ms/step - loss: 1.9411

Final time:  488.6642019748688
2022-09-12 17:57:07.917766: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.01
DENSE_SIZES:  [2312, 1466, 1466, 10]
SPARSE_SIZES:  [32, 14, 14, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  2
SECOND_THRESHOLD:  0.95

60000
9984
[33878 19381 16363 ... 15476 47601 47560]
2022-09-12 17:57:18.223558: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 17:57:19.053567: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 7, 2, 1
Build_allocator: 10, 3, 1
Build_allocator: 8, 2, 2
Build_allocator: 11, 3, 2
Build_allocator: 3, 1, 0
Build_allocator: 4, 1, 1
Build_allocator: 5, 1, 2
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 12, 4, 0
Build_allocator: 13, 4, 1
Build_allocator: 14, 4, 2
Build_allocator: 15, 5, 0
Build_allocator: 16, 5, 1
Build_allocator: 17, 5, 2
Build_allocator: 6, 2, 0
Build_allocator: 9, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0x6df7000
1: &init_state[ilay]0x6df7008
2: &init_state[ilay]0x6df7010
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 734, 1467, }
end_tiles: {734, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{4, 4, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 5, 6, 8, 10, 11, 13, 15, 17, 18, 20, 22, 23, 25, 27, 28, 30, 32, 34, 35, 37, 39, 40, 42, 44, 46, 47, 49, 51, 52, 54, 56, 57, 59, 61, 63, 64, 66, 68, 69, 71, 73, 74, 76, 78, 80, 81, 83, 85, 86, 88, 90, 92, 93, 95, 97, 98, 100, 102, 103, 105, 107, 109, 110, 112, 114, 115, 117, 119, 120, 122, 124, 126, 127, 129, 131, 132, 134, 136, 138, 139, 141, 143, 144, 146, 148, 149, 151, 153, 155, 156, 158, 160, 161, 163, 165, 166, 168, 170, 172, 173, 175, 177, 178, 180, 182, 184, 185, 187, 189, 190, 192, 194, 195, 197, 199, 201, 202, 204, 206, 207, 209, 211, 212, 214, 216, 218, 219, 221, 223, 224, 226, 228, 230, 231, 233, 235, 236, 238, 240, 241, 243, 245, 247, 248, 250, 252, 253, 255, 257, 258, 260, 262, 264, 265, 267, 269, 270, 272, 274, 276, 277, 279, 281, 282, 284, 286, 287, 289, 291, 293, 294, 296, 298, 299, 301, 303, 304, 306, 308, 310, 311, 313, 315, 316, 318, 320, 322, 323, 325, 327, 328, 330, 332, 333, 335, 337, 339, 340, 342, 344, 345, 347, 349, 350, 352, 354, 356, 357, 359, 361, 362, 364, 366, 368, 369, 371, 373, 374, 376, 378, 379, 381, 383, 385, 386, 388, 390, 391, 393, 395, 396, 398, 400, 402, 403, 405, 407, 408, 410, 412, 414, 415, 417, 419, 420, 422, 424, 425, 427, 429, 431, 432, 434, 436, 437, 439, 441, 442, 444, 446, 448, 449, 451, 453, 454, 456, 458, 460, 461, 463, 465, 466, 468, 470, 471, 473, 475, 477, 478, 480, 482, 483, 485, 487, 488, 490, 492, 494, 495, 497, 499, 500, 502, 504, 506, 507, 509, 511, 512, 514, 516, 517, 519, 521, 523, 524, 526, 528, 529, 531, 533, 534, 536, 538, 540, 541, 543, 545, 546, 548, 550, 552, 553, 555, 557, 558, 560, 562, 563, 565, 567, 569, 570, 572, 574, 575, 577, 579, 580, 582, 584, 586, 587, 589, 591, 592, 594, 596, 598, 599, 601, 603, 604, 606, 608, 609, 611, 613, 615, 616, 618, 620, 621, 623, 625, 626, 628, 630, 632, 633, 635, 637, 638, 640, 642, 644, 645, 647, 649, 650, 652, }
{0, 13, 27, 40, 54, 68, 81, 95, 109, 122, 136, 149, 163, 177, 190, 204, 218, 231, 245, 258, 272, 286, 299, 313, 327, 340, 354, 368, 381, 395, 408, 422, 436, 449, 463, 477, 490, 504, 517, 531, 545, 558, 572, 586, 599, 613, 626, 640, }
{654, 655, 657, 659, 661, 662, 664, 666, 667, 669, 671, 672, 674, 676, 678, 679, 681, 683, 684, 686, 688, 690, 691, 693, 695, 696, 698, 700, 701, 703, 705, 707, 708, 710, 712, 713, 715, 717, 718, 720, 722, 724, 725, 727, 729, 730, 732, 734, 736, 737, 739, 741, 742, 744, 746, 747, 749, 751, 753, 754, 756, 758, 759, 761, 763, 764, 766, 768, 770, 771, 773, 775, 776, 778, 780, 782, 783, 785, 787, 788, 790, 792, 793, 795, 797, 799, 800, 802, 804, 805, 807, 809, 810, 812, 814, 816, 817, 819, 821, 822, 824, 826, 828, 829, 831, 833, 834, 836, 838, 839, 841, 843, 845, 846, 848, 850, 851, 853, 855, 856, 858, 860, 862, 863, 865, 867, 868, 870, 872, 874, 875, 877, 879, 880, 882, 884, 885, 887, 889, 891, 892, 894, 896, 897, 899, 901, 902, 904, 906, 908, 909, 911, 913, 914, 916, 918, 920, 921, 923, 925, 926, 928, 930, 931, 933, 935, 937, 938, 940, 942, 943, 945, 947, 948, 950, 952, 954, 955, 957, 959, 960, 962, 964, 966, 967, 969, 971, 972, 974, 976, 977, 979, 981, 983, 984, 986, 988, 989, 991, 993, 994, 996, 998, 1000, 1001, 1003, 1005, 1006, 1008, 1010, 1012, 1013, 1015, 1017, 1018, 1020, 1022, 1023, 1025, 1027, 1029, 1030, 1032, 1034, 1035, 1037, 1039, 1040, 1042, 1044, 1046, 1047, 1049, 1051, 1052, 1054, 1056, 1058, 1059, 1061, 1063, 1064, 1066, 1068, 1069, 1071, 1073, 1075, 1076, 1078, 1080, 1081, 1083, 1085, 1086, 1088, 1090, 1092, 1093, 1095, 1097, 1098, 1100, 1102, 1104, 1105, 1107, 1109, 1110, 1112, 1114, 1115, 1117, 1119, 1121, 1122, 1124, 1126, 1127, 1129, 1131, 1132, 1134, 1136, 1138, 1139, 1141, 1143, 1144, 1146, 1148, 1150, 1151, 1153, 1155, 1156, 1158, 1160, 1161, 1163, 1165, 1167, 1168, 1170, 1172, 1173, 1175, 1177, 1178, 1180, 1182, 1184, 1185, 1187, 1189, 1190, 1192, 1194, 1196, 1197, 1199, 1201, 1202, 1204, 1206, 1207, 1209, 1211, 1213, 1214, 1216, 1218, 1219, 1221, 1223, 1224, 1226, 1228, 1230, 1231, 1233, 1235, 1236, 1238, 1240, 1242, 1243, 1245, 1247, 1248, 1250, 1252, 1253, 1255, 1257, 1259, 1260, 1262, 1264, 1265, 1267, 1269, 1270, 1272, 1274, 1276, 1277, 1279, 1281, 1282, 1284, 1286, 1288, 1289, 1291, 1293, 1294, 1296, 1298, 1299, 1301, 1303, 1305, 1306, }
{654, 667, 681, 695, 708, 722, 736, 749, 763, 776, 790, 804, 817, 831, 845, 858, 872, 885, 899, 913, 926, 940, 954, 967, 981, 994, 1008, 1022, 1035, 1049, 1063, 1076, 1090, 1104, 1117, 1131, 1144, 1158, 1172, 1185, 1199, 1213, 1226, 1240, 1253, 1267, 1281, 1294, }
{1308, 1310, 1311, 1313, 1315, 1316, 1318, 1320, 1322, 1323, 1325, 1327, 1328, 1330, 1332, 1334, 1335, 1337, 1339, 1340, 1342, 1344, 1345, 1347, 1349, 1351, 1352, 1354, 1356, 1357, 1359, 1361, 1362, 1364, 1366, 1368, 1369, 1371, 1373, 1374, 1376, 1378, 1380, 1381, 1383, 1385, 1386, 1388, 1390, 1391, 1393, 1395, 1397, 1398, 1400, 1402, 1403, 1405, 1407, 1408, 1410, 1412, 1414, 1415, 1417, 1419, 1420, 1422, 1424, 1426, 1427, 1429, 1431, 1432, 1434, 1436, 1437, 1439, 1441, 1443, 1444, 1446, 1448, 1449, 1451, 1453, 1454, 1456, 1458, 1460, 1461, 1463, 1465, 1466, 1468, 1470, }
{1308, 1311, 1315, 1318, 1322, 1325, 1328, 1332, 1335, 1339, 1342, 1345, 1349, 1352, 1356, 1359, 1362, 1366, 1369, 1373, 1376, 1380, 1383, 1386, 1390, 1393, 1397, 1400, 1403, 1407, 1410, 1414, 1417, 1420, 1424, 1427, 1431, 1434, 1437, 1441, 1444, 1448, 1451, 1454, 1458, 1461, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 4
num_workers: 24
max_num_states_per_worker: 62
sparse_size_to_use: 14
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 4
num_workers: 24
max_num_states_per_worker: 62
sparse_size_to_use: 14
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,4,14}
 repeated_sparse_spike_ids.shapeToString(): {48,2,4,14}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,14}
slicedInpSpikeIds.shapeToString(): {48,14}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,14}
slicedInpSpikeIds.shapeToString(): {48,14}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{1444, 26, }
start_tile: 1
num_tiles_per_layer[ilay]: 1444
start_tile + num_tiles_per_layer[ilay]: 1445
num_occupied_tiles: 733
batchsize: 48
sparse_size: 14
dLdx.shapeToString(): {733,48,14}
replicated_numGrads.shapeToString(): {733,48}
exchangeGroupSize: 24
numExchangeGroups: 30
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
start_id: 456, end_id: 480
start_id: 480, end_id: 504
start_id: 504, end_id: 528
start_id: 528, end_id: 552
start_id: 552, end_id: 576
start_id: 576, end_id: 600
start_id: 600, end_id: 624
start_id: 624, end_id: 648
start_id: 648, end_id: 672
start_id: 672, end_id: 696
lastExchangeGroupLarger
start_id: 696, end_id: 733
exchangeGroupSizeLast: 37
exchangeBatchSizeLast: 2
start_tile: 1445
num_tiles_per_layer[ilay]: 26
start_tile + num_tiles_per_layer[ilay]: 1471
num_occupied_tiles: 5
batchsize: 48
sparse_size: 14
dLdx.shapeToString(): {5,48,14}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 18:12:52.991534: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[1466 1466   10]
0 733.0
1 733.0
2 5.0
[TileMapping(start_tile=1, end_tile=734), TileMapping(start_tile=734, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=734), TileMapping(start_tile=734, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.2558990251065398
limit=0.2558990251065398
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 1466) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 1466) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_14_14_2_48_1_734_1467_734_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 14), (100 5562034     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][2
                                                                 keras_multi_lif_layer_sparse[0][5
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 5,562,034
Trainable params: 5,553,208
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_14_14_2_48_1_734_1467_734_1467_1472
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_14_14_2_48_1_734_1467_734_1467_1472
52/52 [==============================] - ETA: 0s - loss: 1.728252/52 [==============================] - 936s 18s/step - loss: 1.7282
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 1.042352/52 [==============================] - 0s 6ms/step - loss: 1.0423
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 0.859752/52 [==============================] - 0s 6ms/step - loss: 0.8597
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 0.720352/52 [==============================] - 0s 6ms/step - loss: 0.7203
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 0.592352/52 [==============================] - 0s 6ms/step - loss: 0.5923
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 0.467052/52 [==============================] - 0s 6ms/step - loss: 0.4670
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 0.408252/52 [==============================] - 0s 6ms/step - loss: 0.4082
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 0.366652/52 [==============================] - 0s 6ms/step - loss: 0.3666
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 0.315852/52 [==============================] - 0s 6ms/step - loss: 0.3158
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 0.288452/52 [==============================] - 0s 6ms/step - loss: 0.2884
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 0.267852/52 [==============================] - 0s 6ms/step - loss: 0.2678
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 0.260352/52 [==============================] - 0s 6ms/step - loss: 0.2603
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 0.230852/52 [==============================] - 0s 6ms/step - loss: 0.2308
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 0.222452/52 [==============================] - 0s 6ms/step - loss: 0.2224
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 0.200252/52 [==============================] - 0s 6ms/step - loss: 0.2002
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 0.224952/52 [==============================] - 0s 6ms/step - loss: 0.2249
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 0.194752/52 [==============================] - 0s 6ms/step - loss: 0.1947
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 0.192052/52 [==============================] - 0s 6ms/step - loss: 0.1920
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 0.167552/52 [==============================] - 0s 6ms/step - loss: 0.1675
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 0.192052/52 [==============================] - 0s 7ms/step - loss: 0.1920
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 0.166352/52 [==============================] - 0s 6ms/step - loss: 0.1663
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 0.158252/52 [==============================] - 0s 6ms/step - loss: 0.1582
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 0.134252/52 [==============================] - 0s 6ms/step - loss: 0.1342
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 0.146652/52 [==============================] - 0s 6ms/step - loss: 0.1466
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 0.132152/52 [==============================] - 0s 6ms/step - loss: 0.1321

Final time:  944.2063438892365
2022-09-12 18:13:04.256504: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.01
DENSE_SIZES:  [2312, 978, 978, 976, 10]
SPARSE_SIZES:  [32, 8, 8, 8, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  3
SECOND_THRESHOLD:  0.95

60000
9984
[37331 34528 43110 ...  5742 55261 38470]
2022-09-12 18:13:11.847097: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 18:13:12.615831: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 9, 2, 1
Build_allocator: 13, 3, 1
Build_allocator: 10, 2, 2
Build_allocator: 14, 3, 2
Build_allocator: 11, 2, 3
Build_allocator: 15, 3, 3
Build_allocator: 4, 1, 0
Build_allocator: 5, 1, 1
Build_allocator: 6, 1, 2
Build_allocator: 7, 1, 3
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 16, 4, 0
Build_allocator: 17, 4, 1
Build_allocator: 18, 4, 2
Build_allocator: 19, 4, 3
Build_allocator: 20, 5, 0
Build_allocator: 21, 5, 1
Build_allocator: 22, 5, 2
Build_allocator: 23, 5, 3
Build_allocator: 8, 2, 0
Build_allocator: 12, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0x24690fd0
1: &init_state[ilay]0x24690fd8
2: &init_state[ilay]0x24690fe0
3: &init_state[ilay]0x24690fe8
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 490, 979, 1467, }
end_tiles: {490, 979, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{3, 3, 3, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 4, 6, 7, 9, 10, 12, 13, 15, 16, 18, 19, 21, 23, 24, 26, 27, 29, 30, 32, 33, 35, 36, 38, 39, 41, 42, 44, 46, 47, 49, 50, 52, 53, 55, 56, 58, 59, 61, 62, 64, 65, 67, 69, 70, 72, 73, 75, 76, 78, 79, 81, 82, 84, 85, 87, 88, 90, 92, 93, 95, 96, 98, 99, 101, 102, 104, 105, 107, 108, 110, 111, 113, 115, 116, 118, 119, 121, 122, 124, 125, 127, 128, 130, 131, 133, 134, 136, 138, 139, 141, 142, 144, 145, 147, 148, 150, 151, 153, 154, 156, 157, 159, 161, 162, 164, 165, 167, 168, 170, 171, 173, 174, 176, 177, 179, 180, 182, 184, 185, 187, 188, 190, 191, 193, 194, 196, 197, 199, 200, 202, 203, 205, 207, 208, 210, 211, 213, 214, 216, 217, 219, 220, 222, 223, 225, 226, 228, 230, 231, 233, 234, 236, 237, 239, 240, 242, 243, 245, 246, 248, 249, 251, 253, 254, 256, 257, 259, 260, 262, 263, 265, 266, 268, 269, 271, 272, 274, 276, 277, 279, 280, 282, 283, 285, 286, 288, 289, 291, 292, 294, 295, 297, 299, 300, 302, 303, 305, 306, 308, 309, 311, 312, 314, 315, 317, 318, 320, 322, 323, 325, 326, 328, 329, 331, 332, 334, 335, 337, 338, 340, 341, 343, 345, 346, 348, 349, 351, 352, 354, 355, 357, 358, 360, 361, 363, 364, 366, 368, 369, 371, 372, 374, 375, 377, 378, 380, 381, 383, 384, 386, 387, 389, 391, 392, 394, 395, 397, 398, 400, 401, 403, 404, 406, 407, 409, 410, 412, 414, 415, 417, 418, 420, 421, 423, 424, 426, 427, 429, 430, 432, 433, 435, 437, 438, 440, }
{0, 9, 18, 27, 36, 46, 55, 64, 73, 82, 92, 101, 110, 119, 128, 138, 147, 156, 165, 174, 184, 193, 202, 211, 220, 230, 239, 248, 257, 266, 276, 285, 294, 303, 312, 322, 331, 340, 349, 358, 368, 377, 386, 395, 404, 414, 423, 432, }
{441, 443, 444, 446, 447, 449, 450, 452, 453, 455, 456, 458, 460, 461, 463, 464, 466, 467, 469, 470, 472, 473, 475, 476, 478, 479, 481, 483, 484, 486, 487, 489, 490, 492, 493, 495, 496, 498, 499, 501, 502, 504, 506, 507, 509, 510, 512, 513, 515, 516, 518, 519, 521, 522, 524, 525, 527, 529, 530, 532, 533, 535, 536, 538, 539, 541, 542, 544, 545, 547, 548, 550, 552, 553, 555, 556, 558, 559, 561, 562, 564, 565, 567, 568, 570, 571, 573, 575, 576, 578, 579, 581, 582, 584, 585, 587, 588, 590, 591, 593, 594, 596, 598, 599, 601, 602, 604, 605, 607, 608, 610, 611, 613, 614, 616, 617, 619, 621, 622, 624, 625, 627, 628, 630, 631, 633, 634, 636, 637, 639, 640, 642, 644, 645, 647, 648, 650, 651, 653, 654, 656, 657, 659, 660, 662, 663, 665, 667, 668, 670, 671, 673, 674, 676, 677, 679, 680, 682, 683, 685, 686, 688, 690, 691, 693, 694, 696, 697, 699, 700, 702, 703, 705, 706, 708, 709, 711, 713, 714, 716, 717, 719, 720, 722, 723, 725, 726, 728, 729, 731, 732, 734, 736, 737, 739, 740, 742, 743, 745, 746, 748, 749, 751, 752, 754, 755, 757, 759, 760, 762, 763, 765, 766, 768, 769, 771, 772, 774, 775, 777, 778, 780, 782, 783, 785, 786, 788, 789, 791, 792, 794, 795, 797, 798, 800, 801, 803, 805, 806, 808, 809, 811, 812, 814, 815, 817, 818, 820, 821, 823, 824, 826, 828, 829, 831, 832, 834, 835, 837, 838, 840, 841, 843, 844, 846, 847, 849, 851, 852, 854, 855, 857, 858, 860, 861, 863, 864, 866, 867, 869, 870, 872, 874, 875, 877, 878, 880, 881, }
{441, 450, 460, 469, 478, 487, 496, 506, 515, 524, 533, 542, 552, 561, 570, 579, 588, 598, 607, 616, 625, 634, 644, 653, 662, 671, 680, 690, 699, 708, 717, 726, 736, 745, 754, 763, 772, 782, 791, 800, 809, 818, 828, 837, 846, 855, 864, 874, }
{883, 884, 886, 887, 889, 890, 892, 893, 895, 897, 898, 900, 901, 903, 904, 906, 907, 909, 910, 912, 913, 915, 916, 918, 920, 921, 923, 924, 926, 927, 929, 930, 932, 933, 935, 936, 938, 939, 941, 943, 944, 946, 947, 949, 950, 952, 953, 955, 956, 958, 959, 961, 962, 964, 966, 967, 969, 970, 972, 973, 975, 976, 978, 979, 981, 982, 984, 985, 987, 989, 990, 992, 993, 995, 996, 998, 999, 1001, 1002, 1004, 1005, 1007, 1008, 1010, 1012, 1013, 1015, 1016, 1018, 1019, 1021, 1022, 1024, 1025, 1027, 1028, 1030, 1031, 1033, 1035, 1036, 1038, 1039, 1041, 1042, 1044, 1045, 1047, 1048, 1050, 1051, 1053, 1054, 1056, 1058, 1059, 1061, 1062, 1064, 1065, 1067, 1068, 1070, 1071, 1073, 1074, 1076, 1077, 1079, 1081, 1082, 1084, 1085, 1087, 1088, 1090, 1091, 1093, 1094, 1096, 1097, 1099, 1100, 1102, 1104, 1105, 1107, 1108, 1110, 1111, 1113, 1114, 1116, 1117, 1119, 1120, 1122, 1123, 1125, 1127, 1128, 1130, 1131, 1133, 1134, 1136, 1137, 1139, 1140, 1142, 1143, 1145, 1146, 1148, 1150, 1151, 1153, 1154, 1156, 1157, 1159, 1160, 1162, 1163, 1165, 1166, 1168, 1169, 1171, 1173, 1174, 1176, 1177, 1179, 1180, 1182, 1183, 1185, 1186, 1188, 1189, 1191, 1192, 1194, 1196, 1197, 1199, 1200, 1202, 1203, 1205, 1206, 1208, 1209, 1211, 1212, 1214, 1215, 1217, 1219, 1220, 1222, 1223, 1225, 1226, 1228, 1229, 1231, 1232, 1234, 1235, 1237, 1238, 1240, 1242, 1243, 1245, 1246, 1248, 1249, 1251, 1252, 1254, 1255, 1257, 1258, 1260, 1261, 1263, 1265, 1266, 1268, 1269, 1271, 1272, 1274, 1275, 1277, 1278, 1280, 1281, 1283, 1284, 1286, 1288, 1289, 1291, 1292, 1294, 1295, 1297, 1298, 1300, 1301, 1303, 1304, 1306, 1307, 1309, 1311, 1312, 1314, 1315, 1317, 1318, 1320, 1321, 1323, }
{883, 892, 901, 910, 920, 929, 938, 947, 956, 966, 975, 984, 993, 1002, 1012, 1021, 1030, 1039, 1048, 1058, 1067, 1076, 1085, 1094, 1104, 1113, 1122, 1131, 1140, 1150, 1159, 1168, 1177, 1186, 1196, 1205, 1214, 1223, 1232, 1242, 1251, 1260, 1269, 1278, 1288, 1297, 1306, 1315, }
{1324, 1326, 1327, 1329, 1330, 1332, 1334, 1335, 1337, 1338, 1340, 1341, 1343, 1344, 1346, 1347, 1349, 1350, 1352, 1353, 1355, 1357, 1358, 1360, 1361, 1363, 1364, 1366, 1367, 1369, 1370, 1372, 1373, 1375, 1376, 1378, 1380, 1381, 1383, 1384, 1386, 1387, 1389, 1390, 1392, 1393, 1395, 1396, 1398, 1399, 1401, 1403, 1404, 1406, 1407, 1409, 1410, 1412, 1413, 1415, 1416, 1418, 1419, 1421, 1422, 1424, 1426, 1427, 1429, 1430, 1432, 1433, 1435, 1436, 1438, 1439, 1441, 1442, 1444, 1445, 1447, 1449, 1450, 1452, 1453, 1455, 1456, 1458, 1459, 1461, 1462, 1464, 1465, 1467, 1468, 1470, }
{1324, 1327, 1330, 1334, 1337, 1340, 1343, 1346, 1349, 1352, 1355, 1358, 1361, 1364, 1367, 1370, 1373, 1376, 1380, 1383, 1386, 1389, 1392, 1395, 1398, 1401, 1404, 1407, 1410, 1413, 1416, 1419, 1422, 1426, 1429, 1432, 1435, 1438, 1441, 1444, 1447, 1450, 1453, 1456, 1459, 1462, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 8
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 8
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 8
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,8}
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,8}
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,8}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,8}
slicedInpSpikeIds.shapeToString(): {48,8}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,8}
slicedInpSpikeIds.shapeToString(): {48,8}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,8}
slicedInpSpikeIds.shapeToString(): {48,8}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {488,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{726, 725, 18, }
start_tile: 1
num_tiles_per_layer[ilay]: 726
start_tile + num_tiles_per_layer[ilay]: 727
num_occupied_tiles: 489
batchsize: 48
sparse_size: 8
dLdx.shapeToString(): {489,48,8}
replicated_numGrads.shapeToString(): {489,48}
exchangeGroupSize: 24
numExchangeGroups: 20
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
lastExchangeGroupLarger
start_id: 456, end_id: 489
exchangeGroupSizeLast: 33
exchangeBatchSizeLast: 2
start_tile: 727
num_tiles_per_layer[ilay]: 725
start_tile + num_tiles_per_layer[ilay]: 1452
num_occupied_tiles: 488
batchsize: 48
sparse_size: 8
dLdx.shapeToString(): {488,48,8}
replicated_numGrads.shapeToString(): {488,48}
exchangeGroupSize: 24
numExchangeGroups: 20
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
lastExchangeGroupLarger
start_id: 456, end_id: 488
exchangeGroupSizeLast: 32
exchangeBatchSizeLast: 2
start_tile: 1452
num_tiles_per_layer[ilay]: 18
start_tile + num_tiles_per_layer[ilay]: 1470
num_occupied_tiles: 5
batchsize: 48
sparse_size: 8
dLdx.shapeToString(): {5,48,8}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {488,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 18:25:09.458204: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[978 978 976  10]
0 489.0
1 489.0
2 488.0
3 5.0
[TileMapping(start_tile=1, end_tile=490), TileMapping(start_tile=490, end_tile=979), TileMapping(start_tile=979, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=490), TileMapping(start_tile=490, end_tile=979), TileMapping(start_tile=979, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.31330417999518295
limit=0.31330417999518295
limit=0.31362502409359
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 978) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 978) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 976) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_8_8_8_2_48_1_490_979_1467_490_979_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 8), (100, 4190734     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][3
                                                                 keras_multi_lif_layer_sparse[0][7
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 4,190,734
Trainable params: 4,181,908
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_8_8_8_2_48_1_490_979_1467_490_979_1467_1472
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_8_8_8_2_48_1_490_979_1467_490_979_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.166452/52 [==============================] - 719s 14s/step - loss: 2.1664
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 1.811852/52 [==============================] - 0s 6ms/step - loss: 1.8118
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 1.640552/52 [==============================] - 0s 5ms/step - loss: 1.6405
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 1.602652/52 [==============================] - 0s 5ms/step - loss: 1.6026
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 1.582052/52 [==============================] - 0s 5ms/step - loss: 1.5820
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 1.549552/52 [==============================] - 0s 5ms/step - loss: 1.5495
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 1.541252/52 [==============================] - 0s 5ms/step - loss: 1.5412
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 1.587452/52 [==============================] - 0s 5ms/step - loss: 1.5874
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 1.600752/52 [==============================] - 0s 5ms/step - loss: 1.6007
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 1.590952/52 [==============================] - 0s 5ms/step - loss: 1.5909
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 1.597552/52 [==============================] - 0s 5ms/step - loss: 1.5975
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 1.658752/52 [==============================] - 0s 5ms/step - loss: 1.6587
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 1.663052/52 [==============================] - 0s 5ms/step - loss: 1.6630
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 1.603652/52 [==============================] - 0s 5ms/step - loss: 1.6036
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 1.595452/52 [==============================] - 0s 5ms/step - loss: 1.5954
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 1.643252/52 [==============================] - 0s 5ms/step - loss: 1.6432
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 1.630452/52 [==============================] - 0s 5ms/step - loss: 1.6304
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 1.589152/52 [==============================] - 0s 5ms/step - loss: 1.5891
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 1.575152/52 [==============================] - 0s 5ms/step - loss: 1.5751
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 1.608252/52 [==============================] - 0s 5ms/step - loss: 1.6082
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 1.597552/52 [==============================] - 0s 5ms/step - loss: 1.5975
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 1.569852/52 [==============================] - 0s 5ms/step - loss: 1.5698
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 1.558952/52 [==============================] - 0s 5ms/step - loss: 1.5589
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 1.603752/52 [==============================] - 0s 5ms/step - loss: 1.6037
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 1.582852/52 [==============================] - 0s 5ms/step - loss: 1.5828

Final time:  725.1213619709015
2022-09-12 18:25:18.829350: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.01
DENSE_SIZES:  [2312, 734, 734, 732, 732, 10]
SPARSE_SIZES:  [32, 6, 6, 6, 6, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  4
SECOND_THRESHOLD:  0.95

60000
9984
[16011  8491 30352 ... 36405 21102 23233]
2022-09-12 18:25:27.704058: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 18:25:28.623377: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 11, 2, 1
Build_allocator: 16, 3, 1
Build_allocator: 12, 2, 2
Build_allocator: 17, 3, 2
Build_allocator: 13, 2, 3
Build_allocator: 18, 3, 3
Build_allocator: 14, 2, 4
Build_allocator: 19, 3, 4
Build_allocator: 5, 1, 0
Build_allocator: 6, 1, 1
Build_allocator: 7, 1, 2
Build_allocator: 8, 1, 3
Build_allocator: 9, 1, 4
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 4, 0, 4
Build_allocator: 20, 4, 0
Build_allocator: 21, 4, 1
Build_allocator: 22, 4, 2
Build_allocator: 23, 4, 3
Build_allocator: 24, 4, 4
Build_allocator: 25, 5, 0
Build_allocator: 26, 5, 1
Build_allocator: 27, 5, 2
Build_allocator: 28, 5, 3
Build_allocator: 29, 5, 4
Build_allocator: 10, 2, 0
Build_allocator: 15, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0x1cedc240
1: &init_state[ilay]0x1cedc248
2: &init_state[ilay]0x1cedc250
3: &init_state[ilay]0x1cedc258
4: &init_state[ilay]0x1cedc260
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0
3: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
3: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 368, 735, 1101, 1467, }
end_tiles: {368, 735, 1101, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{2, 2, 2, 2, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 5, 6, 8, 10, 11, 13, 15, 17, 18, 20, 22, 23, 25, 27, 28, 30, 32, 34, 35, 37, 39, 40, 42, 44, 46, 47, 49, 51, 52, 54, 56, 57, 59, 61, 63, 64, 66, 68, 69, 71, 73, 74, 76, 78, 80, 81, 83, 85, 86, 88, 90, 92, 93, 95, 97, 98, 100, 102, 103, 105, 107, 109, 110, 112, 114, 115, 117, 119, 120, 122, 124, 126, 127, 129, 131, 132, 134, 136, 138, 139, 141, 143, 144, 146, 148, 149, 151, 153, 155, 156, 158, 160, 161, 163, 165, 166, 168, 170, 172, 173, 175, 177, 178, 180, 182, 184, 185, 187, 189, 190, 192, 194, 195, 197, 199, 201, 202, 204, 206, 207, 209, 211, 212, 214, 216, 218, 219, 221, 223, 224, 226, 228, 230, 231, 233, 235, 236, 238, 240, 241, 243, 245, 247, 248, 250, 252, 253, 255, 257, 258, 260, 262, 264, 265, 267, 269, 270, 272, 274, 276, 277, 279, 281, 282, 284, 286, 287, 289, 291, 293, 294, 296, 298, 299, 301, 303, 304, 306, 308, 310, 311, 313, 315, 316, 318, 320, 322, 323, 325, }
{0, 6, 13, 20, 27, 34, 40, 47, 54, 61, 68, 74, 81, 88, 95, 102, 109, 115, 122, 129, 136, 143, 149, 156, 163, 170, 177, 184, 190, 197, 204, 211, 218, 224, 231, 238, 245, 252, 258, 265, 272, 279, 286, 293, 299, 306, 313, 320, }
{327, 328, 330, 332, 333, 335, 337, 339, 340, 342, 344, 345, 347, 349, 350, 352, 354, 356, 357, 359, 361, 362, 364, 366, 368, 369, 371, 373, 374, 376, 378, 379, 381, 383, 385, 386, 388, 390, 391, 393, 395, 396, 398, 400, 402, 403, 405, 407, 408, 410, 412, 414, 415, 417, 419, 420, 422, 424, 425, 427, 429, 431, 432, 434, 436, 437, 439, 441, 442, 444, 446, 448, 449, 451, 453, 454, 456, 458, 460, 461, 463, 465, 466, 468, 470, 471, 473, 475, 477, 478, 480, 482, 483, 485, 487, 488, 490, 492, 494, 495, 497, 499, 500, 502, 504, 506, 507, 509, 511, 512, 514, 516, 517, 519, 521, 523, 524, 526, 528, 529, 531, 533, 534, 536, 538, 540, 541, 543, 545, 546, 548, 550, 552, 553, 555, 557, 558, 560, 562, 563, 565, 567, 569, 570, 572, 574, 575, 577, 579, 580, 582, 584, 586, 587, 589, 591, 592, 594, 596, 598, 599, 601, 603, 604, 606, 608, 609, 611, 613, 615, 616, 618, 620, 621, 623, 625, 626, 628, 630, 632, 633, 635, 637, 638, 640, 642, 644, 645, 647, 649, 650, 652, }
{327, 333, 340, 347, 354, 361, 368, 374, 381, 388, 395, 402, 408, 415, 422, 429, 436, 442, 449, 456, 463, 470, 477, 483, 490, 497, 504, 511, 517, 524, 531, 538, 545, 552, 558, 565, 572, 579, 586, 592, 599, 606, 613, 620, 626, 633, 640, 647, }
{654, 655, 657, 659, 661, 662, 664, 666, 667, 669, 671, 672, 674, 676, 678, 679, 681, 683, 684, 686, 688, 690, 691, 693, 695, 696, 698, 700, 701, 703, 705, 707, 708, 710, 712, 713, 715, 717, 718, 720, 722, 724, 725, 727, 729, 730, 732, 734, 736, 737, 739, 741, 742, 744, 746, 747, 749, 751, 753, 754, 756, 758, 759, 761, 763, 764, 766, 768, 770, 771, 773, 775, 776, 778, 780, 782, 783, 785, 787, 788, 790, 792, 793, 795, 797, 799, 800, 802, 804, 805, 807, 809, 810, 812, 814, 816, 817, 819, 821, 822, 824, 826, 828, 829, 831, 833, 834, 836, 838, 839, 841, 843, 845, 846, 848, 850, 851, 853, 855, 856, 858, 860, 862, 863, 865, 867, 868, 870, 872, 874, 875, 877, 879, 880, 882, 884, 885, 887, 889, 891, 892, 894, 896, 897, 899, 901, 902, 904, 906, 908, 909, 911, 913, 914, 916, 918, 920, 921, 923, 925, 926, 928, 930, 931, 933, 935, 937, 938, 940, 942, 943, 945, 947, 948, 950, 952, 954, 955, 957, 959, 960, 962, 964, 966, 967, 969, 971, 972, 974, 976, 977, 979, }
{654, 661, 667, 674, 681, 688, 695, 701, 708, 715, 722, 729, 736, 742, 749, 756, 763, 770, 776, 783, 790, 797, 804, 810, 817, 824, 831, 838, 845, 851, 858, 865, 872, 879, 885, 892, 899, 906, 913, 920, 926, 933, 940, 947, 954, 960, 967, 974, }
{981, 983, 984, 986, 988, 989, 991, 993, 994, 996, 998, 1000, 1001, 1003, 1005, 1006, 1008, 1010, 1012, 1013, 1015, 1017, 1018, 1020, 1022, 1023, 1025, 1027, 1029, 1030, 1032, 1034, 1035, 1037, 1039, 1040, 1042, 1044, 1046, 1047, 1049, 1051, 1052, 1054, 1056, 1058, 1059, 1061, 1063, 1064, 1066, 1068, 1069, 1071, 1073, 1075, 1076, 1078, 1080, 1081, 1083, 1085, 1086, 1088, 1090, 1092, 1093, 1095, 1097, 1098, 1100, 1102, 1104, 1105, 1107, 1109, 1110, 1112, 1114, 1115, 1117, 1119, 1121, 1122, 1124, 1126, 1127, 1129, 1131, 1132, 1134, 1136, 1138, 1139, 1141, 1143, 1144, 1146, 1148, 1150, 1151, 1153, 1155, 1156, 1158, 1160, 1161, 1163, 1165, 1167, 1168, 1170, 1172, 1173, 1175, 1177, 1178, 1180, 1182, 1184, 1185, 1187, 1189, 1190, 1192, 1194, 1196, 1197, 1199, 1201, 1202, 1204, 1206, 1207, 1209, 1211, 1213, 1214, 1216, 1218, 1219, 1221, 1223, 1224, 1226, 1228, 1230, 1231, 1233, 1235, 1236, 1238, 1240, 1242, 1243, 1245, 1247, 1248, 1250, 1252, 1253, 1255, 1257, 1259, 1260, 1262, 1264, 1265, 1267, 1269, 1270, 1272, 1274, 1276, 1277, 1279, 1281, 1282, 1284, 1286, 1288, 1289, 1291, 1293, 1294, 1296, 1298, 1299, 1301, 1303, 1305, 1306, }
{981, 988, 994, 1001, 1008, 1015, 1022, 1029, 1035, 1042, 1049, 1056, 1063, 1069, 1076, 1083, 1090, 1097, 1104, 1110, 1117, 1124, 1131, 1138, 1144, 1151, 1158, 1165, 1172, 1178, 1185, 1192, 1199, 1206, 1213, 1219, 1226, 1233, 1240, 1247, 1253, 1260, 1267, 1274, 1281, 1288, 1294, 1301, }
{1308, 1310, 1311, 1313, 1315, 1316, 1318, 1320, 1322, 1323, 1325, 1327, 1328, 1330, 1332, 1334, 1335, 1337, 1339, 1340, 1342, 1344, 1345, 1347, 1349, 1351, 1352, 1354, 1356, 1357, 1359, 1361, 1362, 1364, 1366, 1368, 1369, 1371, 1373, 1374, 1376, 1378, 1380, 1381, 1383, 1385, 1386, 1388, 1390, 1391, 1393, 1395, 1397, 1398, 1400, 1402, 1403, 1405, 1407, 1408, 1410, 1412, 1414, 1415, 1417, 1419, 1420, 1422, 1424, 1426, 1427, 1429, 1431, 1432, 1434, 1436, 1437, 1439, 1441, 1443, 1444, 1446, 1448, 1449, 1451, 1453, 1454, 1456, 1458, 1460, 1461, 1463, 1465, 1466, 1468, 1470, }
{1308, 1311, 1315, 1318, 1322, 1325, 1328, 1332, 1335, 1339, 1342, 1345, 1349, 1352, 1356, 1359, 1362, 1366, 1369, 1373, 1376, 1380, 1383, 1386, 1390, 1393, 1397, 1400, 1403, 1407, 1410, 1414, 1417, 1420, 1424, 1427, 1431, 1434, 1437, 1441, 1444, 1448, 1451, 1454, 1458, 1461, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 62
sparse_size_to_use: 6
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 62
sparse_size_to_use: 6
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 61
sparse_size_to_use: 6
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 61
sparse_size_to_use: 6
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,6}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,6}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,6}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,6}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 4
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,6}
slicedInpSpikeIds.shapeToString(): {48,6}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,6}
slicedInpSpikeIds.shapeToString(): {48,6}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,6}
slicedInpSpikeIds.shapeToString(): {48,6}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

3
out_spike_ids[ilay].shapeToString(): {100,48,6}
slicedInpSpikeIds.shapeToString(): {48,6}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,2}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{485, 484, 484, 15, }
start_tile: 1
num_tiles_per_layer[ilay]: 485
start_tile + num_tiles_per_layer[ilay]: 486
num_occupied_tiles: 367
batchsize: 48
sparse_size: 6
dLdx.shapeToString(): {367,48,6}
replicated_numGrads.shapeToString(): {367,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 367
exchangeGroupSizeLast: 31
exchangeBatchSizeLast: 2
start_tile: 486
num_tiles_per_layer[ilay]: 484
start_tile + num_tiles_per_layer[ilay]: 970
num_occupied_tiles: 366
batchsize: 48
sparse_size: 6
dLdx.shapeToString(): {366,48,6}
replicated_numGrads.shapeToString(): {366,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 366
exchangeGroupSizeLast: 30
exchangeBatchSizeLast: 2
start_tile: 970
num_tiles_per_layer[ilay]: 484
start_tile + num_tiles_per_layer[ilay]: 1454
num_occupied_tiles: 366
batchsize: 48
sparse_size: 6
dLdx.shapeToString(): {366,48,6}
replicated_numGrads.shapeToString(): {366,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 366
exchangeGroupSizeLast: 30
exchangeBatchSizeLast: 2
start_tile: 1454
num_tiles_per_layer[ilay]: 15
start_tile + num_tiles_per_layer[ilay]: 1469
num_occupied_tiles: 5
batchsize: 48
sparse_size: 6
dLdx.shapeToString(): {5,48,6}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,1}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 18:35:25.633157: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[734 734 732 732  10]
0 367.0
1 367.0
2 366.0
3 366.0
4 5.0
[TileMapping(start_tile=1, end_tile=368), TileMapping(start_tile=368, end_tile=735), TileMapping(start_tile=735, end_tile=1101), TileMapping(start_tile=1101, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=368), TileMapping(start_tile=368, end_tile=735), TileMapping(start_tile=735, end_tile=1101), TileMapping(start_tile=1101, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.3616492648073473
limit=0.3616492648073473
limit=0.3621429841700741
limit=0.3621429841700741
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 734) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 734) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 732) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 732) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_5:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_6_6_6_6_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 6), (100, 3325022     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][4
                                                                 keras_multi_lif_layer_sparse[0][9
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 3,325,022
Trainable params: 3,316,196
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_6_6_6_6_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_6_6_6_6_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.336952/52 [==============================] - 599s 12s/step - loss: 2.3369
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.008752/52 [==============================] - 0s 5ms/step - loss: 2.0087
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 1.941152/52 [==============================] - 0s 5ms/step - loss: 1.9411
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 1.864552/52 [==============================] - 0s 5ms/step - loss: 1.8645
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 1.806552/52 [==============================] - 0s 5ms/step - loss: 1.8065
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 1.623152/52 [==============================] - 0s 5ms/step - loss: 1.6231
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 1.561952/52 [==============================] - 0s 5ms/step - loss: 1.5619
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 1.436452/52 [==============================] - 0s 5ms/step - loss: 1.4364
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 1.350452/52 [==============================] - 0s 5ms/step - loss: 1.3504
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 1.225352/52 [==============================] - 0s 5ms/step - loss: 1.2253
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 1.247752/52 [==============================] - 0s 5ms/step - loss: 1.2477
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 1.170952/52 [==============================] - 0s 7ms/step - loss: 1.1709
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 1.187252/52 [==============================] - 1s 18ms/step - loss: 1.1872
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 1.162852/52 [==============================] - 0s 6ms/step - loss: 1.1628
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 1.244752/52 [==============================] - 0s 5ms/step - loss: 1.2447
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 1.234752/52 [==============================] - 0s 5ms/step - loss: 1.2347
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 1.265352/52 [==============================] - 0s 5ms/step - loss: 1.2653
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 1.262152/52 [==============================] - 0s 5ms/step - loss: 1.2621
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 1.361952/52 [==============================] - 0s 5ms/step - loss: 1.3619
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 1.316752/52 [==============================] - 0s 5ms/step - loss: 1.3167
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 1.351852/52 [==============================] - 0s 5ms/step - loss: 1.3518
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 1.328652/52 [==============================] - 0s 5ms/step - loss: 1.3286
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 1.445152/52 [==============================] - 0s 5ms/step - loss: 1.4451
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 1.426952/52 [==============================] - 0s 5ms/step - loss: 1.4269
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 1.461052/52 [==============================] - 0s 5ms/step - loss: 1.4610

Final time:  605.8257353305817
2022-09-12 18:35:35.311759: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.01
DENSE_SIZES:  [2312, 588, 586, 586, 586, 586, 10]
SPARSE_SIZES:  [32, 4, 4, 4, 4, 4, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  5
SECOND_THRESHOLD:  0.95

60000
9984
[39746 57296 16239 ... 24495 47660 57913]
2022-09-12 18:35:44.495286: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 18:35:45.393285: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 13, 2, 1
Build_allocator: 19, 3, 1
Build_allocator: 14, 2, 2
Build_allocator: 20, 3, 2
Build_allocator: 15, 2, 3
Build_allocator: 21, 3, 3
Build_allocator: 16, 2, 4
Build_allocator: 22, 3, 4
Build_allocator: 17, 2, 5
Build_allocator: 23, 3, 5
Build_allocator: 6, 1, 0
Build_allocator: 7, 1, 1
Build_allocator: 8, 1, 2
Build_allocator: 9, 1, 3
Build_allocator: 10, 1, 4
Build_allocator: 11, 1, 5
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 4, 0, 4
Build_allocator: 5, 0, 5
Build_allocator: 24, 4, 0
Build_allocator: 25, 4, 1
Build_allocator: 26, 4, 2
Build_allocator: 27, 4, 3
Build_allocator: 28, 4, 4
Build_allocator: 29, 4, 5
Build_allocator: 30, 5, 0
Build_allocator: 31, 5, 1
Build_allocator: 32, 5, 2
Build_allocator: 33, 5, 3
Build_allocator: 34, 5, 4
Build_allocator: 35, 5, 5
Build_allocator: 12, 2, 0
Build_allocator: 18, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0xe646050
1: &init_state[ilay]0xe646058
2: &init_state[ilay]0xe646060
3: &init_state[ilay]0xe646068
4: &init_state[ilay]0xe646070
5: &init_state[ilay]0xe646078
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0
3: init_state[ilay]==init_state[ilay+1]: 0
4: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
3: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
4: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 295, 588, 881, 1174, 1467, }
end_tiles: {295, 588, 881, 1174, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{2, 2, 2, 2, 2, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 2, 4, 5, 6, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 25, 26, 27, 29, 30, 32, 33, 34, 36, 37, 39, 40, 41, 43, 44, 46, 47, 48, 50, 51, 52, 54, 55, 57, 58, 59, 61, 62, 64, 65, 66, 68, 69, 71, 72, 73, 75, 76, 78, 79, 80, 82, 83, 85, 86, 87, 89, 90, 92, 93, 94, 96, 97, 98, 100, 101, 103, 104, 105, 107, 108, 110, 111, 112, 114, 115, 117, 118, 119, 121, 122, 124, 125, 126, 128, 129, 131, 132, 133, 135, 136, 138, 139, 140, 142, 143, 144, 146, 147, 149, 150, 151, 153, 154, 156, 157, 158, 160, 161, 163, 164, 165, 167, 168, 170, 171, 172, 174, 175, 177, 178, 179, 181, 182, 184, 185, 186, 188, 189, 190, 192, 193, 195, 196, 197, 199, 200, 202, 203, 204, 206, 207, 209, 210, 211, 213, 214, 216, 217, 218, 220, 221, 223, 224, 225, 227, 228, 230, 231, 232, 234, 235, 236, 238, 239, 241, 242, 243, 245, 246, 248, 249, 250, 252, 253, 255, 256, 257, 259, 260, 262, 263, 264, 266, }
{0, 5, 11, 16, 22, 27, 33, 39, 44, 50, 55, 61, 66, 72, 78, 83, 89, 94, 100, 105, 111, 117, 122, 128, 133, 139, 144, 150, 156, 161, 167, 172, 178, 184, 189, 195, 200, 206, 211, 217, 223, 228, 234, 239, 245, 250, 256, 262, }
{267, 269, 270, 271, 273, 274, 276, 277, 278, 280, 281, 282, 284, 285, 287, 288, 289, 291, 292, 294, 295, 296, 298, 299, 301, 302, 303, 305, 306, 308, 309, 310, 312, 313, 315, 316, 317, 319, 320, 322, 323, 324, 326, 327, 328, 330, 331, 333, 334, 335, 337, 338, 340, 341, 342, 344, 345, 347, 348, 349, 351, 352, 354, 355, 356, 358, 359, 361, 362, 363, 365, 366, 368, 369, 370, 372, 373, 374, 376, 377, 379, 380, 381, 383, 384, 386, 387, 388, 390, 391, 393, 394, 395, 397, 398, 400, 401, 402, 404, 405, 407, 408, 409, 411, 412, 414, 415, 416, 418, 419, 420, 422, 423, 425, 426, 427, 429, 430, 432, 433, 434, 436, 437, 439, 440, 441, 443, 444, 446, 447, 448, 450, 451, 453, 454, 455, 457, 458, 460, 461, 462, 464, 465, 466, 468, 469, 471, 472, 473, 475, 476, 478, 479, 480, 482, 483, 485, 486, 487, 489, 490, 492, 493, 494, 496, 497, 499, 500, 501, 503, 504, 506, 507, 508, 510, 511, 512, 514, 515, 517, 518, 519, 521, 522, 524, 525, 526, 528, 529, 531, 532, 533, }
{267, 273, 278, 284, 289, 295, 301, 306, 312, 317, 323, 328, 334, 340, 345, 351, 356, 362, 368, 373, 379, 384, 390, 395, 401, 407, 412, 418, 423, 429, 434, 440, 446, 451, 457, 462, 468, 473, 479, 485, 490, 496, 501, 507, 512, 518, 524, 529, }
{535, 536, 538, 539, 540, 542, 543, 545, 546, 547, 549, 550, 552, 553, 554, 556, 557, 558, 560, 561, 563, 564, 565, 567, 568, 570, 571, 572, 574, 575, 577, 578, 579, 581, 582, 584, 585, 586, 588, 589, 591, 592, 593, 595, 596, 598, 599, 600, 602, 603, 604, 606, 607, 609, 610, 611, 613, 614, 616, 617, 618, 620, 621, 623, 624, 625, 627, 628, 630, 631, 632, 634, 635, 637, 638, 639, 641, 642, 644, 645, 646, 648, 649, 650, 652, 653, 655, 656, 657, 659, 660, 662, 663, 664, 666, 667, 669, 670, 671, 673, 674, 676, 677, 678, 680, 681, 683, 684, 685, 687, 688, 690, 691, 692, 694, 695, 696, 698, 699, 701, 702, 703, 705, 706, 708, 709, 710, 712, 713, 715, 716, 717, 719, 720, 722, 723, 724, 726, 727, 729, 730, 731, 733, 734, 736, 737, 738, 740, 741, 742, 744, 745, 747, 748, 749, 751, 752, 754, 755, 756, 758, 759, 761, 762, 763, 765, 766, 768, 769, 770, 772, 773, 775, 776, 777, 779, 780, 782, 783, 784, 786, 787, 788, 790, 791, 793, 794, 795, 797, 798, 800, 801, }
{535, 540, 546, 552, 557, 563, 568, 574, 579, 585, 591, 596, 602, 607, 613, 618, 624, 630, 635, 641, 646, 652, 657, 663, 669, 674, 680, 685, 691, 696, 702, 708, 713, 719, 724, 730, 736, 741, 747, 752, 758, 763, 769, 775, 780, 786, 791, 797, }
{802, 804, 805, 807, 808, 809, 811, 812, 814, 815, 816, 818, 819, 821, 822, 823, 825, 826, 828, 829, 830, 832, 833, 834, 836, 837, 839, 840, 841, 843, 844, 846, 847, 848, 850, 851, 853, 854, 855, 857, 858, 860, 861, 862, 864, 865, 867, 868, 869, 871, 872, 874, 875, 876, 878, 879, 880, 882, 883, 885, 886, 887, 889, 890, 892, 893, 894, 896, 897, 899, 900, 901, 903, 904, 906, 907, 908, 910, 911, 913, 914, 915, 917, 918, 920, 921, 922, 924, 925, 926, 928, 929, 931, 932, 933, 935, 936, 938, 939, 940, 942, 943, 945, 946, 947, 949, 950, 952, 953, 954, 956, 957, 959, 960, 961, 963, 964, 966, 967, 968, 970, 971, 972, 974, 975, 977, 978, 979, 981, 982, 984, 985, 986, 988, 989, 991, 992, 993, 995, 996, 998, 999, 1000, 1002, 1003, 1005, 1006, 1007, 1009, 1010, 1012, 1013, 1014, 1016, 1017, 1018, 1020, 1021, 1023, 1024, 1025, 1027, 1028, 1030, 1031, 1032, 1034, 1035, 1037, 1038, 1039, 1041, 1042, 1044, 1045, 1046, 1048, 1049, 1051, 1052, 1053, 1055, 1056, 1058, 1059, 1060, 1062, 1063, 1064, 1066, 1067, 1069, }
{802, 808, 814, 819, 825, 830, 836, 841, 847, 853, 858, 864, 869, 875, 880, 886, 892, 897, 903, 908, 914, 920, 925, 931, 936, 942, 947, 953, 959, 964, 970, 975, 981, 986, 992, 998, 1003, 1009, 1014, 1020, 1025, 1031, 1037, 1042, 1048, 1053, 1059, 1064, }
{1070, 1071, 1073, 1074, 1076, 1077, 1078, 1080, 1081, 1083, 1084, 1085, 1087, 1088, 1090, 1091, 1092, 1094, 1095, 1097, 1098, 1099, 1101, 1102, 1104, 1105, 1106, 1108, 1109, 1110, 1112, 1113, 1115, 1116, 1117, 1119, 1120, 1122, 1123, 1124, 1126, 1127, 1129, 1130, 1131, 1133, 1134, 1136, 1137, 1138, 1140, 1141, 1143, 1144, 1145, 1147, 1148, 1150, 1151, 1152, 1154, 1155, 1156, 1158, 1159, 1161, 1162, 1163, 1165, 1166, 1168, 1169, 1170, 1172, 1173, 1175, 1176, 1177, 1179, 1180, 1182, 1183, 1184, 1186, 1187, 1189, 1190, 1191, 1193, 1194, 1196, 1197, 1198, 1200, 1201, 1202, 1204, 1205, 1207, 1208, 1209, 1211, 1212, 1214, 1215, 1216, 1218, 1219, 1221, 1222, 1223, 1225, 1226, 1228, 1229, 1230, 1232, 1233, 1235, 1236, 1237, 1239, 1240, 1242, 1243, 1244, 1246, 1247, 1248, 1250, 1251, 1253, 1254, 1255, 1257, 1258, 1260, 1261, 1262, 1264, 1265, 1267, 1268, 1269, 1271, 1272, 1274, 1275, 1276, 1278, 1279, 1281, 1282, 1283, 1285, 1286, 1288, 1289, 1290, 1292, 1293, 1294, 1296, 1297, 1299, 1300, 1301, 1303, 1304, 1306, 1307, 1308, 1310, 1311, 1313, 1314, 1315, 1317, 1318, 1320, 1321, 1322, 1324, 1325, 1327, 1328, 1329, 1331, 1332, 1334, 1335, 1336, }
{1070, 1076, 1081, 1087, 1092, 1098, 1104, 1109, 1115, 1120, 1126, 1131, 1137, 1143, 1148, 1154, 1159, 1165, 1170, 1176, 1182, 1187, 1193, 1198, 1204, 1209, 1215, 1221, 1226, 1232, 1237, 1243, 1248, 1254, 1260, 1265, 1271, 1276, 1282, 1288, 1293, 1299, 1304, 1310, 1315, 1321, 1327, 1332, }
{1338, 1339, 1340, 1342, 1343, 1345, 1346, 1347, 1349, 1350, 1352, 1353, 1354, 1356, 1357, 1359, 1360, 1361, 1363, 1364, 1366, 1367, 1368, 1370, 1371, 1373, 1374, 1375, 1377, 1378, 1380, 1381, 1382, 1384, 1385, 1386, 1388, 1389, 1391, 1392, 1393, 1395, 1396, 1398, 1399, 1400, 1402, 1403, 1405, 1406, 1407, 1409, 1410, 1412, 1413, 1414, 1416, 1417, 1419, 1420, 1421, 1423, 1424, 1426, 1427, 1428, 1430, 1431, 1432, 1434, 1435, 1437, 1438, 1439, 1441, 1442, 1444, 1445, 1446, 1448, 1449, 1451, 1452, 1453, 1455, 1456, 1458, 1459, 1460, 1462, 1463, 1465, 1466, 1467, 1469, 1470, }
{1338, 1340, 1343, 1346, 1349, 1352, 1354, 1357, 1360, 1363, 1366, 1368, 1371, 1374, 1377, 1380, 1382, 1385, 1388, 1391, 1393, 1396, 1399, 1402, 1405, 1407, 1410, 1413, 1416, 1419, 1421, 1424, 1427, 1430, 1432, 1435, 1438, 1441, 1444, 1446, 1449, 1452, 1455, 1458, 1460, 1463, 1466, 1469, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 4
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 4
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 4
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 4
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 4
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,4}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,4}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,4}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,4}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,4}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 4
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 5
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,4}
slicedInpSpikeIds.shapeToString(): {48,4}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,4}
slicedInpSpikeIds.shapeToString(): {48,4}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,4}
slicedInpSpikeIds.shapeToString(): {48,4}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

3
out_spike_ids[ilay].shapeToString(): {100,48,4}
slicedInpSpikeIds.shapeToString(): {48,4}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

4
out_spike_ids[ilay].shapeToString(): {100,48,4}
slicedInpSpikeIds.shapeToString(): {48,4}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {294,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 5
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{364, 364, 364, 364, 14, }
start_tile: 1
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 365
num_occupied_tiles: 293
batchsize: 48
sparse_size: 4
dLdx.shapeToString(): {293,48,4}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 365
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 729
num_occupied_tiles: 293
batchsize: 48
sparse_size: 4
dLdx.shapeToString(): {293,48,4}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 729
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 1093
num_occupied_tiles: 293
batchsize: 48
sparse_size: 4
dLdx.shapeToString(): {293,48,4}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 1093
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 1457
num_occupied_tiles: 293
batchsize: 48
sparse_size: 4
dLdx.shapeToString(): {293,48,4}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 1457
num_tiles_per_layer[ilay]: 14
start_tile + num_tiles_per_layer[ilay]: 1471
num_occupied_tiles: 5
batchsize: 48
sparse_size: 4
dLdx.shapeToString(): {5,48,4}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {294,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 5
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 18:43:46.970031: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[588 586 586 586 586  10]
0 294.0
1 293.0
2 293.0
3 293.0
4 293.0
5 5.0
[TileMapping(start_tile=1, end_tile=295), TileMapping(start_tile=295, end_tile=588), TileMapping(start_tile=588, end_tile=881), TileMapping(start_tile=881, end_tile=1174), TileMapping(start_tile=1174, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=295), TileMapping(start_tile=295, end_tile=588), TileMapping(start_tile=588, end_tile=881), TileMapping(start_tile=881, end_tile=1174), TileMapping(start_tile=1174, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.40406101782088427
limit=0.4047499544129309
limit=0.4047499544129309
limit=0.4047499544129309
limit=0.4047499544129309
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 588) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 586) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 586) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 586) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_5:0' shape=(2, 586) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_6:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_4_4_4_4_4_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 4), (100, 2748898     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][5
                                                                 keras_multi_lif_layer_sparse[0][1
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 2,748,898
Trainable params: 2,740,072
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_4_4_4_4_4_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_4_4_4_4_4_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.355652/52 [==============================] - 483s 9s/step - loss: 2.3556
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.297552/52 [==============================] - 0s 5ms/step - loss: 2.2975
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 2.205552/52 [==============================] - 0s 4ms/step - loss: 2.2055
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 2.151052/52 [==============================] - 0s 4ms/step - loss: 2.1510
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 2.144752/52 [==============================] - 0s 4ms/step - loss: 2.1447
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 2.140852/52 [==============================] - 0s 4ms/step - loss: 2.1408
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 2.147852/52 [==============================] - 0s 4ms/step - loss: 2.1478
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 2.169652/52 [==============================] - 0s 5ms/step - loss: 2.1696
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 2.185752/52 [==============================] - 0s 4ms/step - loss: 2.1857
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 2.170852/52 [==============================] - 0s 4ms/step - loss: 2.1708
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 2.157752/52 [==============================] - 0s 4ms/step - loss: 2.1577
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 2.165352/52 [==============================] - 0s 4ms/step - loss: 2.1653
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 2.047852/52 [==============================] - 0s 4ms/step - loss: 2.0478
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 2.001152/52 [==============================] - 0s 4ms/step - loss: 2.0011
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 1.943952/52 [==============================] - 0s 4ms/step - loss: 1.9439
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 1.985052/52 [==============================] - 0s 4ms/step - loss: 1.9850
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 2.016152/52 [==============================] - 0s 4ms/step - loss: 2.0161
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 1.976152/52 [==============================] - 0s 4ms/step - loss: 1.9761
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 1.939352/52 [==============================] - 0s 4ms/step - loss: 1.9393
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 1.958052/52 [==============================] - 0s 4ms/step - loss: 1.9580
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 1.986052/52 [==============================] - 0s 4ms/step - loss: 1.9860
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 1.965752/52 [==============================] - 0s 4ms/step - loss: 1.9657
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 1.928552/52 [==============================] - 0s 4ms/step - loss: 1.9285
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 1.947752/52 [==============================] - 0s 4ms/step - loss: 1.9477
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 1.975652/52 [==============================] - 0s 5ms/step - loss: 1.9756

Final time:  488.93168663978577
2022-09-12 18:43:55.183632: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.01
DENSE_SIZES:  [2312, 1466, 1466, 10]
SPARSE_SIZES:  [32, 14, 14, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  2
SECOND_THRESHOLD:  0.98

60000
9984
[42725  2049 15047 ... 24529 18784 12994]
2022-09-12 18:44:04.219754: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 18:44:05.074033: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 7, 2, 1
Build_allocator: 10, 3, 1
Build_allocator: 8, 2, 2
Build_allocator: 11, 3, 2
Build_allocator: 3, 1, 0
Build_allocator: 4, 1, 1
Build_allocator: 5, 1, 2
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 12, 4, 0
Build_allocator: 13, 4, 1
Build_allocator: 14, 4, 2
Build_allocator: 15, 5, 0
Build_allocator: 16, 5, 1
Build_allocator: 17, 5, 2
Build_allocator: 6, 2, 0
Build_allocator: 9, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0xce0d230
1: &init_state[ilay]0xce0d238
2: &init_state[ilay]0xce0d240
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 734, 1467, }
end_tiles: {734, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{4, 4, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 5, 6, 8, 10, 11, 13, 15, 17, 18, 20, 22, 23, 25, 27, 28, 30, 32, 34, 35, 37, 39, 40, 42, 44, 46, 47, 49, 51, 52, 54, 56, 57, 59, 61, 63, 64, 66, 68, 69, 71, 73, 74, 76, 78, 80, 81, 83, 85, 86, 88, 90, 92, 93, 95, 97, 98, 100, 102, 103, 105, 107, 109, 110, 112, 114, 115, 117, 119, 120, 122, 124, 126, 127, 129, 131, 132, 134, 136, 138, 139, 141, 143, 144, 146, 148, 149, 151, 153, 155, 156, 158, 160, 161, 163, 165, 166, 168, 170, 172, 173, 175, 177, 178, 180, 182, 184, 185, 187, 189, 190, 192, 194, 195, 197, 199, 201, 202, 204, 206, 207, 209, 211, 212, 214, 216, 218, 219, 221, 223, 224, 226, 228, 230, 231, 233, 235, 236, 238, 240, 241, 243, 245, 247, 248, 250, 252, 253, 255, 257, 258, 260, 262, 264, 265, 267, 269, 270, 272, 274, 276, 277, 279, 281, 282, 284, 286, 287, 289, 291, 293, 294, 296, 298, 299, 301, 303, 304, 306, 308, 310, 311, 313, 315, 316, 318, 320, 322, 323, 325, 327, 328, 330, 332, 333, 335, 337, 339, 340, 342, 344, 345, 347, 349, 350, 352, 354, 356, 357, 359, 361, 362, 364, 366, 368, 369, 371, 373, 374, 376, 378, 379, 381, 383, 385, 386, 388, 390, 391, 393, 395, 396, 398, 400, 402, 403, 405, 407, 408, 410, 412, 414, 415, 417, 419, 420, 422, 424, 425, 427, 429, 431, 432, 434, 436, 437, 439, 441, 442, 444, 446, 448, 449, 451, 453, 454, 456, 458, 460, 461, 463, 465, 466, 468, 470, 471, 473, 475, 477, 478, 480, 482, 483, 485, 487, 488, 490, 492, 494, 495, 497, 499, 500, 502, 504, 506, 507, 509, 511, 512, 514, 516, 517, 519, 521, 523, 524, 526, 528, 529, 531, 533, 534, 536, 538, 540, 541, 543, 545, 546, 548, 550, 552, 553, 555, 557, 558, 560, 562, 563, 565, 567, 569, 570, 572, 574, 575, 577, 579, 580, 582, 584, 586, 587, 589, 591, 592, 594, 596, 598, 599, 601, 603, 604, 606, 608, 609, 611, 613, 615, 616, 618, 620, 621, 623, 625, 626, 628, 630, 632, 633, 635, 637, 638, 640, 642, 644, 645, 647, 649, 650, 652, }
{0, 13, 27, 40, 54, 68, 81, 95, 109, 122, 136, 149, 163, 177, 190, 204, 218, 231, 245, 258, 272, 286, 299, 313, 327, 340, 354, 368, 381, 395, 408, 422, 436, 449, 463, 477, 490, 504, 517, 531, 545, 558, 572, 586, 599, 613, 626, 640, }
{654, 655, 657, 659, 661, 662, 664, 666, 667, 669, 671, 672, 674, 676, 678, 679, 681, 683, 684, 686, 688, 690, 691, 693, 695, 696, 698, 700, 701, 703, 705, 707, 708, 710, 712, 713, 715, 717, 718, 720, 722, 724, 725, 727, 729, 730, 732, 734, 736, 737, 739, 741, 742, 744, 746, 747, 749, 751, 753, 754, 756, 758, 759, 761, 763, 764, 766, 768, 770, 771, 773, 775, 776, 778, 780, 782, 783, 785, 787, 788, 790, 792, 793, 795, 797, 799, 800, 802, 804, 805, 807, 809, 810, 812, 814, 816, 817, 819, 821, 822, 824, 826, 828, 829, 831, 833, 834, 836, 838, 839, 841, 843, 845, 846, 848, 850, 851, 853, 855, 856, 858, 860, 862, 863, 865, 867, 868, 870, 872, 874, 875, 877, 879, 880, 882, 884, 885, 887, 889, 891, 892, 894, 896, 897, 899, 901, 902, 904, 906, 908, 909, 911, 913, 914, 916, 918, 920, 921, 923, 925, 926, 928, 930, 931, 933, 935, 937, 938, 940, 942, 943, 945, 947, 948, 950, 952, 954, 955, 957, 959, 960, 962, 964, 966, 967, 969, 971, 972, 974, 976, 977, 979, 981, 983, 984, 986, 988, 989, 991, 993, 994, 996, 998, 1000, 1001, 1003, 1005, 1006, 1008, 1010, 1012, 1013, 1015, 1017, 1018, 1020, 1022, 1023, 1025, 1027, 1029, 1030, 1032, 1034, 1035, 1037, 1039, 1040, 1042, 1044, 1046, 1047, 1049, 1051, 1052, 1054, 1056, 1058, 1059, 1061, 1063, 1064, 1066, 1068, 1069, 1071, 1073, 1075, 1076, 1078, 1080, 1081, 1083, 1085, 1086, 1088, 1090, 1092, 1093, 1095, 1097, 1098, 1100, 1102, 1104, 1105, 1107, 1109, 1110, 1112, 1114, 1115, 1117, 1119, 1121, 1122, 1124, 1126, 1127, 1129, 1131, 1132, 1134, 1136, 1138, 1139, 1141, 1143, 1144, 1146, 1148, 1150, 1151, 1153, 1155, 1156, 1158, 1160, 1161, 1163, 1165, 1167, 1168, 1170, 1172, 1173, 1175, 1177, 1178, 1180, 1182, 1184, 1185, 1187, 1189, 1190, 1192, 1194, 1196, 1197, 1199, 1201, 1202, 1204, 1206, 1207, 1209, 1211, 1213, 1214, 1216, 1218, 1219, 1221, 1223, 1224, 1226, 1228, 1230, 1231, 1233, 1235, 1236, 1238, 1240, 1242, 1243, 1245, 1247, 1248, 1250, 1252, 1253, 1255, 1257, 1259, 1260, 1262, 1264, 1265, 1267, 1269, 1270, 1272, 1274, 1276, 1277, 1279, 1281, 1282, 1284, 1286, 1288, 1289, 1291, 1293, 1294, 1296, 1298, 1299, 1301, 1303, 1305, 1306, }
{654, 667, 681, 695, 708, 722, 736, 749, 763, 776, 790, 804, 817, 831, 845, 858, 872, 885, 899, 913, 926, 940, 954, 967, 981, 994, 1008, 1022, 1035, 1049, 1063, 1076, 1090, 1104, 1117, 1131, 1144, 1158, 1172, 1185, 1199, 1213, 1226, 1240, 1253, 1267, 1281, 1294, }
{1308, 1310, 1311, 1313, 1315, 1316, 1318, 1320, 1322, 1323, 1325, 1327, 1328, 1330, 1332, 1334, 1335, 1337, 1339, 1340, 1342, 1344, 1345, 1347, 1349, 1351, 1352, 1354, 1356, 1357, 1359, 1361, 1362, 1364, 1366, 1368, 1369, 1371, 1373, 1374, 1376, 1378, 1380, 1381, 1383, 1385, 1386, 1388, 1390, 1391, 1393, 1395, 1397, 1398, 1400, 1402, 1403, 1405, 1407, 1408, 1410, 1412, 1414, 1415, 1417, 1419, 1420, 1422, 1424, 1426, 1427, 1429, 1431, 1432, 1434, 1436, 1437, 1439, 1441, 1443, 1444, 1446, 1448, 1449, 1451, 1453, 1454, 1456, 1458, 1460, 1461, 1463, 1465, 1466, 1468, 1470, }
{1308, 1311, 1315, 1318, 1322, 1325, 1328, 1332, 1335, 1339, 1342, 1345, 1349, 1352, 1356, 1359, 1362, 1366, 1369, 1373, 1376, 1380, 1383, 1386, 1390, 1393, 1397, 1400, 1403, 1407, 1410, 1414, 1417, 1420, 1424, 1427, 1431, 1434, 1437, 1441, 1444, 1448, 1451, 1454, 1458, 1461, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 4
num_workers: 24
max_num_states_per_worker: 62
sparse_size_to_use: 14
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 4
num_workers: 24
max_num_states_per_worker: 62
sparse_size_to_use: 14
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,4,14}
 repeated_sparse_spike_ids.shapeToString(): {48,2,4,14}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,14}
slicedInpSpikeIds.shapeToString(): {48,14}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,14}
slicedInpSpikeIds.shapeToString(): {48,14}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{1444, 26, }
start_tile: 1
num_tiles_per_layer[ilay]: 1444
start_tile + num_tiles_per_layer[ilay]: 1445
num_occupied_tiles: 733
batchsize: 48
sparse_size: 14
dLdx.shapeToString(): {733,48,14}
replicated_numGrads.shapeToString(): {733,48}
exchangeGroupSize: 24
numExchangeGroups: 30
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
start_id: 456, end_id: 480
start_id: 480, end_id: 504
start_id: 504, end_id: 528
start_id: 528, end_id: 552
start_id: 552, end_id: 576
start_id: 576, end_id: 600
start_id: 600, end_id: 624
start_id: 624, end_id: 648
start_id: 648, end_id: 672
start_id: 672, end_id: 696
lastExchangeGroupLarger
start_id: 696, end_id: 733
exchangeGroupSizeLast: 37
exchangeBatchSizeLast: 2
start_tile: 1445
num_tiles_per_layer[ilay]: 26
start_tile + num_tiles_per_layer[ilay]: 1471
num_occupied_tiles: 5
batchsize: 48
sparse_size: 14
dLdx.shapeToString(): {5,48,14}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 18:59:40.828680: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[1466 1466   10]
0 733.0
1 733.0
2 5.0
[TileMapping(start_tile=1, end_tile=734), TileMapping(start_tile=734, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=734), TileMapping(start_tile=734, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.2558990251065398
limit=0.2558990251065398
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 1466) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 1466) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_14_14_2_48_1_734_1467_734_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 14), (100 5562034     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][2
                                                                 keras_multi_lif_layer_sparse[0][5
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 5,562,034
Trainable params: 5,553,208
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_14_14_2_48_1_734_1467_734_1467_1472
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_14_14_2_48_1_734_1467_734_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.018052/52 [==============================] - 938s 18s/step - loss: 2.0180
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 1.821552/52 [==============================] - 0s 6ms/step - loss: 1.8215
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 1.775952/52 [==============================] - 0s 6ms/step - loss: 1.7759
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 1.665352/52 [==============================] - 0s 6ms/step - loss: 1.6653
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 1.524052/52 [==============================] - 0s 6ms/step - loss: 1.5240
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 1.493852/52 [==============================] - 0s 6ms/step - loss: 1.4938
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 1.512652/52 [==============================] - 0s 6ms/step - loss: 1.5126
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 1.478152/52 [==============================] - 0s 6ms/step - loss: 1.4781
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 1.241552/52 [==============================] - 0s 6ms/step - loss: 1.2415
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 1.249352/52 [==============================] - 0s 6ms/step - loss: 1.2493
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 1.275852/52 [==============================] - 0s 6ms/step - loss: 1.2758
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 1.219952/52 [==============================] - 0s 7ms/step - loss: 1.2199
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 1.195552/52 [==============================] - 0s 6ms/step - loss: 1.1955
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 1.212852/52 [==============================] - 0s 6ms/step - loss: 1.2128
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 1.271752/52 [==============================] - 0s 6ms/step - loss: 1.2717
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 1.270652/52 [==============================] - 0s 6ms/step - loss: 1.2706
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 1.206552/52 [==============================] - 0s 6ms/step - loss: 1.2065
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 1.216952/52 [==============================] - 0s 6ms/step - loss: 1.2169
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 1.247152/52 [==============================] - 0s 6ms/step - loss: 1.2471
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 1.204052/52 [==============================] - 0s 6ms/step - loss: 1.2040
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 1.193952/52 [==============================] - 0s 6ms/step - loss: 1.1939
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 1.224952/52 [==============================] - 0s 6ms/step - loss: 1.2249
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 1.256052/52 [==============================] - 0s 6ms/step - loss: 1.2560
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 1.212652/52 [==============================] - 0s 6ms/step - loss: 1.2126
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 1.208952/52 [==============================] - 0s 6ms/step - loss: 1.2089

Final time:  946.0798780918121
2022-09-12 18:59:52.173989: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.01
DENSE_SIZES:  [2312, 978, 978, 976, 10]
SPARSE_SIZES:  [32, 8, 8, 8, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  3
SECOND_THRESHOLD:  0.98

60000
9984
[10593 51515 59952 ... 13682 29873  7510]
2022-09-12 19:00:00.769783: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 19:00:01.605120: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 9, 2, 1
Build_allocator: 13, 3, 1
Build_allocator: 10, 2, 2
Build_allocator: 14, 3, 2
Build_allocator: 11, 2, 3
Build_allocator: 15, 3, 3
Build_allocator: 4, 1, 0
Build_allocator: 5, 1, 1
Build_allocator: 6, 1, 2
Build_allocator: 7, 1, 3
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 16, 4, 0
Build_allocator: 17, 4, 1
Build_allocator: 18, 4, 2
Build_allocator: 19, 4, 3
Build_allocator: 20, 5, 0
Build_allocator: 21, 5, 1
Build_allocator: 22, 5, 2
Build_allocator: 23, 5, 3
Build_allocator: 8, 2, 0
Build_allocator: 12, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0x1ffe3e50
1: &init_state[ilay]0x1ffe3e58
2: &init_state[ilay]0x1ffe3e60
3: &init_state[ilay]0x1ffe3e68
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 490, 979, 1467, }
end_tiles: {490, 979, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{3, 3, 3, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 4, 6, 7, 9, 10, 12, 13, 15, 16, 18, 19, 21, 23, 24, 26, 27, 29, 30, 32, 33, 35, 36, 38, 39, 41, 42, 44, 46, 47, 49, 50, 52, 53, 55, 56, 58, 59, 61, 62, 64, 65, 67, 69, 70, 72, 73, 75, 76, 78, 79, 81, 82, 84, 85, 87, 88, 90, 92, 93, 95, 96, 98, 99, 101, 102, 104, 105, 107, 108, 110, 111, 113, 115, 116, 118, 119, 121, 122, 124, 125, 127, 128, 130, 131, 133, 134, 136, 138, 139, 141, 142, 144, 145, 147, 148, 150, 151, 153, 154, 156, 157, 159, 161, 162, 164, 165, 167, 168, 170, 171, 173, 174, 176, 177, 179, 180, 182, 184, 185, 187, 188, 190, 191, 193, 194, 196, 197, 199, 200, 202, 203, 205, 207, 208, 210, 211, 213, 214, 216, 217, 219, 220, 222, 223, 225, 226, 228, 230, 231, 233, 234, 236, 237, 239, 240, 242, 243, 245, 246, 248, 249, 251, 253, 254, 256, 257, 259, 260, 262, 263, 265, 266, 268, 269, 271, 272, 274, 276, 277, 279, 280, 282, 283, 285, 286, 288, 289, 291, 292, 294, 295, 297, 299, 300, 302, 303, 305, 306, 308, 309, 311, 312, 314, 315, 317, 318, 320, 322, 323, 325, 326, 328, 329, 331, 332, 334, 335, 337, 338, 340, 341, 343, 345, 346, 348, 349, 351, 352, 354, 355, 357, 358, 360, 361, 363, 364, 366, 368, 369, 371, 372, 374, 375, 377, 378, 380, 381, 383, 384, 386, 387, 389, 391, 392, 394, 395, 397, 398, 400, 401, 403, 404, 406, 407, 409, 410, 412, 414, 415, 417, 418, 420, 421, 423, 424, 426, 427, 429, 430, 432, 433, 435, 437, 438, 440, }
{0, 9, 18, 27, 36, 46, 55, 64, 73, 82, 92, 101, 110, 119, 128, 138, 147, 156, 165, 174, 184, 193, 202, 211, 220, 230, 239, 248, 257, 266, 276, 285, 294, 303, 312, 322, 331, 340, 349, 358, 368, 377, 386, 395, 404, 414, 423, 432, }
{441, 443, 444, 446, 447, 449, 450, 452, 453, 455, 456, 458, 460, 461, 463, 464, 466, 467, 469, 470, 472, 473, 475, 476, 478, 479, 481, 483, 484, 486, 487, 489, 490, 492, 493, 495, 496, 498, 499, 501, 502, 504, 506, 507, 509, 510, 512, 513, 515, 516, 518, 519, 521, 522, 524, 525, 527, 529, 530, 532, 533, 535, 536, 538, 539, 541, 542, 544, 545, 547, 548, 550, 552, 553, 555, 556, 558, 559, 561, 562, 564, 565, 567, 568, 570, 571, 573, 575, 576, 578, 579, 581, 582, 584, 585, 587, 588, 590, 591, 593, 594, 596, 598, 599, 601, 602, 604, 605, 607, 608, 610, 611, 613, 614, 616, 617, 619, 621, 622, 624, 625, 627, 628, 630, 631, 633, 634, 636, 637, 639, 640, 642, 644, 645, 647, 648, 650, 651, 653, 654, 656, 657, 659, 660, 662, 663, 665, 667, 668, 670, 671, 673, 674, 676, 677, 679, 680, 682, 683, 685, 686, 688, 690, 691, 693, 694, 696, 697, 699, 700, 702, 703, 705, 706, 708, 709, 711, 713, 714, 716, 717, 719, 720, 722, 723, 725, 726, 728, 729, 731, 732, 734, 736, 737, 739, 740, 742, 743, 745, 746, 748, 749, 751, 752, 754, 755, 757, 759, 760, 762, 763, 765, 766, 768, 769, 771, 772, 774, 775, 777, 778, 780, 782, 783, 785, 786, 788, 789, 791, 792, 794, 795, 797, 798, 800, 801, 803, 805, 806, 808, 809, 811, 812, 814, 815, 817, 818, 820, 821, 823, 824, 826, 828, 829, 831, 832, 834, 835, 837, 838, 840, 841, 843, 844, 846, 847, 849, 851, 852, 854, 855, 857, 858, 860, 861, 863, 864, 866, 867, 869, 870, 872, 874, 875, 877, 878, 880, 881, }
{441, 450, 460, 469, 478, 487, 496, 506, 515, 524, 533, 542, 552, 561, 570, 579, 588, 598, 607, 616, 625, 634, 644, 653, 662, 671, 680, 690, 699, 708, 717, 726, 736, 745, 754, 763, 772, 782, 791, 800, 809, 818, 828, 837, 846, 855, 864, 874, }
{883, 884, 886, 887, 889, 890, 892, 893, 895, 897, 898, 900, 901, 903, 904, 906, 907, 909, 910, 912, 913, 915, 916, 918, 920, 921, 923, 924, 926, 927, 929, 930, 932, 933, 935, 936, 938, 939, 941, 943, 944, 946, 947, 949, 950, 952, 953, 955, 956, 958, 959, 961, 962, 964, 966, 967, 969, 970, 972, 973, 975, 976, 978, 979, 981, 982, 984, 985, 987, 989, 990, 992, 993, 995, 996, 998, 999, 1001, 1002, 1004, 1005, 1007, 1008, 1010, 1012, 1013, 1015, 1016, 1018, 1019, 1021, 1022, 1024, 1025, 1027, 1028, 1030, 1031, 1033, 1035, 1036, 1038, 1039, 1041, 1042, 1044, 1045, 1047, 1048, 1050, 1051, 1053, 1054, 1056, 1058, 1059, 1061, 1062, 1064, 1065, 1067, 1068, 1070, 1071, 1073, 1074, 1076, 1077, 1079, 1081, 1082, 1084, 1085, 1087, 1088, 1090, 1091, 1093, 1094, 1096, 1097, 1099, 1100, 1102, 1104, 1105, 1107, 1108, 1110, 1111, 1113, 1114, 1116, 1117, 1119, 1120, 1122, 1123, 1125, 1127, 1128, 1130, 1131, 1133, 1134, 1136, 1137, 1139, 1140, 1142, 1143, 1145, 1146, 1148, 1150, 1151, 1153, 1154, 1156, 1157, 1159, 1160, 1162, 1163, 1165, 1166, 1168, 1169, 1171, 1173, 1174, 1176, 1177, 1179, 1180, 1182, 1183, 1185, 1186, 1188, 1189, 1191, 1192, 1194, 1196, 1197, 1199, 1200, 1202, 1203, 1205, 1206, 1208, 1209, 1211, 1212, 1214, 1215, 1217, 1219, 1220, 1222, 1223, 1225, 1226, 1228, 1229, 1231, 1232, 1234, 1235, 1237, 1238, 1240, 1242, 1243, 1245, 1246, 1248, 1249, 1251, 1252, 1254, 1255, 1257, 1258, 1260, 1261, 1263, 1265, 1266, 1268, 1269, 1271, 1272, 1274, 1275, 1277, 1278, 1280, 1281, 1283, 1284, 1286, 1288, 1289, 1291, 1292, 1294, 1295, 1297, 1298, 1300, 1301, 1303, 1304, 1306, 1307, 1309, 1311, 1312, 1314, 1315, 1317, 1318, 1320, 1321, 1323, }
{883, 892, 901, 910, 920, 929, 938, 947, 956, 966, 975, 984, 993, 1002, 1012, 1021, 1030, 1039, 1048, 1058, 1067, 1076, 1085, 1094, 1104, 1113, 1122, 1131, 1140, 1150, 1159, 1168, 1177, 1186, 1196, 1205, 1214, 1223, 1232, 1242, 1251, 1260, 1269, 1278, 1288, 1297, 1306, 1315, }
{1324, 1326, 1327, 1329, 1330, 1332, 1334, 1335, 1337, 1338, 1340, 1341, 1343, 1344, 1346, 1347, 1349, 1350, 1352, 1353, 1355, 1357, 1358, 1360, 1361, 1363, 1364, 1366, 1367, 1369, 1370, 1372, 1373, 1375, 1376, 1378, 1380, 1381, 1383, 1384, 1386, 1387, 1389, 1390, 1392, 1393, 1395, 1396, 1398, 1399, 1401, 1403, 1404, 1406, 1407, 1409, 1410, 1412, 1413, 1415, 1416, 1418, 1419, 1421, 1422, 1424, 1426, 1427, 1429, 1430, 1432, 1433, 1435, 1436, 1438, 1439, 1441, 1442, 1444, 1445, 1447, 1449, 1450, 1452, 1453, 1455, 1456, 1458, 1459, 1461, 1462, 1464, 1465, 1467, 1468, 1470, }
{1324, 1327, 1330, 1334, 1337, 1340, 1343, 1346, 1349, 1352, 1355, 1358, 1361, 1364, 1367, 1370, 1373, 1376, 1380, 1383, 1386, 1389, 1392, 1395, 1398, 1401, 1404, 1407, 1410, 1413, 1416, 1419, 1422, 1426, 1429, 1432, 1435, 1438, 1441, 1444, 1447, 1450, 1453, 1456, 1459, 1462, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 8
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 8
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 8
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,8}
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,8}
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,8}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,8}
slicedInpSpikeIds.shapeToString(): {48,8}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,8}
slicedInpSpikeIds.shapeToString(): {48,8}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,8}
slicedInpSpikeIds.shapeToString(): {48,8}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {488,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{726, 725, 18, }
start_tile: 1
num_tiles_per_layer[ilay]: 726
start_tile + num_tiles_per_layer[ilay]: 727
num_occupied_tiles: 489
batchsize: 48
sparse_size: 8
dLdx.shapeToString(): {489,48,8}
replicated_numGrads.shapeToString(): {489,48}
exchangeGroupSize: 24
numExchangeGroups: 20
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
lastExchangeGroupLarger
start_id: 456, end_id: 489
exchangeGroupSizeLast: 33
exchangeBatchSizeLast: 2
start_tile: 727
num_tiles_per_layer[ilay]: 725
start_tile + num_tiles_per_layer[ilay]: 1452
num_occupied_tiles: 488
batchsize: 48
sparse_size: 8
dLdx.shapeToString(): {488,48,8}
replicated_numGrads.shapeToString(): {488,48}
exchangeGroupSize: 24
numExchangeGroups: 20
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
lastExchangeGroupLarger
start_id: 456, end_id: 488
exchangeGroupSizeLast: 32
exchangeBatchSizeLast: 2
start_tile: 1452
num_tiles_per_layer[ilay]: 18
start_tile + num_tiles_per_layer[ilay]: 1470
num_occupied_tiles: 5
batchsize: 48
sparse_size: 8
dLdx.shapeToString(): {5,48,8}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {488,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 19:11:59.275877: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[978 978 976  10]
0 489.0
1 489.0
2 488.0
3 5.0
[TileMapping(start_tile=1, end_tile=490), TileMapping(start_tile=490, end_tile=979), TileMapping(start_tile=979, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=490), TileMapping(start_tile=490, end_tile=979), TileMapping(start_tile=979, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.31330417999518295
limit=0.31330417999518295
limit=0.31362502409359
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 978) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 978) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 976) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_8_8_8_2_48_1_490_979_1467_490_979_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 8), (100, 4190734     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][3
                                                                 keras_multi_lif_layer_sparse[0][7
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 4,190,734
Trainable params: 4,181,908
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_8_8_8_2_48_1_490_979_1467_490_979_1467_1472
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_8_8_8_2_48_1_490_979_1467_490_979_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.165552/52 [==============================] - 720s 14s/step - loss: 2.1655
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 1.801552/52 [==============================] - 0s 6ms/step - loss: 1.8015
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 1.481652/52 [==============================] - 0s 6ms/step - loss: 1.4816
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 1.246152/52 [==============================] - 0s 5ms/step - loss: 1.2461
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 0.886452/52 [==============================] - 0s 5ms/step - loss: 0.8864
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 0.624252/52 [==============================] - 0s 5ms/step - loss: 0.6242
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 0.564352/52 [==============================] - 0s 5ms/step - loss: 0.5643
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 0.484652/52 [==============================] - 0s 5ms/step - loss: 0.4846
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 0.446252/52 [==============================] - 0s 5ms/step - loss: 0.4462
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 0.390852/52 [==============================] - 0s 5ms/step - loss: 0.3908
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 0.392752/52 [==============================] - 0s 6ms/step - loss: 0.3927
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 0.357652/52 [==============================] - 0s 5ms/step - loss: 0.3576
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 0.335652/52 [==============================] - 0s 5ms/step - loss: 0.3356
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 0.300452/52 [==============================] - 0s 5ms/step - loss: 0.3004
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 0.344352/52 [==============================] - 0s 5ms/step - loss: 0.3443
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 0.294452/52 [==============================] - 0s 5ms/step - loss: 0.2944
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 0.269652/52 [==============================] - 0s 5ms/step - loss: 0.2696
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 0.251252/52 [==============================] - 0s 5ms/step - loss: 0.2512
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 0.275752/52 [==============================] - 0s 7ms/step - loss: 0.2757
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 0.257652/52 [==============================] - 0s 5ms/step - loss: 0.2576
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 0.246352/52 [==============================] - 0s 5ms/step - loss: 0.2463
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 0.214352/52 [==============================] - 0s 5ms/step - loss: 0.2143
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 0.247952/52 [==============================] - 0s 5ms/step - loss: 0.2479
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 0.237652/52 [==============================] - 0s 5ms/step - loss: 0.2376
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 0.221852/52 [==============================] - 0s 5ms/step - loss: 0.2218

Final time:  726.3178517818451
2022-09-12 19:12:08.808171: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.01
DENSE_SIZES:  [2312, 734, 734, 732, 732, 10]
SPARSE_SIZES:  [32, 6, 6, 6, 6, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  4
SECOND_THRESHOLD:  0.98

60000
9984
[  607 37056  7445 ... 15581 26132 23095]
2022-09-12 19:12:17.462506: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 19:12:18.333700: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 11, 2, 1
Build_allocator: 16, 3, 1
Build_allocator: 12, 2, 2
Build_allocator: 17, 3, 2
Build_allocator: 13, 2, 3
Build_allocator: 18, 3, 3
Build_allocator: 14, 2, 4
Build_allocator: 19, 3, 4
Build_allocator: 5, 1, 0
Build_allocator: 6, 1, 1
Build_allocator: 7, 1, 2
Build_allocator: 8, 1, 3
Build_allocator: 9, 1, 4
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 4, 0, 4
Build_allocator: 20, 4, 0
Build_allocator: 21, 4, 1
Build_allocator: 22, 4, 2
Build_allocator: 23, 4, 3
Build_allocator: 24, 4, 4
Build_allocator: 25, 5, 0
Build_allocator: 26, 5, 1
Build_allocator: 27, 5, 2
Build_allocator: 28, 5, 3
Build_allocator: 29, 5, 4
Build_allocator: 10, 2, 0
Build_allocator: 15, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0x263d8840
1: &init_state[ilay]0x263d8848
2: &init_state[ilay]0x263d8850
3: &init_state[ilay]0x263d8858
4: &init_state[ilay]0x263d8860
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0
3: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
3: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 368, 735, 1101, 1467, }
end_tiles: {368, 735, 1101, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{2, 2, 2, 2, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 5, 6, 8, 10, 11, 13, 15, 17, 18, 20, 22, 23, 25, 27, 28, 30, 32, 34, 35, 37, 39, 40, 42, 44, 46, 47, 49, 51, 52, 54, 56, 57, 59, 61, 63, 64, 66, 68, 69, 71, 73, 74, 76, 78, 80, 81, 83, 85, 86, 88, 90, 92, 93, 95, 97, 98, 100, 102, 103, 105, 107, 109, 110, 112, 114, 115, 117, 119, 120, 122, 124, 126, 127, 129, 131, 132, 134, 136, 138, 139, 141, 143, 144, 146, 148, 149, 151, 153, 155, 156, 158, 160, 161, 163, 165, 166, 168, 170, 172, 173, 175, 177, 178, 180, 182, 184, 185, 187, 189, 190, 192, 194, 195, 197, 199, 201, 202, 204, 206, 207, 209, 211, 212, 214, 216, 218, 219, 221, 223, 224, 226, 228, 230, 231, 233, 235, 236, 238, 240, 241, 243, 245, 247, 248, 250, 252, 253, 255, 257, 258, 260, 262, 264, 265, 267, 269, 270, 272, 274, 276, 277, 279, 281, 282, 284, 286, 287, 289, 291, 293, 294, 296, 298, 299, 301, 303, 304, 306, 308, 310, 311, 313, 315, 316, 318, 320, 322, 323, 325, }
{0, 6, 13, 20, 27, 34, 40, 47, 54, 61, 68, 74, 81, 88, 95, 102, 109, 115, 122, 129, 136, 143, 149, 156, 163, 170, 177, 184, 190, 197, 204, 211, 218, 224, 231, 238, 245, 252, 258, 265, 272, 279, 286, 293, 299, 306, 313, 320, }
{327, 328, 330, 332, 333, 335, 337, 339, 340, 342, 344, 345, 347, 349, 350, 352, 354, 356, 357, 359, 361, 362, 364, 366, 368, 369, 371, 373, 374, 376, 378, 379, 381, 383, 385, 386, 388, 390, 391, 393, 395, 396, 398, 400, 402, 403, 405, 407, 408, 410, 412, 414, 415, 417, 419, 420, 422, 424, 425, 427, 429, 431, 432, 434, 436, 437, 439, 441, 442, 444, 446, 448, 449, 451, 453, 454, 456, 458, 460, 461, 463, 465, 466, 468, 470, 471, 473, 475, 477, 478, 480, 482, 483, 485, 487, 488, 490, 492, 494, 495, 497, 499, 500, 502, 504, 506, 507, 509, 511, 512, 514, 516, 517, 519, 521, 523, 524, 526, 528, 529, 531, 533, 534, 536, 538, 540, 541, 543, 545, 546, 548, 550, 552, 553, 555, 557, 558, 560, 562, 563, 565, 567, 569, 570, 572, 574, 575, 577, 579, 580, 582, 584, 586, 587, 589, 591, 592, 594, 596, 598, 599, 601, 603, 604, 606, 608, 609, 611, 613, 615, 616, 618, 620, 621, 623, 625, 626, 628, 630, 632, 633, 635, 637, 638, 640, 642, 644, 645, 647, 649, 650, 652, }
{327, 333, 340, 347, 354, 361, 368, 374, 381, 388, 395, 402, 408, 415, 422, 429, 436, 442, 449, 456, 463, 470, 477, 483, 490, 497, 504, 511, 517, 524, 531, 538, 545, 552, 558, 565, 572, 579, 586, 592, 599, 606, 613, 620, 626, 633, 640, 647, }
{654, 655, 657, 659, 661, 662, 664, 666, 667, 669, 671, 672, 674, 676, 678, 679, 681, 683, 684, 686, 688, 690, 691, 693, 695, 696, 698, 700, 701, 703, 705, 707, 708, 710, 712, 713, 715, 717, 718, 720, 722, 724, 725, 727, 729, 730, 732, 734, 736, 737, 739, 741, 742, 744, 746, 747, 749, 751, 753, 754, 756, 758, 759, 761, 763, 764, 766, 768, 770, 771, 773, 775, 776, 778, 780, 782, 783, 785, 787, 788, 790, 792, 793, 795, 797, 799, 800, 802, 804, 805, 807, 809, 810, 812, 814, 816, 817, 819, 821, 822, 824, 826, 828, 829, 831, 833, 834, 836, 838, 839, 841, 843, 845, 846, 848, 850, 851, 853, 855, 856, 858, 860, 862, 863, 865, 867, 868, 870, 872, 874, 875, 877, 879, 880, 882, 884, 885, 887, 889, 891, 892, 894, 896, 897, 899, 901, 902, 904, 906, 908, 909, 911, 913, 914, 916, 918, 920, 921, 923, 925, 926, 928, 930, 931, 933, 935, 937, 938, 940, 942, 943, 945, 947, 948, 950, 952, 954, 955, 957, 959, 960, 962, 964, 966, 967, 969, 971, 972, 974, 976, 977, 979, }
{654, 661, 667, 674, 681, 688, 695, 701, 708, 715, 722, 729, 736, 742, 749, 756, 763, 770, 776, 783, 790, 797, 804, 810, 817, 824, 831, 838, 845, 851, 858, 865, 872, 879, 885, 892, 899, 906, 913, 920, 926, 933, 940, 947, 954, 960, 967, 974, }
{981, 983, 984, 986, 988, 989, 991, 993, 994, 996, 998, 1000, 1001, 1003, 1005, 1006, 1008, 1010, 1012, 1013, 1015, 1017, 1018, 1020, 1022, 1023, 1025, 1027, 1029, 1030, 1032, 1034, 1035, 1037, 1039, 1040, 1042, 1044, 1046, 1047, 1049, 1051, 1052, 1054, 1056, 1058, 1059, 1061, 1063, 1064, 1066, 1068, 1069, 1071, 1073, 1075, 1076, 1078, 1080, 1081, 1083, 1085, 1086, 1088, 1090, 1092, 1093, 1095, 1097, 1098, 1100, 1102, 1104, 1105, 1107, 1109, 1110, 1112, 1114, 1115, 1117, 1119, 1121, 1122, 1124, 1126, 1127, 1129, 1131, 1132, 1134, 1136, 1138, 1139, 1141, 1143, 1144, 1146, 1148, 1150, 1151, 1153, 1155, 1156, 1158, 1160, 1161, 1163, 1165, 1167, 1168, 1170, 1172, 1173, 1175, 1177, 1178, 1180, 1182, 1184, 1185, 1187, 1189, 1190, 1192, 1194, 1196, 1197, 1199, 1201, 1202, 1204, 1206, 1207, 1209, 1211, 1213, 1214, 1216, 1218, 1219, 1221, 1223, 1224, 1226, 1228, 1230, 1231, 1233, 1235, 1236, 1238, 1240, 1242, 1243, 1245, 1247, 1248, 1250, 1252, 1253, 1255, 1257, 1259, 1260, 1262, 1264, 1265, 1267, 1269, 1270, 1272, 1274, 1276, 1277, 1279, 1281, 1282, 1284, 1286, 1288, 1289, 1291, 1293, 1294, 1296, 1298, 1299, 1301, 1303, 1305, 1306, }
{981, 988, 994, 1001, 1008, 1015, 1022, 1029, 1035, 1042, 1049, 1056, 1063, 1069, 1076, 1083, 1090, 1097, 1104, 1110, 1117, 1124, 1131, 1138, 1144, 1151, 1158, 1165, 1172, 1178, 1185, 1192, 1199, 1206, 1213, 1219, 1226, 1233, 1240, 1247, 1253, 1260, 1267, 1274, 1281, 1288, 1294, 1301, }
{1308, 1310, 1311, 1313, 1315, 1316, 1318, 1320, 1322, 1323, 1325, 1327, 1328, 1330, 1332, 1334, 1335, 1337, 1339, 1340, 1342, 1344, 1345, 1347, 1349, 1351, 1352, 1354, 1356, 1357, 1359, 1361, 1362, 1364, 1366, 1368, 1369, 1371, 1373, 1374, 1376, 1378, 1380, 1381, 1383, 1385, 1386, 1388, 1390, 1391, 1393, 1395, 1397, 1398, 1400, 1402, 1403, 1405, 1407, 1408, 1410, 1412, 1414, 1415, 1417, 1419, 1420, 1422, 1424, 1426, 1427, 1429, 1431, 1432, 1434, 1436, 1437, 1439, 1441, 1443, 1444, 1446, 1448, 1449, 1451, 1453, 1454, 1456, 1458, 1460, 1461, 1463, 1465, 1466, 1468, 1470, }
{1308, 1311, 1315, 1318, 1322, 1325, 1328, 1332, 1335, 1339, 1342, 1345, 1349, 1352, 1356, 1359, 1362, 1366, 1369, 1373, 1376, 1380, 1383, 1386, 1390, 1393, 1397, 1400, 1403, 1407, 1410, 1414, 1417, 1420, 1424, 1427, 1431, 1434, 1437, 1441, 1444, 1448, 1451, 1454, 1458, 1461, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 62
sparse_size_to_use: 6
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 62
sparse_size_to_use: 6
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 61
sparse_size_to_use: 6
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 61
sparse_size_to_use: 6
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,6}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,6}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,6}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,6}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 4
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,6}
slicedInpSpikeIds.shapeToString(): {48,6}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,6}
slicedInpSpikeIds.shapeToString(): {48,6}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,6}
slicedInpSpikeIds.shapeToString(): {48,6}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

3
out_spike_ids[ilay].shapeToString(): {100,48,6}
slicedInpSpikeIds.shapeToString(): {48,6}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,2}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{485, 484, 484, 15, }
start_tile: 1
num_tiles_per_layer[ilay]: 485
start_tile + num_tiles_per_layer[ilay]: 486
num_occupied_tiles: 367
batchsize: 48
sparse_size: 6
dLdx.shapeToString(): {367,48,6}
replicated_numGrads.shapeToString(): {367,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 367
exchangeGroupSizeLast: 31
exchangeBatchSizeLast: 2
start_tile: 486
num_tiles_per_layer[ilay]: 484
start_tile + num_tiles_per_layer[ilay]: 970
num_occupied_tiles: 366
batchsize: 48
sparse_size: 6
dLdx.shapeToString(): {366,48,6}
replicated_numGrads.shapeToString(): {366,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 366
exchangeGroupSizeLast: 30
exchangeBatchSizeLast: 2
start_tile: 970
num_tiles_per_layer[ilay]: 484
start_tile + num_tiles_per_layer[ilay]: 1454
num_occupied_tiles: 366
batchsize: 48
sparse_size: 6
dLdx.shapeToString(): {366,48,6}
replicated_numGrads.shapeToString(): {366,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 366
exchangeGroupSizeLast: 30
exchangeBatchSizeLast: 2
start_tile: 1454
num_tiles_per_layer[ilay]: 15
start_tile + num_tiles_per_layer[ilay]: 1469
num_occupied_tiles: 5
batchsize: 48
sparse_size: 6
dLdx.shapeToString(): {5,48,6}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,1}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 19:22:19.077819: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[734 734 732 732  10]
0 367.0
1 367.0
2 366.0
3 366.0
4 5.0
[TileMapping(start_tile=1, end_tile=368), TileMapping(start_tile=368, end_tile=735), TileMapping(start_tile=735, end_tile=1101), TileMapping(start_tile=1101, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=368), TileMapping(start_tile=368, end_tile=735), TileMapping(start_tile=735, end_tile=1101), TileMapping(start_tile=1101, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.3616492648073473
limit=0.3616492648073473
limit=0.3621429841700741
limit=0.3621429841700741
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 734) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 734) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 732) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 732) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_5:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_6_6_6_6_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 6), (100, 3325022     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][4
                                                                 keras_multi_lif_layer_sparse[0][9
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 3,325,022
Trainable params: 3,316,196
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_6_6_6_6_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_6_6_6_6_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.338352/52 [==============================] - 603s 12s/step - loss: 2.3383
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.005852/52 [==============================] - 0s 5ms/step - loss: 2.0058
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 1.914552/52 [==============================] - 0s 5ms/step - loss: 1.9145
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 1.888852/52 [==============================] - 0s 5ms/step - loss: 1.8888
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 1.895552/52 [==============================] - 0s 5ms/step - loss: 1.8955
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 1.942852/52 [==============================] - 0s 5ms/step - loss: 1.9428
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 1.972552/52 [==============================] - 0s 5ms/step - loss: 1.9725
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 1.951952/52 [==============================] - 0s 5ms/step - loss: 1.9519
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 1.942552/52 [==============================] - 0s 5ms/step - loss: 1.9425
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 1.962752/52 [==============================] - 0s 5ms/step - loss: 1.9627
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 1.945552/52 [==============================] - 0s 5ms/step - loss: 1.9455
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 1.957352/52 [==============================] - 0s 5ms/step - loss: 1.9573
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 1.939052/52 [==============================] - 0s 6ms/step - loss: 1.9390
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 1.946452/52 [==============================] - 0s 5ms/step - loss: 1.9464
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 1.919052/52 [==============================] - 0s 5ms/step - loss: 1.9190
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 1.935352/52 [==============================] - 0s 5ms/step - loss: 1.9353
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 1.920852/52 [==============================] - 0s 5ms/step - loss: 1.9208
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 1.932052/52 [==============================] - 0s 5ms/step - loss: 1.9320
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 1.924452/52 [==============================] - 0s 5ms/step - loss: 1.9244
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 1.940752/52 [==============================] - 0s 5ms/step - loss: 1.9407
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 1.920152/52 [==============================] - 0s 5ms/step - loss: 1.9201
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 1.938252/52 [==============================] - 0s 5ms/step - loss: 1.9382
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 1.922352/52 [==============================] - 0s 5ms/step - loss: 1.9223
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 1.940552/52 [==============================] - 0s 5ms/step - loss: 1.9405
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 1.926352/52 [==============================] - 0s 5ms/step - loss: 1.9263

Final time:  608.7761549949646
2022-09-12 19:22:28.393902: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.01
DENSE_SIZES:  [2312, 588, 586, 586, 586, 586, 10]
SPARSE_SIZES:  [32, 4, 4, 4, 4, 4, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  5
SECOND_THRESHOLD:  0.98

60000
9984
[44341  6109 18341 ... 16395 27691 19486]
2022-09-12 19:22:37.664903: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 19:22:38.551381: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 13, 2, 1
Build_allocator: 19, 3, 1
Build_allocator: 14, 2, 2
Build_allocator: 20, 3, 2
Build_allocator: 15, 2, 3
Build_allocator: 21, 3, 3
Build_allocator: 16, 2, 4
Build_allocator: 22, 3, 4
Build_allocator: 17, 2, 5
Build_allocator: 23, 3, 5
Build_allocator: 6, 1, 0
Build_allocator: 7, 1, 1
Build_allocator: 8, 1, 2
Build_allocator: 9, 1, 3
Build_allocator: 10, 1, 4
Build_allocator: 11, 1, 5
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 4, 0, 4
Build_allocator: 5, 0, 5
Build_allocator: 24, 4, 0
Build_allocator: 25, 4, 1
Build_allocator: 26, 4, 2
Build_allocator: 27, 4, 3
Build_allocator: 28, 4, 4
Build_allocator: 29, 4, 5
Build_allocator: 30, 5, 0
Build_allocator: 31, 5, 1
Build_allocator: 32, 5, 2
Build_allocator: 33, 5, 3
Build_allocator: 34, 5, 4
Build_allocator: 35, 5, 5
Build_allocator: 12, 2, 0
Build_allocator: 18, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0x1cd4cfc0
1: &init_state[ilay]0x1cd4cfc8
2: &init_state[ilay]0x1cd4cfd0
3: &init_state[ilay]0x1cd4cfd8
4: &init_state[ilay]0x1cd4cfe0
5: &init_state[ilay]0x1cd4cfe8
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0
3: init_state[ilay]==init_state[ilay+1]: 0
4: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
3: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
4: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 295, 588, 881, 1174, 1467, }
end_tiles: {295, 588, 881, 1174, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{2, 2, 2, 2, 2, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 2, 4, 5, 6, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 25, 26, 27, 29, 30, 32, 33, 34, 36, 37, 39, 40, 41, 43, 44, 46, 47, 48, 50, 51, 52, 54, 55, 57, 58, 59, 61, 62, 64, 65, 66, 68, 69, 71, 72, 73, 75, 76, 78, 79, 80, 82, 83, 85, 86, 87, 89, 90, 92, 93, 94, 96, 97, 98, 100, 101, 103, 104, 105, 107, 108, 110, 111, 112, 114, 115, 117, 118, 119, 121, 122, 124, 125, 126, 128, 129, 131, 132, 133, 135, 136, 138, 139, 140, 142, 143, 144, 146, 147, 149, 150, 151, 153, 154, 156, 157, 158, 160, 161, 163, 164, 165, 167, 168, 170, 171, 172, 174, 175, 177, 178, 179, 181, 182, 184, 185, 186, 188, 189, 190, 192, 193, 195, 196, 197, 199, 200, 202, 203, 204, 206, 207, 209, 210, 211, 213, 214, 216, 217, 218, 220, 221, 223, 224, 225, 227, 228, 230, 231, 232, 234, 235, 236, 238, 239, 241, 242, 243, 245, 246, 248, 249, 250, 252, 253, 255, 256, 257, 259, 260, 262, 263, 264, 266, }
{0, 5, 11, 16, 22, 27, 33, 39, 44, 50, 55, 61, 66, 72, 78, 83, 89, 94, 100, 105, 111, 117, 122, 128, 133, 139, 144, 150, 156, 161, 167, 172, 178, 184, 189, 195, 200, 206, 211, 217, 223, 228, 234, 239, 245, 250, 256, 262, }
{267, 269, 270, 271, 273, 274, 276, 277, 278, 280, 281, 282, 284, 285, 287, 288, 289, 291, 292, 294, 295, 296, 298, 299, 301, 302, 303, 305, 306, 308, 309, 310, 312, 313, 315, 316, 317, 319, 320, 322, 323, 324, 326, 327, 328, 330, 331, 333, 334, 335, 337, 338, 340, 341, 342, 344, 345, 347, 348, 349, 351, 352, 354, 355, 356, 358, 359, 361, 362, 363, 365, 366, 368, 369, 370, 372, 373, 374, 376, 377, 379, 380, 381, 383, 384, 386, 387, 388, 390, 391, 393, 394, 395, 397, 398, 400, 401, 402, 404, 405, 407, 408, 409, 411, 412, 414, 415, 416, 418, 419, 420, 422, 423, 425, 426, 427, 429, 430, 432, 433, 434, 436, 437, 439, 440, 441, 443, 444, 446, 447, 448, 450, 451, 453, 454, 455, 457, 458, 460, 461, 462, 464, 465, 466, 468, 469, 471, 472, 473, 475, 476, 478, 479, 480, 482, 483, 485, 486, 487, 489, 490, 492, 493, 494, 496, 497, 499, 500, 501, 503, 504, 506, 507, 508, 510, 511, 512, 514, 515, 517, 518, 519, 521, 522, 524, 525, 526, 528, 529, 531, 532, 533, }
{267, 273, 278, 284, 289, 295, 301, 306, 312, 317, 323, 328, 334, 340, 345, 351, 356, 362, 368, 373, 379, 384, 390, 395, 401, 407, 412, 418, 423, 429, 434, 440, 446, 451, 457, 462, 468, 473, 479, 485, 490, 496, 501, 507, 512, 518, 524, 529, }
{535, 536, 538, 539, 540, 542, 543, 545, 546, 547, 549, 550, 552, 553, 554, 556, 557, 558, 560, 561, 563, 564, 565, 567, 568, 570, 571, 572, 574, 575, 577, 578, 579, 581, 582, 584, 585, 586, 588, 589, 591, 592, 593, 595, 596, 598, 599, 600, 602, 603, 604, 606, 607, 609, 610, 611, 613, 614, 616, 617, 618, 620, 621, 623, 624, 625, 627, 628, 630, 631, 632, 634, 635, 637, 638, 639, 641, 642, 644, 645, 646, 648, 649, 650, 652, 653, 655, 656, 657, 659, 660, 662, 663, 664, 666, 667, 669, 670, 671, 673, 674, 676, 677, 678, 680, 681, 683, 684, 685, 687, 688, 690, 691, 692, 694, 695, 696, 698, 699, 701, 702, 703, 705, 706, 708, 709, 710, 712, 713, 715, 716, 717, 719, 720, 722, 723, 724, 726, 727, 729, 730, 731, 733, 734, 736, 737, 738, 740, 741, 742, 744, 745, 747, 748, 749, 751, 752, 754, 755, 756, 758, 759, 761, 762, 763, 765, 766, 768, 769, 770, 772, 773, 775, 776, 777, 779, 780, 782, 783, 784, 786, 787, 788, 790, 791, 793, 794, 795, 797, 798, 800, 801, }
{535, 540, 546, 552, 557, 563, 568, 574, 579, 585, 591, 596, 602, 607, 613, 618, 624, 630, 635, 641, 646, 652, 657, 663, 669, 674, 680, 685, 691, 696, 702, 708, 713, 719, 724, 730, 736, 741, 747, 752, 758, 763, 769, 775, 780, 786, 791, 797, }
{802, 804, 805, 807, 808, 809, 811, 812, 814, 815, 816, 818, 819, 821, 822, 823, 825, 826, 828, 829, 830, 832, 833, 834, 836, 837, 839, 840, 841, 843, 844, 846, 847, 848, 850, 851, 853, 854, 855, 857, 858, 860, 861, 862, 864, 865, 867, 868, 869, 871, 872, 874, 875, 876, 878, 879, 880, 882, 883, 885, 886, 887, 889, 890, 892, 893, 894, 896, 897, 899, 900, 901, 903, 904, 906, 907, 908, 910, 911, 913, 914, 915, 917, 918, 920, 921, 922, 924, 925, 926, 928, 929, 931, 932, 933, 935, 936, 938, 939, 940, 942, 943, 945, 946, 947, 949, 950, 952, 953, 954, 956, 957, 959, 960, 961, 963, 964, 966, 967, 968, 970, 971, 972, 974, 975, 977, 978, 979, 981, 982, 984, 985, 986, 988, 989, 991, 992, 993, 995, 996, 998, 999, 1000, 1002, 1003, 1005, 1006, 1007, 1009, 1010, 1012, 1013, 1014, 1016, 1017, 1018, 1020, 1021, 1023, 1024, 1025, 1027, 1028, 1030, 1031, 1032, 1034, 1035, 1037, 1038, 1039, 1041, 1042, 1044, 1045, 1046, 1048, 1049, 1051, 1052, 1053, 1055, 1056, 1058, 1059, 1060, 1062, 1063, 1064, 1066, 1067, 1069, }
{802, 808, 814, 819, 825, 830, 836, 841, 847, 853, 858, 864, 869, 875, 880, 886, 892, 897, 903, 908, 914, 920, 925, 931, 936, 942, 947, 953, 959, 964, 970, 975, 981, 986, 992, 998, 1003, 1009, 1014, 1020, 1025, 1031, 1037, 1042, 1048, 1053, 1059, 1064, }
{1070, 1071, 1073, 1074, 1076, 1077, 1078, 1080, 1081, 1083, 1084, 1085, 1087, 1088, 1090, 1091, 1092, 1094, 1095, 1097, 1098, 1099, 1101, 1102, 1104, 1105, 1106, 1108, 1109, 1110, 1112, 1113, 1115, 1116, 1117, 1119, 1120, 1122, 1123, 1124, 1126, 1127, 1129, 1130, 1131, 1133, 1134, 1136, 1137, 1138, 1140, 1141, 1143, 1144, 1145, 1147, 1148, 1150, 1151, 1152, 1154, 1155, 1156, 1158, 1159, 1161, 1162, 1163, 1165, 1166, 1168, 1169, 1170, 1172, 1173, 1175, 1176, 1177, 1179, 1180, 1182, 1183, 1184, 1186, 1187, 1189, 1190, 1191, 1193, 1194, 1196, 1197, 1198, 1200, 1201, 1202, 1204, 1205, 1207, 1208, 1209, 1211, 1212, 1214, 1215, 1216, 1218, 1219, 1221, 1222, 1223, 1225, 1226, 1228, 1229, 1230, 1232, 1233, 1235, 1236, 1237, 1239, 1240, 1242, 1243, 1244, 1246, 1247, 1248, 1250, 1251, 1253, 1254, 1255, 1257, 1258, 1260, 1261, 1262, 1264, 1265, 1267, 1268, 1269, 1271, 1272, 1274, 1275, 1276, 1278, 1279, 1281, 1282, 1283, 1285, 1286, 1288, 1289, 1290, 1292, 1293, 1294, 1296, 1297, 1299, 1300, 1301, 1303, 1304, 1306, 1307, 1308, 1310, 1311, 1313, 1314, 1315, 1317, 1318, 1320, 1321, 1322, 1324, 1325, 1327, 1328, 1329, 1331, 1332, 1334, 1335, 1336, }
{1070, 1076, 1081, 1087, 1092, 1098, 1104, 1109, 1115, 1120, 1126, 1131, 1137, 1143, 1148, 1154, 1159, 1165, 1170, 1176, 1182, 1187, 1193, 1198, 1204, 1209, 1215, 1221, 1226, 1232, 1237, 1243, 1248, 1254, 1260, 1265, 1271, 1276, 1282, 1288, 1293, 1299, 1304, 1310, 1315, 1321, 1327, 1332, }
{1338, 1339, 1340, 1342, 1343, 1345, 1346, 1347, 1349, 1350, 1352, 1353, 1354, 1356, 1357, 1359, 1360, 1361, 1363, 1364, 1366, 1367, 1368, 1370, 1371, 1373, 1374, 1375, 1377, 1378, 1380, 1381, 1382, 1384, 1385, 1386, 1388, 1389, 1391, 1392, 1393, 1395, 1396, 1398, 1399, 1400, 1402, 1403, 1405, 1406, 1407, 1409, 1410, 1412, 1413, 1414, 1416, 1417, 1419, 1420, 1421, 1423, 1424, 1426, 1427, 1428, 1430, 1431, 1432, 1434, 1435, 1437, 1438, 1439, 1441, 1442, 1444, 1445, 1446, 1448, 1449, 1451, 1452, 1453, 1455, 1456, 1458, 1459, 1460, 1462, 1463, 1465, 1466, 1467, 1469, 1470, }
{1338, 1340, 1343, 1346, 1349, 1352, 1354, 1357, 1360, 1363, 1366, 1368, 1371, 1374, 1377, 1380, 1382, 1385, 1388, 1391, 1393, 1396, 1399, 1402, 1405, 1407, 1410, 1413, 1416, 1419, 1421, 1424, 1427, 1430, 1432, 1435, 1438, 1441, 1444, 1446, 1449, 1452, 1455, 1458, 1460, 1463, 1466, 1469, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 4
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 4
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 4
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 4
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 4
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,4}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,4}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,4}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,4}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,4}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 4
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 5
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,4}
slicedInpSpikeIds.shapeToString(): {48,4}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,4}
slicedInpSpikeIds.shapeToString(): {48,4}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,4}
slicedInpSpikeIds.shapeToString(): {48,4}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

3
out_spike_ids[ilay].shapeToString(): {100,48,4}
slicedInpSpikeIds.shapeToString(): {48,4}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

4
out_spike_ids[ilay].shapeToString(): {100,48,4}
slicedInpSpikeIds.shapeToString(): {48,4}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {294,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 5
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{364, 364, 364, 364, 14, }
start_tile: 1
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 365
num_occupied_tiles: 293
batchsize: 48
sparse_size: 4
dLdx.shapeToString(): {293,48,4}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 365
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 729
num_occupied_tiles: 293
batchsize: 48
sparse_size: 4
dLdx.shapeToString(): {293,48,4}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 729
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 1093
num_occupied_tiles: 293
batchsize: 48
sparse_size: 4
dLdx.shapeToString(): {293,48,4}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 1093
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 1457
num_occupied_tiles: 293
batchsize: 48
sparse_size: 4
dLdx.shapeToString(): {293,48,4}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 1457
num_tiles_per_layer[ilay]: 14
start_tile + num_tiles_per_layer[ilay]: 1471
num_occupied_tiles: 5
batchsize: 48
sparse_size: 4
dLdx.shapeToString(): {5,48,4}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {294,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 5
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 19:30:41.166844: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[588 586 586 586 586  10]
0 294.0
1 293.0
2 293.0
3 293.0
4 293.0
5 5.0
[TileMapping(start_tile=1, end_tile=295), TileMapping(start_tile=295, end_tile=588), TileMapping(start_tile=588, end_tile=881), TileMapping(start_tile=881, end_tile=1174), TileMapping(start_tile=1174, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=295), TileMapping(start_tile=295, end_tile=588), TileMapping(start_tile=588, end_tile=881), TileMapping(start_tile=881, end_tile=1174), TileMapping(start_tile=1174, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.40406101782088427
limit=0.4047499544129309
limit=0.4047499544129309
limit=0.4047499544129309
limit=0.4047499544129309
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 588) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 586) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 586) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 586) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_5:0' shape=(2, 586) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_6:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_4_4_4_4_4_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 4), (100, 2748898     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][5
                                                                 keras_multi_lif_layer_sparse[0][1
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 2,748,898
Trainable params: 2,740,072
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_4_4_4_4_4_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_4_4_4_4_4_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.354152/52 [==============================] - 484s 9s/step - loss: 2.3541
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.300952/52 [==============================] - 0s 5ms/step - loss: 2.3009
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 2.219952/52 [==============================] - 0s 4ms/step - loss: 2.2199
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 2.140652/52 [==============================] - 0s 4ms/step - loss: 2.1406
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 2.135552/52 [==============================] - 0s 4ms/step - loss: 2.1355
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 2.168852/52 [==============================] - 0s 4ms/step - loss: 2.1688
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 2.160752/52 [==============================] - 0s 4ms/step - loss: 2.1607
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 2.171352/52 [==============================] - 0s 4ms/step - loss: 2.1713
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 2.154652/52 [==============================] - 0s 4ms/step - loss: 2.1546
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 2.182252/52 [==============================] - 0s 4ms/step - loss: 2.1822
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 2.162152/52 [==============================] - 0s 4ms/step - loss: 2.1621
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 2.169452/52 [==============================] - 0s 4ms/step - loss: 2.1694
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 2.164452/52 [==============================] - 0s 4ms/step - loss: 2.1644
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 2.186052/52 [==============================] - 0s 4ms/step - loss: 2.1860
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 2.053752/52 [==============================] - 0s 4ms/step - loss: 2.0537
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 2.042652/52 [==============================] - 0s 4ms/step - loss: 2.0426
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 2.017652/52 [==============================] - 0s 5ms/step - loss: 2.0176
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 2.047052/52 [==============================] - 0s 4ms/step - loss: 2.0470
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 1.999952/52 [==============================] - 0s 4ms/step - loss: 1.9999
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 2.036752/52 [==============================] - 0s 5ms/step - loss: 2.0367
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 2.002052/52 [==============================] - 0s 5ms/step - loss: 2.0020
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 2.055352/52 [==============================] - 0s 5ms/step - loss: 2.0553
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 2.011052/52 [==============================] - 0s 4ms/step - loss: 2.0110
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 2.045652/52 [==============================] - 0s 5ms/step - loss: 2.0456
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 2.034752/52 [==============================] - 0s 5ms/step - loss: 2.0347

Final time:  489.99461030960083
2022-09-12 19:30:49.338075: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.05
DENSE_SIZES:  [2312, 1466, 1466, 10]
SPARSE_SIZES:  [32, 72, 72, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  2
SECOND_THRESHOLD:  0.9

60000
9984
[48844 10336 59367 ... 57260 19800 32321]
2022-09-12 19:30:57.940266: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 19:30:58.829897: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 7, 2, 1
Build_allocator: 10, 3, 1
Build_allocator: 8, 2, 2
Build_allocator: 11, 3, 2
Build_allocator: 3, 1, 0
Build_allocator: 4, 1, 1
Build_allocator: 5, 1, 2
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 12, 4, 0
Build_allocator: 13, 4, 1
Build_allocator: 14, 4, 2
Build_allocator: 15, 5, 0
Build_allocator: 16, 5, 1
Build_allocator: 17, 5, 2
Build_allocator: 6, 2, 0
Build_allocator: 9, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0x5c0f5b0
1: &init_state[ilay]0x5c0f5b8
2: &init_state[ilay]0x5c0f5c0
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 734, 1467, }
end_tiles: {734, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{4, 4, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 5, 6, 8, 10, 11, 13, 15, 17, 18, 20, 22, 23, 25, 27, 28, 30, 32, 34, 35, 37, 39, 40, 42, 44, 46, 47, 49, 51, 52, 54, 56, 57, 59, 61, 63, 64, 66, 68, 69, 71, 73, 74, 76, 78, 80, 81, 83, 85, 86, 88, 90, 92, 93, 95, 97, 98, 100, 102, 103, 105, 107, 109, 110, 112, 114, 115, 117, 119, 120, 122, 124, 126, 127, 129, 131, 132, 134, 136, 138, 139, 141, 143, 144, 146, 148, 149, 151, 153, 155, 156, 158, 160, 161, 163, 165, 166, 168, 170, 172, 173, 175, 177, 178, 180, 182, 184, 185, 187, 189, 190, 192, 194, 195, 197, 199, 201, 202, 204, 206, 207, 209, 211, 212, 214, 216, 218, 219, 221, 223, 224, 226, 228, 230, 231, 233, 235, 236, 238, 240, 241, 243, 245, 247, 248, 250, 252, 253, 255, 257, 258, 260, 262, 264, 265, 267, 269, 270, 272, 274, 276, 277, 279, 281, 282, 284, 286, 287, 289, 291, 293, 294, 296, 298, 299, 301, 303, 304, 306, 308, 310, 311, 313, 315, 316, 318, 320, 322, 323, 325, 327, 328, 330, 332, 333, 335, 337, 339, 340, 342, 344, 345, 347, 349, 350, 352, 354, 356, 357, 359, 361, 362, 364, 366, 368, 369, 371, 373, 374, 376, 378, 379, 381, 383, 385, 386, 388, 390, 391, 393, 395, 396, 398, 400, 402, 403, 405, 407, 408, 410, 412, 414, 415, 417, 419, 420, 422, 424, 425, 427, 429, 431, 432, 434, 436, 437, 439, 441, 442, 444, 446, 448, 449, 451, 453, 454, 456, 458, 460, 461, 463, 465, 466, 468, 470, 471, 473, 475, 477, 478, 480, 482, 483, 485, 487, 488, 490, 492, 494, 495, 497, 499, 500, 502, 504, 506, 507, 509, 511, 512, 514, 516, 517, 519, 521, 523, 524, 526, 528, 529, 531, 533, 534, 536, 538, 540, 541, 543, 545, 546, 548, 550, 552, 553, 555, 557, 558, 560, 562, 563, 565, 567, 569, 570, 572, 574, 575, 577, 579, 580, 582, 584, 586, 587, 589, 591, 592, 594, 596, 598, 599, 601, 603, 604, 606, 608, 609, 611, 613, 615, 616, 618, 620, 621, 623, 625, 626, 628, 630, 632, 633, 635, 637, 638, 640, 642, 644, 645, 647, 649, 650, 652, }
{0, 13, 27, 40, 54, 68, 81, 95, 109, 122, 136, 149, 163, 177, 190, 204, 218, 231, 245, 258, 272, 286, 299, 313, 327, 340, 354, 368, 381, 395, 408, 422, 436, 449, 463, 477, 490, 504, 517, 531, 545, 558, 572, 586, 599, 613, 626, 640, }
{654, 655, 657, 659, 661, 662, 664, 666, 667, 669, 671, 672, 674, 676, 678, 679, 681, 683, 684, 686, 688, 690, 691, 693, 695, 696, 698, 700, 701, 703, 705, 707, 708, 710, 712, 713, 715, 717, 718, 720, 722, 724, 725, 727, 729, 730, 732, 734, 736, 737, 739, 741, 742, 744, 746, 747, 749, 751, 753, 754, 756, 758, 759, 761, 763, 764, 766, 768, 770, 771, 773, 775, 776, 778, 780, 782, 783, 785, 787, 788, 790, 792, 793, 795, 797, 799, 800, 802, 804, 805, 807, 809, 810, 812, 814, 816, 817, 819, 821, 822, 824, 826, 828, 829, 831, 833, 834, 836, 838, 839, 841, 843, 845, 846, 848, 850, 851, 853, 855, 856, 858, 860, 862, 863, 865, 867, 868, 870, 872, 874, 875, 877, 879, 880, 882, 884, 885, 887, 889, 891, 892, 894, 896, 897, 899, 901, 902, 904, 906, 908, 909, 911, 913, 914, 916, 918, 920, 921, 923, 925, 926, 928, 930, 931, 933, 935, 937, 938, 940, 942, 943, 945, 947, 948, 950, 952, 954, 955, 957, 959, 960, 962, 964, 966, 967, 969, 971, 972, 974, 976, 977, 979, 981, 983, 984, 986, 988, 989, 991, 993, 994, 996, 998, 1000, 1001, 1003, 1005, 1006, 1008, 1010, 1012, 1013, 1015, 1017, 1018, 1020, 1022, 1023, 1025, 1027, 1029, 1030, 1032, 1034, 1035, 1037, 1039, 1040, 1042, 1044, 1046, 1047, 1049, 1051, 1052, 1054, 1056, 1058, 1059, 1061, 1063, 1064, 1066, 1068, 1069, 1071, 1073, 1075, 1076, 1078, 1080, 1081, 1083, 1085, 1086, 1088, 1090, 1092, 1093, 1095, 1097, 1098, 1100, 1102, 1104, 1105, 1107, 1109, 1110, 1112, 1114, 1115, 1117, 1119, 1121, 1122, 1124, 1126, 1127, 1129, 1131, 1132, 1134, 1136, 1138, 1139, 1141, 1143, 1144, 1146, 1148, 1150, 1151, 1153, 1155, 1156, 1158, 1160, 1161, 1163, 1165, 1167, 1168, 1170, 1172, 1173, 1175, 1177, 1178, 1180, 1182, 1184, 1185, 1187, 1189, 1190, 1192, 1194, 1196, 1197, 1199, 1201, 1202, 1204, 1206, 1207, 1209, 1211, 1213, 1214, 1216, 1218, 1219, 1221, 1223, 1224, 1226, 1228, 1230, 1231, 1233, 1235, 1236, 1238, 1240, 1242, 1243, 1245, 1247, 1248, 1250, 1252, 1253, 1255, 1257, 1259, 1260, 1262, 1264, 1265, 1267, 1269, 1270, 1272, 1274, 1276, 1277, 1279, 1281, 1282, 1284, 1286, 1288, 1289, 1291, 1293, 1294, 1296, 1298, 1299, 1301, 1303, 1305, 1306, }
{654, 667, 681, 695, 708, 722, 736, 749, 763, 776, 790, 804, 817, 831, 845, 858, 872, 885, 899, 913, 926, 940, 954, 967, 981, 994, 1008, 1022, 1035, 1049, 1063, 1076, 1090, 1104, 1117, 1131, 1144, 1158, 1172, 1185, 1199, 1213, 1226, 1240, 1253, 1267, 1281, 1294, }
{1308, 1310, 1311, 1313, 1315, 1316, 1318, 1320, 1322, 1323, 1325, 1327, 1328, 1330, 1332, 1334, 1335, 1337, 1339, 1340, 1342, 1344, 1345, 1347, 1349, 1351, 1352, 1354, 1356, 1357, 1359, 1361, 1362, 1364, 1366, 1368, 1369, 1371, 1373, 1374, 1376, 1378, 1380, 1381, 1383, 1385, 1386, 1388, 1390, 1391, 1393, 1395, 1397, 1398, 1400, 1402, 1403, 1405, 1407, 1408, 1410, 1412, 1414, 1415, 1417, 1419, 1420, 1422, 1424, 1426, 1427, 1429, 1431, 1432, 1434, 1436, 1437, 1439, 1441, 1443, 1444, 1446, 1448, 1449, 1451, 1453, 1454, 1456, 1458, 1460, 1461, 1463, 1465, 1466, 1468, 1470, }
{1308, 1311, 1315, 1318, 1322, 1325, 1328, 1332, 1335, 1339, 1342, 1345, 1349, 1352, 1356, 1359, 1362, 1366, 1369, 1373, 1376, 1380, 1383, 1386, 1390, 1393, 1397, 1400, 1403, 1407, 1410, 1414, 1417, 1420, 1424, 1427, 1431, 1434, 1437, 1441, 1444, 1448, 1451, 1454, 1458, 1461, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 4
num_workers: 24
max_num_states_per_worker: 62
sparse_size_to_use: 62
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 4
num_workers: 24
max_num_states_per_worker: 62
sparse_size_to_use: 62
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,4,72}
 repeated_sparse_spike_ids.shapeToString(): {48,2,4,72}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,72}
slicedInpSpikeIds.shapeToString(): {48,72}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,72}
slicedInpSpikeIds.shapeToString(): {48,72}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{1444, 26, }
start_tile: 1
num_tiles_per_layer[ilay]: 1444
start_tile + num_tiles_per_layer[ilay]: 1445
num_occupied_tiles: 733
batchsize: 48
sparse_size: 72
dLdx.shapeToString(): {733,48,72}
replicated_numGrads.shapeToString(): {733,48}
exchangeGroupSize: 24
numExchangeGroups: 30
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
start_id: 456, end_id: 480
start_id: 480, end_id: 504
start_id: 504, end_id: 528
start_id: 528, end_id: 552
start_id: 552, end_id: 576
start_id: 576, end_id: 600
start_id: 600, end_id: 624
start_id: 624, end_id: 648
start_id: 648, end_id: 672
start_id: 672, end_id: 696
lastExchangeGroupLarger
start_id: 696, end_id: 733
exchangeGroupSizeLast: 37
exchangeBatchSizeLast: 2
start_tile: 1445
num_tiles_per_layer[ilay]: 26
start_tile + num_tiles_per_layer[ilay]: 1471
num_occupied_tiles: 5
batchsize: 48
sparse_size: 72
dLdx.shapeToString(): {5,48,72}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 19:47:10.685344: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[1466 1466   10]
0 733.0
1 733.0
2 5.0
[TileMapping(start_tile=1, end_tile=734), TileMapping(start_tile=734, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=734), TileMapping(start_tile=734, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.2558990251065398
limit=0.2558990251065398
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 1466) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 1466) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_72_72_2_48_1_734_1467_734_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 72), (100 5562034     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][2
                                                                 keras_multi_lif_layer_sparse[0][5
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 5,562,034
Trainable params: 5,553,208
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_72_72_2_48_1_734_1467_734_1467_1472
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_72_72_2_48_1_734_1467_734_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.405052/52 [==============================] - 975s 19s/step - loss: 2.4050
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.188752/52 [==============================] - 1s 22ms/step - loss: 2.1887
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 2.030152/52 [==============================] - 0s 8ms/step - loss: 2.0301
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 1.971552/52 [==============================] - 0s 8ms/step - loss: 1.9715
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 1.945152/52 [==============================] - 0s 9ms/step - loss: 1.9451
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 1.931452/52 [==============================] - 0s 9ms/step - loss: 1.9314
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 1.725252/52 [==============================] - 0s 9ms/step - loss: 1.7252
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 1.682552/52 [==============================] - 0s 8ms/step - loss: 1.6825
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 1.389052/52 [==============================] - 0s 8ms/step - loss: 1.3890
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 1.405252/52 [==============================] - 0s 8ms/step - loss: 1.4052
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 1.383852/52 [==============================] - 0s 8ms/step - loss: 1.3838
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 1.328552/52 [==============================] - 0s 8ms/step - loss: 1.3285
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 1.236552/52 [==============================] - 0s 8ms/step - loss: 1.2365
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 1.219352/52 [==============================] - 0s 8ms/step - loss: 1.2193
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 1.139452/52 [==============================] - 0s 8ms/step - loss: 1.1394
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 1.219352/52 [==============================] - 0s 9ms/step - loss: 1.2193
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 1.162852/52 [==============================] - 0s 9ms/step - loss: 1.1628
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 1.134152/52 [==============================] - 0s 8ms/step - loss: 1.1341
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 1.120652/52 [==============================] - 1s 10ms/step - loss: 1.1206
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 1.156452/52 [==============================] - 0s 9ms/step - loss: 1.1564
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 1.134352/52 [==============================] - 0s 9ms/step - loss: 1.1343
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 1.083252/52 [==============================] - 0s 9ms/step - loss: 1.0832
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 1.072652/52 [==============================] - 0s 9ms/step - loss: 1.0726
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 1.073652/52 [==============================] - 0s 9ms/step - loss: 1.0736
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 1.150152/52 [==============================] - 0s 9ms/step - loss: 1.1501

Final time:  986.6439020633698
2022-09-12 19:47:26.548789: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.05
DENSE_SIZES:  [2312, 978, 978, 976, 10]
SPARSE_SIZES:  [32, 48, 48, 48, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  3
SECOND_THRESHOLD:  0.9

60000
9984
[25907 13082 30757 ... 17438 14374 23687]
2022-09-12 19:47:35.583062: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 19:47:36.425618: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 9, 2, 1
Build_allocator: 13, 3, 1
Build_allocator: 10, 2, 2
Build_allocator: 14, 3, 2
Build_allocator: 11, 2, 3
Build_allocator: 15, 3, 3
Build_allocator: 4, 1, 0
Build_allocator: 5, 1, 1
Build_allocator: 6, 1, 2
Build_allocator: 7, 1, 3
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 16, 4, 0
Build_allocator: 17, 4, 1
Build_allocator: 18, 4, 2
Build_allocator: 19, 4, 3
Build_allocator: 20, 5, 0
Build_allocator: 21, 5, 1
Build_allocator: 22, 5, 2
Build_allocator: 23, 5, 3
Build_allocator: 8, 2, 0
Build_allocator: 12, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0x2ba833b0
1: &init_state[ilay]0x2ba833b8
2: &init_state[ilay]0x2ba833c0
3: &init_state[ilay]0x2ba833c8
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 490, 979, 1467, }
end_tiles: {490, 979, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{3, 3, 3, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 4, 6, 7, 9, 10, 12, 13, 15, 16, 18, 19, 21, 23, 24, 26, 27, 29, 30, 32, 33, 35, 36, 38, 39, 41, 42, 44, 46, 47, 49, 50, 52, 53, 55, 56, 58, 59, 61, 62, 64, 65, 67, 69, 70, 72, 73, 75, 76, 78, 79, 81, 82, 84, 85, 87, 88, 90, 92, 93, 95, 96, 98, 99, 101, 102, 104, 105, 107, 108, 110, 111, 113, 115, 116, 118, 119, 121, 122, 124, 125, 127, 128, 130, 131, 133, 134, 136, 138, 139, 141, 142, 144, 145, 147, 148, 150, 151, 153, 154, 156, 157, 159, 161, 162, 164, 165, 167, 168, 170, 171, 173, 174, 176, 177, 179, 180, 182, 184, 185, 187, 188, 190, 191, 193, 194, 196, 197, 199, 200, 202, 203, 205, 207, 208, 210, 211, 213, 214, 216, 217, 219, 220, 222, 223, 225, 226, 228, 230, 231, 233, 234, 236, 237, 239, 240, 242, 243, 245, 246, 248, 249, 251, 253, 254, 256, 257, 259, 260, 262, 263, 265, 266, 268, 269, 271, 272, 274, 276, 277, 279, 280, 282, 283, 285, 286, 288, 289, 291, 292, 294, 295, 297, 299, 300, 302, 303, 305, 306, 308, 309, 311, 312, 314, 315, 317, 318, 320, 322, 323, 325, 326, 328, 329, 331, 332, 334, 335, 337, 338, 340, 341, 343, 345, 346, 348, 349, 351, 352, 354, 355, 357, 358, 360, 361, 363, 364, 366, 368, 369, 371, 372, 374, 375, 377, 378, 380, 381, 383, 384, 386, 387, 389, 391, 392, 394, 395, 397, 398, 400, 401, 403, 404, 406, 407, 409, 410, 412, 414, 415, 417, 418, 420, 421, 423, 424, 426, 427, 429, 430, 432, 433, 435, 437, 438, 440, }
{0, 9, 18, 27, 36, 46, 55, 64, 73, 82, 92, 101, 110, 119, 128, 138, 147, 156, 165, 174, 184, 193, 202, 211, 220, 230, 239, 248, 257, 266, 276, 285, 294, 303, 312, 322, 331, 340, 349, 358, 368, 377, 386, 395, 404, 414, 423, 432, }
{441, 443, 444, 446, 447, 449, 450, 452, 453, 455, 456, 458, 460, 461, 463, 464, 466, 467, 469, 470, 472, 473, 475, 476, 478, 479, 481, 483, 484, 486, 487, 489, 490, 492, 493, 495, 496, 498, 499, 501, 502, 504, 506, 507, 509, 510, 512, 513, 515, 516, 518, 519, 521, 522, 524, 525, 527, 529, 530, 532, 533, 535, 536, 538, 539, 541, 542, 544, 545, 547, 548, 550, 552, 553, 555, 556, 558, 559, 561, 562, 564, 565, 567, 568, 570, 571, 573, 575, 576, 578, 579, 581, 582, 584, 585, 587, 588, 590, 591, 593, 594, 596, 598, 599, 601, 602, 604, 605, 607, 608, 610, 611, 613, 614, 616, 617, 619, 621, 622, 624, 625, 627, 628, 630, 631, 633, 634, 636, 637, 639, 640, 642, 644, 645, 647, 648, 650, 651, 653, 654, 656, 657, 659, 660, 662, 663, 665, 667, 668, 670, 671, 673, 674, 676, 677, 679, 680, 682, 683, 685, 686, 688, 690, 691, 693, 694, 696, 697, 699, 700, 702, 703, 705, 706, 708, 709, 711, 713, 714, 716, 717, 719, 720, 722, 723, 725, 726, 728, 729, 731, 732, 734, 736, 737, 739, 740, 742, 743, 745, 746, 748, 749, 751, 752, 754, 755, 757, 759, 760, 762, 763, 765, 766, 768, 769, 771, 772, 774, 775, 777, 778, 780, 782, 783, 785, 786, 788, 789, 791, 792, 794, 795, 797, 798, 800, 801, 803, 805, 806, 808, 809, 811, 812, 814, 815, 817, 818, 820, 821, 823, 824, 826, 828, 829, 831, 832, 834, 835, 837, 838, 840, 841, 843, 844, 846, 847, 849, 851, 852, 854, 855, 857, 858, 860, 861, 863, 864, 866, 867, 869, 870, 872, 874, 875, 877, 878, 880, 881, }
{441, 450, 460, 469, 478, 487, 496, 506, 515, 524, 533, 542, 552, 561, 570, 579, 588, 598, 607, 616, 625, 634, 644, 653, 662, 671, 680, 690, 699, 708, 717, 726, 736, 745, 754, 763, 772, 782, 791, 800, 809, 818, 828, 837, 846, 855, 864, 874, }
{883, 884, 886, 887, 889, 890, 892, 893, 895, 897, 898, 900, 901, 903, 904, 906, 907, 909, 910, 912, 913, 915, 916, 918, 920, 921, 923, 924, 926, 927, 929, 930, 932, 933, 935, 936, 938, 939, 941, 943, 944, 946, 947, 949, 950, 952, 953, 955, 956, 958, 959, 961, 962, 964, 966, 967, 969, 970, 972, 973, 975, 976, 978, 979, 981, 982, 984, 985, 987, 989, 990, 992, 993, 995, 996, 998, 999, 1001, 1002, 1004, 1005, 1007, 1008, 1010, 1012, 1013, 1015, 1016, 1018, 1019, 1021, 1022, 1024, 1025, 1027, 1028, 1030, 1031, 1033, 1035, 1036, 1038, 1039, 1041, 1042, 1044, 1045, 1047, 1048, 1050, 1051, 1053, 1054, 1056, 1058, 1059, 1061, 1062, 1064, 1065, 1067, 1068, 1070, 1071, 1073, 1074, 1076, 1077, 1079, 1081, 1082, 1084, 1085, 1087, 1088, 1090, 1091, 1093, 1094, 1096, 1097, 1099, 1100, 1102, 1104, 1105, 1107, 1108, 1110, 1111, 1113, 1114, 1116, 1117, 1119, 1120, 1122, 1123, 1125, 1127, 1128, 1130, 1131, 1133, 1134, 1136, 1137, 1139, 1140, 1142, 1143, 1145, 1146, 1148, 1150, 1151, 1153, 1154, 1156, 1157, 1159, 1160, 1162, 1163, 1165, 1166, 1168, 1169, 1171, 1173, 1174, 1176, 1177, 1179, 1180, 1182, 1183, 1185, 1186, 1188, 1189, 1191, 1192, 1194, 1196, 1197, 1199, 1200, 1202, 1203, 1205, 1206, 1208, 1209, 1211, 1212, 1214, 1215, 1217, 1219, 1220, 1222, 1223, 1225, 1226, 1228, 1229, 1231, 1232, 1234, 1235, 1237, 1238, 1240, 1242, 1243, 1245, 1246, 1248, 1249, 1251, 1252, 1254, 1255, 1257, 1258, 1260, 1261, 1263, 1265, 1266, 1268, 1269, 1271, 1272, 1274, 1275, 1277, 1278, 1280, 1281, 1283, 1284, 1286, 1288, 1289, 1291, 1292, 1294, 1295, 1297, 1298, 1300, 1301, 1303, 1304, 1306, 1307, 1309, 1311, 1312, 1314, 1315, 1317, 1318, 1320, 1321, 1323, }
{883, 892, 901, 910, 920, 929, 938, 947, 956, 966, 975, 984, 993, 1002, 1012, 1021, 1030, 1039, 1048, 1058, 1067, 1076, 1085, 1094, 1104, 1113, 1122, 1131, 1140, 1150, 1159, 1168, 1177, 1186, 1196, 1205, 1214, 1223, 1232, 1242, 1251, 1260, 1269, 1278, 1288, 1297, 1306, 1315, }
{1324, 1326, 1327, 1329, 1330, 1332, 1334, 1335, 1337, 1338, 1340, 1341, 1343, 1344, 1346, 1347, 1349, 1350, 1352, 1353, 1355, 1357, 1358, 1360, 1361, 1363, 1364, 1366, 1367, 1369, 1370, 1372, 1373, 1375, 1376, 1378, 1380, 1381, 1383, 1384, 1386, 1387, 1389, 1390, 1392, 1393, 1395, 1396, 1398, 1399, 1401, 1403, 1404, 1406, 1407, 1409, 1410, 1412, 1413, 1415, 1416, 1418, 1419, 1421, 1422, 1424, 1426, 1427, 1429, 1430, 1432, 1433, 1435, 1436, 1438, 1439, 1441, 1442, 1444, 1445, 1447, 1449, 1450, 1452, 1453, 1455, 1456, 1458, 1459, 1461, 1462, 1464, 1465, 1467, 1468, 1470, }
{1324, 1327, 1330, 1334, 1337, 1340, 1343, 1346, 1349, 1352, 1355, 1358, 1361, 1364, 1367, 1370, 1373, 1376, 1380, 1383, 1386, 1389, 1392, 1395, 1398, 1401, 1404, 1407, 1410, 1413, 1416, 1419, 1422, 1426, 1429, 1432, 1435, 1438, 1441, 1444, 1447, 1450, 1453, 1456, 1459, 1462, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 48
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 48
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 48
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,48}
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,48}
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,48}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,48}
slicedInpSpikeIds.shapeToString(): {48,48}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,48}
slicedInpSpikeIds.shapeToString(): {48,48}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,48}
slicedInpSpikeIds.shapeToString(): {48,48}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {488,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{726, 725, 18, }
start_tile: 1
num_tiles_per_layer[ilay]: 726
start_tile + num_tiles_per_layer[ilay]: 727
num_occupied_tiles: 489
batchsize: 48
sparse_size: 48
dLdx.shapeToString(): {489,48,48}
replicated_numGrads.shapeToString(): {489,48}
exchangeGroupSize: 24
numExchangeGroups: 20
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
lastExchangeGroupLarger
start_id: 456, end_id: 489
exchangeGroupSizeLast: 33
exchangeBatchSizeLast: 2
start_tile: 727
num_tiles_per_layer[ilay]: 725
start_tile + num_tiles_per_layer[ilay]: 1452
num_occupied_tiles: 488
batchsize: 48
sparse_size: 48
dLdx.shapeToString(): {488,48,48}
replicated_numGrads.shapeToString(): {488,48}
exchangeGroupSize: 24
numExchangeGroups: 20
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
lastExchangeGroupLarger
start_id: 456, end_id: 488
exchangeGroupSizeLast: 32
exchangeBatchSizeLast: 2
start_tile: 1452
num_tiles_per_layer[ilay]: 18
start_tile + num_tiles_per_layer[ilay]: 1470
num_occupied_tiles: 5
batchsize: 48
sparse_size: 48
dLdx.shapeToString(): {5,48,48}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {488,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 20:07:10.128526: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[978 978 976  10]
0 489.0
1 489.0
2 488.0
3 5.0
[TileMapping(start_tile=1, end_tile=490), TileMapping(start_tile=490, end_tile=979), TileMapping(start_tile=979, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=490), TileMapping(start_tile=490, end_tile=979), TileMapping(start_tile=979, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.31330417999518295
limit=0.31330417999518295
limit=0.31362502409359
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 978) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 978) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 976) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_48_48_48_2_48_1_490_979_1467_490_979_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 48), (100 4190734     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][3
                                                                 keras_multi_lif_layer_sparse[0][7
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 4,190,734
Trainable params: 4,181,908
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_48_48_48_2_48_1_490_979_1467_490_979_1467_1472
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_48_48_48_2_48_1_490_979_1467_490_979_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.482352/52 [==============================] - 1176s 23s/step - loss: 2.4823
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.174952/52 [==============================] - 0s 7ms/step - loss: 2.1749
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 2.128052/52 [==============================] - 0s 7ms/step - loss: 2.1280
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 2.155652/52 [==============================] - 0s 7ms/step - loss: 2.1556
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 2.196252/52 [==============================] - 0s 7ms/step - loss: 2.1962
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 2.165252/52 [==============================] - 0s 7ms/step - loss: 2.1652
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 1.994852/52 [==============================] - 0s 7ms/step - loss: 1.9948
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 1.776452/52 [==============================] - 0s 7ms/step - loss: 1.7764
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 1.778152/52 [==============================] - 0s 7ms/step - loss: 1.7781
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 1.883752/52 [==============================] - 0s 7ms/step - loss: 1.8837
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 1.864352/52 [==============================] - 0s 7ms/step - loss: 1.8643
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 1.839552/52 [==============================] - 0s 7ms/step - loss: 1.8395
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 1.894652/52 [==============================] - 0s 7ms/step - loss: 1.8946
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 1.834552/52 [==============================] - 0s 7ms/step - loss: 1.8345
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 1.814952/52 [==============================] - 0s 7ms/step - loss: 1.8149
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 1.848652/52 [==============================] - 0s 7ms/step - loss: 1.8486
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 1.882252/52 [==============================] - 0s 7ms/step - loss: 1.8822
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 1.860552/52 [==============================] - 0s 7ms/step - loss: 1.8605
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 1.846552/52 [==============================] - 0s 7ms/step - loss: 1.8465
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 1.856152/52 [==============================] - 0s 7ms/step - loss: 1.8561
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 1.884052/52 [==============================] - 0s 7ms/step - loss: 1.8840
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 1.865552/52 [==============================] - 0s 7ms/step - loss: 1.8655
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 1.837452/52 [==============================] - 0s 7ms/step - loss: 1.8374
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 1.734552/52 [==============================] - 1s 14ms/step - loss: 1.7345
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 1.791452/52 [==============================] - 1s 12ms/step - loss: 1.7914

Final time:  1185.0499625205994
2022-09-12 20:07:22.534848: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.05
DENSE_SIZES:  [2312, 734, 734, 732, 732, 10]
SPARSE_SIZES:  [32, 36, 36, 36, 36, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  4
SECOND_THRESHOLD:  0.9

60000
9984
[35064 56181 37974 ... 38479 52391 43184]
2022-09-12 20:07:31.894784: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 20:07:32.778723: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 11, 2, 1
Build_allocator: 16, 3, 1
Build_allocator: 12, 2, 2
Build_allocator: 17, 3, 2
Build_allocator: 13, 2, 3
Build_allocator: 18, 3, 3
Build_allocator: 14, 2, 4
Build_allocator: 19, 3, 4
Build_allocator: 5, 1, 0
Build_allocator: 6, 1, 1
Build_allocator: 7, 1, 2
Build_allocator: 8, 1, 3
Build_allocator: 9, 1, 4
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 4, 0, 4
Build_allocator: 20, 4, 0
Build_allocator: 21, 4, 1
Build_allocator: 22, 4, 2
Build_allocator: 23, 4, 3
Build_allocator: 24, 4, 4
Build_allocator: 25, 5, 0
Build_allocator: 26, 5, 1
Build_allocator: 27, 5, 2
Build_allocator: 28, 5, 3
Build_allocator: 29, 5, 4
Build_allocator: 10, 2, 0
Build_allocator: 15, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0x27cdf710
1: &init_state[ilay]0x27cdf718
2: &init_state[ilay]0x27cdf720
3: &init_state[ilay]0x27cdf728
4: &init_state[ilay]0x27cdf730
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0
3: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
3: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 368, 735, 1101, 1467, }
end_tiles: {368, 735, 1101, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{2, 2, 2, 2, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 5, 6, 8, 10, 11, 13, 15, 17, 18, 20, 22, 23, 25, 27, 28, 30, 32, 34, 35, 37, 39, 40, 42, 44, 46, 47, 49, 51, 52, 54, 56, 57, 59, 61, 63, 64, 66, 68, 69, 71, 73, 74, 76, 78, 80, 81, 83, 85, 86, 88, 90, 92, 93, 95, 97, 98, 100, 102, 103, 105, 107, 109, 110, 112, 114, 115, 117, 119, 120, 122, 124, 126, 127, 129, 131, 132, 134, 136, 138, 139, 141, 143, 144, 146, 148, 149, 151, 153, 155, 156, 158, 160, 161, 163, 165, 166, 168, 170, 172, 173, 175, 177, 178, 180, 182, 184, 185, 187, 189, 190, 192, 194, 195, 197, 199, 201, 202, 204, 206, 207, 209, 211, 212, 214, 216, 218, 219, 221, 223, 224, 226, 228, 230, 231, 233, 235, 236, 238, 240, 241, 243, 245, 247, 248, 250, 252, 253, 255, 257, 258, 260, 262, 264, 265, 267, 269, 270, 272, 274, 276, 277, 279, 281, 282, 284, 286, 287, 289, 291, 293, 294, 296, 298, 299, 301, 303, 304, 306, 308, 310, 311, 313, 315, 316, 318, 320, 322, 323, 325, }
{0, 6, 13, 20, 27, 34, 40, 47, 54, 61, 68, 74, 81, 88, 95, 102, 109, 115, 122, 129, 136, 143, 149, 156, 163, 170, 177, 184, 190, 197, 204, 211, 218, 224, 231, 238, 245, 252, 258, 265, 272, 279, 286, 293, 299, 306, 313, 320, }
{327, 328, 330, 332, 333, 335, 337, 339, 340, 342, 344, 345, 347, 349, 350, 352, 354, 356, 357, 359, 361, 362, 364, 366, 368, 369, 371, 373, 374, 376, 378, 379, 381, 383, 385, 386, 388, 390, 391, 393, 395, 396, 398, 400, 402, 403, 405, 407, 408, 410, 412, 414, 415, 417, 419, 420, 422, 424, 425, 427, 429, 431, 432, 434, 436, 437, 439, 441, 442, 444, 446, 448, 449, 451, 453, 454, 456, 458, 460, 461, 463, 465, 466, 468, 470, 471, 473, 475, 477, 478, 480, 482, 483, 485, 487, 488, 490, 492, 494, 495, 497, 499, 500, 502, 504, 506, 507, 509, 511, 512, 514, 516, 517, 519, 521, 523, 524, 526, 528, 529, 531, 533, 534, 536, 538, 540, 541, 543, 545, 546, 548, 550, 552, 553, 555, 557, 558, 560, 562, 563, 565, 567, 569, 570, 572, 574, 575, 577, 579, 580, 582, 584, 586, 587, 589, 591, 592, 594, 596, 598, 599, 601, 603, 604, 606, 608, 609, 611, 613, 615, 616, 618, 620, 621, 623, 625, 626, 628, 630, 632, 633, 635, 637, 638, 640, 642, 644, 645, 647, 649, 650, 652, }
{327, 333, 340, 347, 354, 361, 368, 374, 381, 388, 395, 402, 408, 415, 422, 429, 436, 442, 449, 456, 463, 470, 477, 483, 490, 497, 504, 511, 517, 524, 531, 538, 545, 552, 558, 565, 572, 579, 586, 592, 599, 606, 613, 620, 626, 633, 640, 647, }
{654, 655, 657, 659, 661, 662, 664, 666, 667, 669, 671, 672, 674, 676, 678, 679, 681, 683, 684, 686, 688, 690, 691, 693, 695, 696, 698, 700, 701, 703, 705, 707, 708, 710, 712, 713, 715, 717, 718, 720, 722, 724, 725, 727, 729, 730, 732, 734, 736, 737, 739, 741, 742, 744, 746, 747, 749, 751, 753, 754, 756, 758, 759, 761, 763, 764, 766, 768, 770, 771, 773, 775, 776, 778, 780, 782, 783, 785, 787, 788, 790, 792, 793, 795, 797, 799, 800, 802, 804, 805, 807, 809, 810, 812, 814, 816, 817, 819, 821, 822, 824, 826, 828, 829, 831, 833, 834, 836, 838, 839, 841, 843, 845, 846, 848, 850, 851, 853, 855, 856, 858, 860, 862, 863, 865, 867, 868, 870, 872, 874, 875, 877, 879, 880, 882, 884, 885, 887, 889, 891, 892, 894, 896, 897, 899, 901, 902, 904, 906, 908, 909, 911, 913, 914, 916, 918, 920, 921, 923, 925, 926, 928, 930, 931, 933, 935, 937, 938, 940, 942, 943, 945, 947, 948, 950, 952, 954, 955, 957, 959, 960, 962, 964, 966, 967, 969, 971, 972, 974, 976, 977, 979, }
{654, 661, 667, 674, 681, 688, 695, 701, 708, 715, 722, 729, 736, 742, 749, 756, 763, 770, 776, 783, 790, 797, 804, 810, 817, 824, 831, 838, 845, 851, 858, 865, 872, 879, 885, 892, 899, 906, 913, 920, 926, 933, 940, 947, 954, 960, 967, 974, }
{981, 983, 984, 986, 988, 989, 991, 993, 994, 996, 998, 1000, 1001, 1003, 1005, 1006, 1008, 1010, 1012, 1013, 1015, 1017, 1018, 1020, 1022, 1023, 1025, 1027, 1029, 1030, 1032, 1034, 1035, 1037, 1039, 1040, 1042, 1044, 1046, 1047, 1049, 1051, 1052, 1054, 1056, 1058, 1059, 1061, 1063, 1064, 1066, 1068, 1069, 1071, 1073, 1075, 1076, 1078, 1080, 1081, 1083, 1085, 1086, 1088, 1090, 1092, 1093, 1095, 1097, 1098, 1100, 1102, 1104, 1105, 1107, 1109, 1110, 1112, 1114, 1115, 1117, 1119, 1121, 1122, 1124, 1126, 1127, 1129, 1131, 1132, 1134, 1136, 1138, 1139, 1141, 1143, 1144, 1146, 1148, 1150, 1151, 1153, 1155, 1156, 1158, 1160, 1161, 1163, 1165, 1167, 1168, 1170, 1172, 1173, 1175, 1177, 1178, 1180, 1182, 1184, 1185, 1187, 1189, 1190, 1192, 1194, 1196, 1197, 1199, 1201, 1202, 1204, 1206, 1207, 1209, 1211, 1213, 1214, 1216, 1218, 1219, 1221, 1223, 1224, 1226, 1228, 1230, 1231, 1233, 1235, 1236, 1238, 1240, 1242, 1243, 1245, 1247, 1248, 1250, 1252, 1253, 1255, 1257, 1259, 1260, 1262, 1264, 1265, 1267, 1269, 1270, 1272, 1274, 1276, 1277, 1279, 1281, 1282, 1284, 1286, 1288, 1289, 1291, 1293, 1294, 1296, 1298, 1299, 1301, 1303, 1305, 1306, }
{981, 988, 994, 1001, 1008, 1015, 1022, 1029, 1035, 1042, 1049, 1056, 1063, 1069, 1076, 1083, 1090, 1097, 1104, 1110, 1117, 1124, 1131, 1138, 1144, 1151, 1158, 1165, 1172, 1178, 1185, 1192, 1199, 1206, 1213, 1219, 1226, 1233, 1240, 1247, 1253, 1260, 1267, 1274, 1281, 1288, 1294, 1301, }
{1308, 1310, 1311, 1313, 1315, 1316, 1318, 1320, 1322, 1323, 1325, 1327, 1328, 1330, 1332, 1334, 1335, 1337, 1339, 1340, 1342, 1344, 1345, 1347, 1349, 1351, 1352, 1354, 1356, 1357, 1359, 1361, 1362, 1364, 1366, 1368, 1369, 1371, 1373, 1374, 1376, 1378, 1380, 1381, 1383, 1385, 1386, 1388, 1390, 1391, 1393, 1395, 1397, 1398, 1400, 1402, 1403, 1405, 1407, 1408, 1410, 1412, 1414, 1415, 1417, 1419, 1420, 1422, 1424, 1426, 1427, 1429, 1431, 1432, 1434, 1436, 1437, 1439, 1441, 1443, 1444, 1446, 1448, 1449, 1451, 1453, 1454, 1456, 1458, 1460, 1461, 1463, 1465, 1466, 1468, 1470, }
{1308, 1311, 1315, 1318, 1322, 1325, 1328, 1332, 1335, 1339, 1342, 1345, 1349, 1352, 1356, 1359, 1362, 1366, 1369, 1373, 1376, 1380, 1383, 1386, 1390, 1393, 1397, 1400, 1403, 1407, 1410, 1414, 1417, 1420, 1424, 1427, 1431, 1434, 1437, 1441, 1444, 1448, 1451, 1454, 1458, 1461, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 62
sparse_size_to_use: 36
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 62
sparse_size_to_use: 36
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 61
sparse_size_to_use: 36
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 61
sparse_size_to_use: 36
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,36}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,36}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,36}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,36}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 4
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,36}
slicedInpSpikeIds.shapeToString(): {48,36}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,36}
slicedInpSpikeIds.shapeToString(): {48,36}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,36}
slicedInpSpikeIds.shapeToString(): {48,36}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

3
out_spike_ids[ilay].shapeToString(): {100,48,36}
slicedInpSpikeIds.shapeToString(): {48,36}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,2}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{485, 484, 484, 15, }
start_tile: 1
num_tiles_per_layer[ilay]: 485
start_tile + num_tiles_per_layer[ilay]: 486
num_occupied_tiles: 367
batchsize: 48
sparse_size: 36
dLdx.shapeToString(): {367,48,36}
replicated_numGrads.shapeToString(): {367,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 367
exchangeGroupSizeLast: 31
exchangeBatchSizeLast: 2
start_tile: 486
num_tiles_per_layer[ilay]: 484
start_tile + num_tiles_per_layer[ilay]: 970
num_occupied_tiles: 366
batchsize: 48
sparse_size: 36
dLdx.shapeToString(): {366,48,36}
replicated_numGrads.shapeToString(): {366,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 366
exchangeGroupSizeLast: 30
exchangeBatchSizeLast: 2
start_tile: 970
num_tiles_per_layer[ilay]: 484
start_tile + num_tiles_per_layer[ilay]: 1454
num_occupied_tiles: 366
batchsize: 48
sparse_size: 36
dLdx.shapeToString(): {366,48,36}
replicated_numGrads.shapeToString(): {366,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 366
exchangeGroupSizeLast: 30
exchangeBatchSizeLast: 2
start_tile: 1454
num_tiles_per_layer[ilay]: 15
start_tile + num_tiles_per_layer[ilay]: 1469
num_occupied_tiles: 5
batchsize: 48
sparse_size: 36
dLdx.shapeToString(): {5,48,36}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,1}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 20:21:25.207383: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[734 734 732 732  10]
0 367.0
1 367.0
2 366.0
3 366.0
4 5.0
[TileMapping(start_tile=1, end_tile=368), TileMapping(start_tile=368, end_tile=735), TileMapping(start_tile=735, end_tile=1101), TileMapping(start_tile=1101, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=368), TileMapping(start_tile=368, end_tile=735), TileMapping(start_tile=735, end_tile=1101), TileMapping(start_tile=1101, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.3616492648073473
limit=0.3616492648073473
limit=0.3621429841700741
limit=0.3621429841700741
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 734) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 734) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 732) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 732) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_5:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_36_36_36_36_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 36), (100 3325022     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][4
                                                                 keras_multi_lif_layer_sparse[0][9
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 3,325,022
Trainable params: 3,316,196
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_36_36_36_36_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_36_36_36_36_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.684852/52 [==============================] - 835s 16s/step - loss: 2.6848
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.253152/52 [==============================] - 0s 6ms/step - loss: 2.2531
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 2.009952/52 [==============================] - 0s 6ms/step - loss: 2.0099
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 1.935852/52 [==============================] - 0s 6ms/step - loss: 1.9358
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 1.906752/52 [==============================] - 0s 6ms/step - loss: 1.9067
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 1.897152/52 [==============================] - 0s 6ms/step - loss: 1.8971
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 1.934752/52 [==============================] - 0s 6ms/step - loss: 1.9347
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 1.905152/52 [==============================] - 0s 6ms/step - loss: 1.9051
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 1.887252/52 [==============================] - 0s 6ms/step - loss: 1.8872
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 1.901552/52 [==============================] - 0s 6ms/step - loss: 1.9015
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 1.921252/52 [==============================] - 0s 6ms/step - loss: 1.9212
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 1.899252/52 [==============================] - 0s 6ms/step - loss: 1.8992
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 1.877652/52 [==============================] - 0s 6ms/step - loss: 1.8776
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 1.874852/52 [==============================] - 0s 6ms/step - loss: 1.8748
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 1.924052/52 [==============================] - 0s 6ms/step - loss: 1.9240
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 2.106952/52 [==============================] - 0s 6ms/step - loss: 2.1069
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 2.112552/52 [==============================] - 0s 6ms/step - loss: 2.1125
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 1.992952/52 [==============================] - 0s 6ms/step - loss: 1.9929
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 1.934252/52 [==============================] - 0s 6ms/step - loss: 1.9342
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 2.110252/52 [==============================] - 0s 6ms/step - loss: 2.1102
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 2.093252/52 [==============================] - 0s 6ms/step - loss: 2.0932
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 2.009952/52 [==============================] - 0s 6ms/step - loss: 2.0099
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 1.952652/52 [==============================] - 0s 6ms/step - loss: 1.9526
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 1.897052/52 [==============================] - 0s 6ms/step - loss: 1.8970
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 1.907552/52 [==============================] - 0s 6ms/step - loss: 1.9075

Final time:  842.2105672359467
2022-09-12 20:21:36.042992: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.05
DENSE_SIZES:  [2312, 588, 586, 586, 586, 586, 10]
SPARSE_SIZES:  [32, 28, 28, 28, 28, 28, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  5
SECOND_THRESHOLD:  0.9

60000
9984
[34529 31838 47714 ... 22662 44978 59846]
2022-09-12 20:21:44.389189: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 20:21:45.291544: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 13, 2, 1
Build_allocator: 19, 3, 1
Build_allocator: 14, 2, 2
Build_allocator: 20, 3, 2
Build_allocator: 15, 2, 3
Build_allocator: 21, 3, 3
Build_allocator: 16, 2, 4
Build_allocator: 22, 3, 4
Build_allocator: 17, 2, 5
Build_allocator: 23, 3, 5
Build_allocator: 6, 1, 0
Build_allocator: 7, 1, 1
Build_allocator: 8, 1, 2
Build_allocator: 9, 1, 3
Build_allocator: 10, 1, 4
Build_allocator: 11, 1, 5
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 4, 0, 4
Build_allocator: 5, 0, 5
Build_allocator: 24, 4, 0
Build_allocator: 25, 4, 1
Build_allocator: 26, 4, 2
Build_allocator: 27, 4, 3
Build_allocator: 28, 4, 4
Build_allocator: 29, 4, 5
Build_allocator: 30, 5, 0
Build_allocator: 31, 5, 1
Build_allocator: 32, 5, 2
Build_allocator: 33, 5, 3
Build_allocator: 34, 5, 4
Build_allocator: 35, 5, 5
Build_allocator: 12, 2, 0
Build_allocator: 18, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0xd21e240
1: &init_state[ilay]0xd21e248
2: &init_state[ilay]0xd21e250
3: &init_state[ilay]0xd21e258
4: &init_state[ilay]0xd21e260
5: &init_state[ilay]0xd21e268
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0
3: init_state[ilay]==init_state[ilay+1]: 0
4: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
3: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
4: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 295, 588, 881, 1174, 1467, }
end_tiles: {295, 588, 881, 1174, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{2, 2, 2, 2, 2, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 2, 4, 5, 6, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 25, 26, 27, 29, 30, 32, 33, 34, 36, 37, 39, 40, 41, 43, 44, 46, 47, 48, 50, 51, 52, 54, 55, 57, 58, 59, 61, 62, 64, 65, 66, 68, 69, 71, 72, 73, 75, 76, 78, 79, 80, 82, 83, 85, 86, 87, 89, 90, 92, 93, 94, 96, 97, 98, 100, 101, 103, 104, 105, 107, 108, 110, 111, 112, 114, 115, 117, 118, 119, 121, 122, 124, 125, 126, 128, 129, 131, 132, 133, 135, 136, 138, 139, 140, 142, 143, 144, 146, 147, 149, 150, 151, 153, 154, 156, 157, 158, 160, 161, 163, 164, 165, 167, 168, 170, 171, 172, 174, 175, 177, 178, 179, 181, 182, 184, 185, 186, 188, 189, 190, 192, 193, 195, 196, 197, 199, 200, 202, 203, 204, 206, 207, 209, 210, 211, 213, 214, 216, 217, 218, 220, 221, 223, 224, 225, 227, 228, 230, 231, 232, 234, 235, 236, 238, 239, 241, 242, 243, 245, 246, 248, 249, 250, 252, 253, 255, 256, 257, 259, 260, 262, 263, 264, 266, }
{0, 5, 11, 16, 22, 27, 33, 39, 44, 50, 55, 61, 66, 72, 78, 83, 89, 94, 100, 105, 111, 117, 122, 128, 133, 139, 144, 150, 156, 161, 167, 172, 178, 184, 189, 195, 200, 206, 211, 217, 223, 228, 234, 239, 245, 250, 256, 262, }
{267, 269, 270, 271, 273, 274, 276, 277, 278, 280, 281, 282, 284, 285, 287, 288, 289, 291, 292, 294, 295, 296, 298, 299, 301, 302, 303, 305, 306, 308, 309, 310, 312, 313, 315, 316, 317, 319, 320, 322, 323, 324, 326, 327, 328, 330, 331, 333, 334, 335, 337, 338, 340, 341, 342, 344, 345, 347, 348, 349, 351, 352, 354, 355, 356, 358, 359, 361, 362, 363, 365, 366, 368, 369, 370, 372, 373, 374, 376, 377, 379, 380, 381, 383, 384, 386, 387, 388, 390, 391, 393, 394, 395, 397, 398, 400, 401, 402, 404, 405, 407, 408, 409, 411, 412, 414, 415, 416, 418, 419, 420, 422, 423, 425, 426, 427, 429, 430, 432, 433, 434, 436, 437, 439, 440, 441, 443, 444, 446, 447, 448, 450, 451, 453, 454, 455, 457, 458, 460, 461, 462, 464, 465, 466, 468, 469, 471, 472, 473, 475, 476, 478, 479, 480, 482, 483, 485, 486, 487, 489, 490, 492, 493, 494, 496, 497, 499, 500, 501, 503, 504, 506, 507, 508, 510, 511, 512, 514, 515, 517, 518, 519, 521, 522, 524, 525, 526, 528, 529, 531, 532, 533, }
{267, 273, 278, 284, 289, 295, 301, 306, 312, 317, 323, 328, 334, 340, 345, 351, 356, 362, 368, 373, 379, 384, 390, 395, 401, 407, 412, 418, 423, 429, 434, 440, 446, 451, 457, 462, 468, 473, 479, 485, 490, 496, 501, 507, 512, 518, 524, 529, }
{535, 536, 538, 539, 540, 542, 543, 545, 546, 547, 549, 550, 552, 553, 554, 556, 557, 558, 560, 561, 563, 564, 565, 567, 568, 570, 571, 572, 574, 575, 577, 578, 579, 581, 582, 584, 585, 586, 588, 589, 591, 592, 593, 595, 596, 598, 599, 600, 602, 603, 604, 606, 607, 609, 610, 611, 613, 614, 616, 617, 618, 620, 621, 623, 624, 625, 627, 628, 630, 631, 632, 634, 635, 637, 638, 639, 641, 642, 644, 645, 646, 648, 649, 650, 652, 653, 655, 656, 657, 659, 660, 662, 663, 664, 666, 667, 669, 670, 671, 673, 674, 676, 677, 678, 680, 681, 683, 684, 685, 687, 688, 690, 691, 692, 694, 695, 696, 698, 699, 701, 702, 703, 705, 706, 708, 709, 710, 712, 713, 715, 716, 717, 719, 720, 722, 723, 724, 726, 727, 729, 730, 731, 733, 734, 736, 737, 738, 740, 741, 742, 744, 745, 747, 748, 749, 751, 752, 754, 755, 756, 758, 759, 761, 762, 763, 765, 766, 768, 769, 770, 772, 773, 775, 776, 777, 779, 780, 782, 783, 784, 786, 787, 788, 790, 791, 793, 794, 795, 797, 798, 800, 801, }
{535, 540, 546, 552, 557, 563, 568, 574, 579, 585, 591, 596, 602, 607, 613, 618, 624, 630, 635, 641, 646, 652, 657, 663, 669, 674, 680, 685, 691, 696, 702, 708, 713, 719, 724, 730, 736, 741, 747, 752, 758, 763, 769, 775, 780, 786, 791, 797, }
{802, 804, 805, 807, 808, 809, 811, 812, 814, 815, 816, 818, 819, 821, 822, 823, 825, 826, 828, 829, 830, 832, 833, 834, 836, 837, 839, 840, 841, 843, 844, 846, 847, 848, 850, 851, 853, 854, 855, 857, 858, 860, 861, 862, 864, 865, 867, 868, 869, 871, 872, 874, 875, 876, 878, 879, 880, 882, 883, 885, 886, 887, 889, 890, 892, 893, 894, 896, 897, 899, 900, 901, 903, 904, 906, 907, 908, 910, 911, 913, 914, 915, 917, 918, 920, 921, 922, 924, 925, 926, 928, 929, 931, 932, 933, 935, 936, 938, 939, 940, 942, 943, 945, 946, 947, 949, 950, 952, 953, 954, 956, 957, 959, 960, 961, 963, 964, 966, 967, 968, 970, 971, 972, 974, 975, 977, 978, 979, 981, 982, 984, 985, 986, 988, 989, 991, 992, 993, 995, 996, 998, 999, 1000, 1002, 1003, 1005, 1006, 1007, 1009, 1010, 1012, 1013, 1014, 1016, 1017, 1018, 1020, 1021, 1023, 1024, 1025, 1027, 1028, 1030, 1031, 1032, 1034, 1035, 1037, 1038, 1039, 1041, 1042, 1044, 1045, 1046, 1048, 1049, 1051, 1052, 1053, 1055, 1056, 1058, 1059, 1060, 1062, 1063, 1064, 1066, 1067, 1069, }
{802, 808, 814, 819, 825, 830, 836, 841, 847, 853, 858, 864, 869, 875, 880, 886, 892, 897, 903, 908, 914, 920, 925, 931, 936, 942, 947, 953, 959, 964, 970, 975, 981, 986, 992, 998, 1003, 1009, 1014, 1020, 1025, 1031, 1037, 1042, 1048, 1053, 1059, 1064, }
{1070, 1071, 1073, 1074, 1076, 1077, 1078, 1080, 1081, 1083, 1084, 1085, 1087, 1088, 1090, 1091, 1092, 1094, 1095, 1097, 1098, 1099, 1101, 1102, 1104, 1105, 1106, 1108, 1109, 1110, 1112, 1113, 1115, 1116, 1117, 1119, 1120, 1122, 1123, 1124, 1126, 1127, 1129, 1130, 1131, 1133, 1134, 1136, 1137, 1138, 1140, 1141, 1143, 1144, 1145, 1147, 1148, 1150, 1151, 1152, 1154, 1155, 1156, 1158, 1159, 1161, 1162, 1163, 1165, 1166, 1168, 1169, 1170, 1172, 1173, 1175, 1176, 1177, 1179, 1180, 1182, 1183, 1184, 1186, 1187, 1189, 1190, 1191, 1193, 1194, 1196, 1197, 1198, 1200, 1201, 1202, 1204, 1205, 1207, 1208, 1209, 1211, 1212, 1214, 1215, 1216, 1218, 1219, 1221, 1222, 1223, 1225, 1226, 1228, 1229, 1230, 1232, 1233, 1235, 1236, 1237, 1239, 1240, 1242, 1243, 1244, 1246, 1247, 1248, 1250, 1251, 1253, 1254, 1255, 1257, 1258, 1260, 1261, 1262, 1264, 1265, 1267, 1268, 1269, 1271, 1272, 1274, 1275, 1276, 1278, 1279, 1281, 1282, 1283, 1285, 1286, 1288, 1289, 1290, 1292, 1293, 1294, 1296, 1297, 1299, 1300, 1301, 1303, 1304, 1306, 1307, 1308, 1310, 1311, 1313, 1314, 1315, 1317, 1318, 1320, 1321, 1322, 1324, 1325, 1327, 1328, 1329, 1331, 1332, 1334, 1335, 1336, }
{1070, 1076, 1081, 1087, 1092, 1098, 1104, 1109, 1115, 1120, 1126, 1131, 1137, 1143, 1148, 1154, 1159, 1165, 1170, 1176, 1182, 1187, 1193, 1198, 1204, 1209, 1215, 1221, 1226, 1232, 1237, 1243, 1248, 1254, 1260, 1265, 1271, 1276, 1282, 1288, 1293, 1299, 1304, 1310, 1315, 1321, 1327, 1332, }
{1338, 1339, 1340, 1342, 1343, 1345, 1346, 1347, 1349, 1350, 1352, 1353, 1354, 1356, 1357, 1359, 1360, 1361, 1363, 1364, 1366, 1367, 1368, 1370, 1371, 1373, 1374, 1375, 1377, 1378, 1380, 1381, 1382, 1384, 1385, 1386, 1388, 1389, 1391, 1392, 1393, 1395, 1396, 1398, 1399, 1400, 1402, 1403, 1405, 1406, 1407, 1409, 1410, 1412, 1413, 1414, 1416, 1417, 1419, 1420, 1421, 1423, 1424, 1426, 1427, 1428, 1430, 1431, 1432, 1434, 1435, 1437, 1438, 1439, 1441, 1442, 1444, 1445, 1446, 1448, 1449, 1451, 1452, 1453, 1455, 1456, 1458, 1459, 1460, 1462, 1463, 1465, 1466, 1467, 1469, 1470, }
{1338, 1340, 1343, 1346, 1349, 1352, 1354, 1357, 1360, 1363, 1366, 1368, 1371, 1374, 1377, 1380, 1382, 1385, 1388, 1391, 1393, 1396, 1399, 1402, 1405, 1407, 1410, 1413, 1416, 1419, 1421, 1424, 1427, 1430, 1432, 1435, 1438, 1441, 1444, 1446, 1449, 1452, 1455, 1458, 1460, 1463, 1466, 1469, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 28
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 28
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 28
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 28
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 28
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,28}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,28}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,28}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,28}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,28}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 4
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 5
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,28}
slicedInpSpikeIds.shapeToString(): {48,28}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,28}
slicedInpSpikeIds.shapeToString(): {48,28}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,28}
slicedInpSpikeIds.shapeToString(): {48,28}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

3
out_spike_ids[ilay].shapeToString(): {100,48,28}
slicedInpSpikeIds.shapeToString(): {48,28}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

4
out_spike_ids[ilay].shapeToString(): {100,48,28}
slicedInpSpikeIds.shapeToString(): {48,28}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {294,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 5
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{364, 364, 364, 364, 14, }
start_tile: 1
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 365
num_occupied_tiles: 293
batchsize: 48
sparse_size: 28
dLdx.shapeToString(): {293,48,28}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 365
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 729
num_occupied_tiles: 293
batchsize: 48
sparse_size: 28
dLdx.shapeToString(): {293,48,28}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 729
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 1093
num_occupied_tiles: 293
batchsize: 48
sparse_size: 28
dLdx.shapeToString(): {293,48,28}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 1093
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 1457
num_occupied_tiles: 293
batchsize: 48
sparse_size: 28
dLdx.shapeToString(): {293,48,28}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 1457
num_tiles_per_layer[ilay]: 14
start_tile + num_tiles_per_layer[ilay]: 1471
num_occupied_tiles: 5
batchsize: 48
sparse_size: 28
dLdx.shapeToString(): {5,48,28}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {294,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 5
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 20:30:19.510895: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[588 586 586 586 586  10]
0 294.0
1 293.0
2 293.0
3 293.0
4 293.0
5 5.0
[TileMapping(start_tile=1, end_tile=295), TileMapping(start_tile=295, end_tile=588), TileMapping(start_tile=588, end_tile=881), TileMapping(start_tile=881, end_tile=1174), TileMapping(start_tile=1174, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=295), TileMapping(start_tile=295, end_tile=588), TileMapping(start_tile=588, end_tile=881), TileMapping(start_tile=881, end_tile=1174), TileMapping(start_tile=1174, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.40406101782088427
limit=0.4047499544129309
limit=0.4047499544129309
limit=0.4047499544129309
limit=0.4047499544129309
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 588) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 586) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 586) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 586) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_5:0' shape=(2, 586) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_6:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_28_28_28_28_28_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 28), (100 2748898     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][5
                                                                 keras_multi_lif_layer_sparse[0][1
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 2,748,898
Trainable params: 2,740,072
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_28_28_28_28_28_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_28_28_28_28_28_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.771652/52 [==============================] - 518s 10s/step - loss: 2.7716
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.146952/52 [==============================] - 0s 6ms/step - loss: 2.1469
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 2.121352/52 [==============================] - 0s 5ms/step - loss: 2.1213
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 2.082652/52 [==============================] - 0s 6ms/step - loss: 2.0826
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 2.129752/52 [==============================] - 0s 6ms/step - loss: 2.1297
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 2.098852/52 [==============================] - 0s 6ms/step - loss: 2.0988
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 2.086652/52 [==============================] - 0s 6ms/step - loss: 2.0866
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 2.065552/52 [==============================] - 0s 6ms/step - loss: 2.0655
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 2.085052/52 [==============================] - 0s 6ms/step - loss: 2.0850
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 2.079852/52 [==============================] - 0s 6ms/step - loss: 2.0798
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 2.072352/52 [==============================] - 0s 6ms/step - loss: 2.0723
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 2.075352/52 [==============================] - 0s 6ms/step - loss: 2.0753
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 2.085352/52 [==============================] - 0s 6ms/step - loss: 2.0853
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 2.079452/52 [==============================] - 0s 6ms/step - loss: 2.0794
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 2.078152/52 [==============================] - 0s 6ms/step - loss: 2.0781
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 2.082252/52 [==============================] - 0s 6ms/step - loss: 2.0822
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 2.135152/52 [==============================] - 0s 6ms/step - loss: 2.1351
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 2.118652/52 [==============================] - 0s 6ms/step - loss: 2.1186
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 2.088452/52 [==============================] - 0s 6ms/step - loss: 2.0884
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 2.085852/52 [==============================] - 0s 6ms/step - loss: 2.0858
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 2.096652/52 [==============================] - 0s 6ms/step - loss: 2.0966
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 2.094152/52 [==============================] - 0s 6ms/step - loss: 2.0941
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 2.033652/52 [==============================] - 0s 6ms/step - loss: 2.0336
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 1.975452/52 [==============================] - 0s 6ms/step - loss: 1.9754
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 1.967252/52 [==============================] - 0s 6ms/step - loss: 1.9672

Final time:  524.9763278961182
2022-09-12 20:30:31.335605: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.05
DENSE_SIZES:  [2312, 1466, 1466, 10]
SPARSE_SIZES:  [32, 72, 72, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  2
SECOND_THRESHOLD:  0.95

60000
9984
[26552 27799 54339 ... 40993 40612 58045]
2022-09-12 20:30:40.186399: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 20:30:41.063055: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 7, 2, 1
Build_allocator: 10, 3, 1
Build_allocator: 8, 2, 2
Build_allocator: 11, 3, 2
Build_allocator: 3, 1, 0
Build_allocator: 4, 1, 1
Build_allocator: 5, 1, 2
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 12, 4, 0
Build_allocator: 13, 4, 1
Build_allocator: 14, 4, 2
Build_allocator: 15, 5, 0
Build_allocator: 16, 5, 1
Build_allocator: 17, 5, 2
Build_allocator: 6, 2, 0
Build_allocator: 9, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0x7c29e10
1: &init_state[ilay]0x7c29e18
2: &init_state[ilay]0x7c29e20
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 734, 1467, }
end_tiles: {734, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{4, 4, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 5, 6, 8, 10, 11, 13, 15, 17, 18, 20, 22, 23, 25, 27, 28, 30, 32, 34, 35, 37, 39, 40, 42, 44, 46, 47, 49, 51, 52, 54, 56, 57, 59, 61, 63, 64, 66, 68, 69, 71, 73, 74, 76, 78, 80, 81, 83, 85, 86, 88, 90, 92, 93, 95, 97, 98, 100, 102, 103, 105, 107, 109, 110, 112, 114, 115, 117, 119, 120, 122, 124, 126, 127, 129, 131, 132, 134, 136, 138, 139, 141, 143, 144, 146, 148, 149, 151, 153, 155, 156, 158, 160, 161, 163, 165, 166, 168, 170, 172, 173, 175, 177, 178, 180, 182, 184, 185, 187, 189, 190, 192, 194, 195, 197, 199, 201, 202, 204, 206, 207, 209, 211, 212, 214, 216, 218, 219, 221, 223, 224, 226, 228, 230, 231, 233, 235, 236, 238, 240, 241, 243, 245, 247, 248, 250, 252, 253, 255, 257, 258, 260, 262, 264, 265, 267, 269, 270, 272, 274, 276, 277, 279, 281, 282, 284, 286, 287, 289, 291, 293, 294, 296, 298, 299, 301, 303, 304, 306, 308, 310, 311, 313, 315, 316, 318, 320, 322, 323, 325, 327, 328, 330, 332, 333, 335, 337, 339, 340, 342, 344, 345, 347, 349, 350, 352, 354, 356, 357, 359, 361, 362, 364, 366, 368, 369, 371, 373, 374, 376, 378, 379, 381, 383, 385, 386, 388, 390, 391, 393, 395, 396, 398, 400, 402, 403, 405, 407, 408, 410, 412, 414, 415, 417, 419, 420, 422, 424, 425, 427, 429, 431, 432, 434, 436, 437, 439, 441, 442, 444, 446, 448, 449, 451, 453, 454, 456, 458, 460, 461, 463, 465, 466, 468, 470, 471, 473, 475, 477, 478, 480, 482, 483, 485, 487, 488, 490, 492, 494, 495, 497, 499, 500, 502, 504, 506, 507, 509, 511, 512, 514, 516, 517, 519, 521, 523, 524, 526, 528, 529, 531, 533, 534, 536, 538, 540, 541, 543, 545, 546, 548, 550, 552, 553, 555, 557, 558, 560, 562, 563, 565, 567, 569, 570, 572, 574, 575, 577, 579, 580, 582, 584, 586, 587, 589, 591, 592, 594, 596, 598, 599, 601, 603, 604, 606, 608, 609, 611, 613, 615, 616, 618, 620, 621, 623, 625, 626, 628, 630, 632, 633, 635, 637, 638, 640, 642, 644, 645, 647, 649, 650, 652, }
{0, 13, 27, 40, 54, 68, 81, 95, 109, 122, 136, 149, 163, 177, 190, 204, 218, 231, 245, 258, 272, 286, 299, 313, 327, 340, 354, 368, 381, 395, 408, 422, 436, 449, 463, 477, 490, 504, 517, 531, 545, 558, 572, 586, 599, 613, 626, 640, }
{654, 655, 657, 659, 661, 662, 664, 666, 667, 669, 671, 672, 674, 676, 678, 679, 681, 683, 684, 686, 688, 690, 691, 693, 695, 696, 698, 700, 701, 703, 705, 707, 708, 710, 712, 713, 715, 717, 718, 720, 722, 724, 725, 727, 729, 730, 732, 734, 736, 737, 739, 741, 742, 744, 746, 747, 749, 751, 753, 754, 756, 758, 759, 761, 763, 764, 766, 768, 770, 771, 773, 775, 776, 778, 780, 782, 783, 785, 787, 788, 790, 792, 793, 795, 797, 799, 800, 802, 804, 805, 807, 809, 810, 812, 814, 816, 817, 819, 821, 822, 824, 826, 828, 829, 831, 833, 834, 836, 838, 839, 841, 843, 845, 846, 848, 850, 851, 853, 855, 856, 858, 860, 862, 863, 865, 867, 868, 870, 872, 874, 875, 877, 879, 880, 882, 884, 885, 887, 889, 891, 892, 894, 896, 897, 899, 901, 902, 904, 906, 908, 909, 911, 913, 914, 916, 918, 920, 921, 923, 925, 926, 928, 930, 931, 933, 935, 937, 938, 940, 942, 943, 945, 947, 948, 950, 952, 954, 955, 957, 959, 960, 962, 964, 966, 967, 969, 971, 972, 974, 976, 977, 979, 981, 983, 984, 986, 988, 989, 991, 993, 994, 996, 998, 1000, 1001, 1003, 1005, 1006, 1008, 1010, 1012, 1013, 1015, 1017, 1018, 1020, 1022, 1023, 1025, 1027, 1029, 1030, 1032, 1034, 1035, 1037, 1039, 1040, 1042, 1044, 1046, 1047, 1049, 1051, 1052, 1054, 1056, 1058, 1059, 1061, 1063, 1064, 1066, 1068, 1069, 1071, 1073, 1075, 1076, 1078, 1080, 1081, 1083, 1085, 1086, 1088, 1090, 1092, 1093, 1095, 1097, 1098, 1100, 1102, 1104, 1105, 1107, 1109, 1110, 1112, 1114, 1115, 1117, 1119, 1121, 1122, 1124, 1126, 1127, 1129, 1131, 1132, 1134, 1136, 1138, 1139, 1141, 1143, 1144, 1146, 1148, 1150, 1151, 1153, 1155, 1156, 1158, 1160, 1161, 1163, 1165, 1167, 1168, 1170, 1172, 1173, 1175, 1177, 1178, 1180, 1182, 1184, 1185, 1187, 1189, 1190, 1192, 1194, 1196, 1197, 1199, 1201, 1202, 1204, 1206, 1207, 1209, 1211, 1213, 1214, 1216, 1218, 1219, 1221, 1223, 1224, 1226, 1228, 1230, 1231, 1233, 1235, 1236, 1238, 1240, 1242, 1243, 1245, 1247, 1248, 1250, 1252, 1253, 1255, 1257, 1259, 1260, 1262, 1264, 1265, 1267, 1269, 1270, 1272, 1274, 1276, 1277, 1279, 1281, 1282, 1284, 1286, 1288, 1289, 1291, 1293, 1294, 1296, 1298, 1299, 1301, 1303, 1305, 1306, }
{654, 667, 681, 695, 708, 722, 736, 749, 763, 776, 790, 804, 817, 831, 845, 858, 872, 885, 899, 913, 926, 940, 954, 967, 981, 994, 1008, 1022, 1035, 1049, 1063, 1076, 1090, 1104, 1117, 1131, 1144, 1158, 1172, 1185, 1199, 1213, 1226, 1240, 1253, 1267, 1281, 1294, }
{1308, 1310, 1311, 1313, 1315, 1316, 1318, 1320, 1322, 1323, 1325, 1327, 1328, 1330, 1332, 1334, 1335, 1337, 1339, 1340, 1342, 1344, 1345, 1347, 1349, 1351, 1352, 1354, 1356, 1357, 1359, 1361, 1362, 1364, 1366, 1368, 1369, 1371, 1373, 1374, 1376, 1378, 1380, 1381, 1383, 1385, 1386, 1388, 1390, 1391, 1393, 1395, 1397, 1398, 1400, 1402, 1403, 1405, 1407, 1408, 1410, 1412, 1414, 1415, 1417, 1419, 1420, 1422, 1424, 1426, 1427, 1429, 1431, 1432, 1434, 1436, 1437, 1439, 1441, 1443, 1444, 1446, 1448, 1449, 1451, 1453, 1454, 1456, 1458, 1460, 1461, 1463, 1465, 1466, 1468, 1470, }
{1308, 1311, 1315, 1318, 1322, 1325, 1328, 1332, 1335, 1339, 1342, 1345, 1349, 1352, 1356, 1359, 1362, 1366, 1369, 1373, 1376, 1380, 1383, 1386, 1390, 1393, 1397, 1400, 1403, 1407, 1410, 1414, 1417, 1420, 1424, 1427, 1431, 1434, 1437, 1441, 1444, 1448, 1451, 1454, 1458, 1461, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 4
num_workers: 24
max_num_states_per_worker: 62
sparse_size_to_use: 62
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 4
num_workers: 24
max_num_states_per_worker: 62
sparse_size_to_use: 62
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,4,72}
 repeated_sparse_spike_ids.shapeToString(): {48,2,4,72}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,72}
slicedInpSpikeIds.shapeToString(): {48,72}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,72}
slicedInpSpikeIds.shapeToString(): {48,72}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{1444, 26, }
start_tile: 1
num_tiles_per_layer[ilay]: 1444
start_tile + num_tiles_per_layer[ilay]: 1445
num_occupied_tiles: 733
batchsize: 48
sparse_size: 72
dLdx.shapeToString(): {733,48,72}
replicated_numGrads.shapeToString(): {733,48}
exchangeGroupSize: 24
numExchangeGroups: 30
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
start_id: 456, end_id: 480
start_id: 480, end_id: 504
start_id: 504, end_id: 528
start_id: 528, end_id: 552
start_id: 552, end_id: 576
start_id: 576, end_id: 600
start_id: 600, end_id: 624
start_id: 624, end_id: 648
start_id: 648, end_id: 672
start_id: 672, end_id: 696
lastExchangeGroupLarger
start_id: 696, end_id: 733
exchangeGroupSizeLast: 37
exchangeBatchSizeLast: 2
start_tile: 1445
num_tiles_per_layer[ilay]: 26
start_tile + num_tiles_per_layer[ilay]: 1471
num_occupied_tiles: 5
batchsize: 48
sparse_size: 72
dLdx.shapeToString(): {5,48,72}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 20:46:16.670769: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[1466 1466   10]
0 733.0
1 733.0
2 5.0
[TileMapping(start_tile=1, end_tile=734), TileMapping(start_tile=734, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=734), TileMapping(start_tile=734, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.2558990251065398
limit=0.2558990251065398
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 1466) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 1466) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_72_72_2_48_1_734_1467_734_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 72), (100 5562034     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][2
                                                                 keras_multi_lif_layer_sparse[0][5
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 5,562,034
Trainable params: 5,553,208
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_72_72_2_48_1_734_1467_734_1467_1472
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_72_72_2_48_1_734_1467_734_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.488052/52 [==============================] - 938s 18s/step - loss: 2.4880
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.293052/52 [==============================] - 0s 9ms/step - loss: 2.2930
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 2.252352/52 [==============================] - 0s 8ms/step - loss: 2.2523
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 2.054452/52 [==============================] - 0s 8ms/step - loss: 2.0544
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 1.907652/52 [==============================] - 0s 8ms/step - loss: 1.9076
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 1.955252/52 [==============================] - 0s 8ms/step - loss: 1.9552
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 1.889752/52 [==============================] - 0s 8ms/step - loss: 1.8897
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 1.828952/52 [==============================] - 0s 8ms/step - loss: 1.8289
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 1.817752/52 [==============================] - 0s 8ms/step - loss: 1.8177
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 1.732452/52 [==============================] - 0s 8ms/step - loss: 1.7324
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 1.734052/52 [==============================] - 1s 10ms/step - loss: 1.7340
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 1.689152/52 [==============================] - 1s 10ms/step - loss: 1.6891
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 1.700252/52 [==============================] - 0s 9ms/step - loss: 1.7002
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 1.670852/52 [==============================] - 0s 9ms/step - loss: 1.6708
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 1.697952/52 [==============================] - 0s 9ms/step - loss: 1.6979
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 1.653752/52 [==============================] - 0s 9ms/step - loss: 1.6537
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 1.660352/52 [==============================] - 0s 9ms/step - loss: 1.6603
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 1.654352/52 [==============================] - 0s 9ms/step - loss: 1.6543
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 1.657752/52 [==============================] - 0s 9ms/step - loss: 1.6577
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 1.654652/52 [==============================] - 0s 9ms/step - loss: 1.6546
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 1.664852/52 [==============================] - 0s 9ms/step - loss: 1.6648
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 1.681252/52 [==============================] - 0s 9ms/step - loss: 1.6812
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 1.683452/52 [==============================] - 0s 9ms/step - loss: 1.6834
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 1.669252/52 [==============================] - 0s 9ms/step - loss: 1.6692
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 1.671752/52 [==============================] - 0s 9ms/step - loss: 1.6717

Final time:  949.032158613205
2022-09-12 20:46:31.423367: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.05
DENSE_SIZES:  [2312, 978, 978, 976, 10]
SPARSE_SIZES:  [32, 48, 48, 48, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  3
SECOND_THRESHOLD:  0.95

60000
9984
[ 5614 30052 27782 ... 51594 15859 13698]
2022-09-12 20:46:39.606479: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 20:46:40.459041: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 9, 2, 1
Build_allocator: 13, 3, 1
Build_allocator: 10, 2, 2
Build_allocator: 14, 3, 2
Build_allocator: 11, 2, 3
Build_allocator: 15, 3, 3
Build_allocator: 4, 1, 0
Build_allocator: 5, 1, 1
Build_allocator: 6, 1, 2
Build_allocator: 7, 1, 3
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 16, 4, 0
Build_allocator: 17, 4, 1
Build_allocator: 18, 4, 2
Build_allocator: 19, 4, 3
Build_allocator: 20, 5, 0
Build_allocator: 21, 5, 1
Build_allocator: 22, 5, 2
Build_allocator: 23, 5, 3
Build_allocator: 8, 2, 0
Build_allocator: 12, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0x21ea90c0
1: &init_state[ilay]0x21ea90c8
2: &init_state[ilay]0x21ea90d0
3: &init_state[ilay]0x21ea90d8
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 490, 979, 1467, }
end_tiles: {490, 979, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{3, 3, 3, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 4, 6, 7, 9, 10, 12, 13, 15, 16, 18, 19, 21, 23, 24, 26, 27, 29, 30, 32, 33, 35, 36, 38, 39, 41, 42, 44, 46, 47, 49, 50, 52, 53, 55, 56, 58, 59, 61, 62, 64, 65, 67, 69, 70, 72, 73, 75, 76, 78, 79, 81, 82, 84, 85, 87, 88, 90, 92, 93, 95, 96, 98, 99, 101, 102, 104, 105, 107, 108, 110, 111, 113, 115, 116, 118, 119, 121, 122, 124, 125, 127, 128, 130, 131, 133, 134, 136, 138, 139, 141, 142, 144, 145, 147, 148, 150, 151, 153, 154, 156, 157, 159, 161, 162, 164, 165, 167, 168, 170, 171, 173, 174, 176, 177, 179, 180, 182, 184, 185, 187, 188, 190, 191, 193, 194, 196, 197, 199, 200, 202, 203, 205, 207, 208, 210, 211, 213, 214, 216, 217, 219, 220, 222, 223, 225, 226, 228, 230, 231, 233, 234, 236, 237, 239, 240, 242, 243, 245, 246, 248, 249, 251, 253, 254, 256, 257, 259, 260, 262, 263, 265, 266, 268, 269, 271, 272, 274, 276, 277, 279, 280, 282, 283, 285, 286, 288, 289, 291, 292, 294, 295, 297, 299, 300, 302, 303, 305, 306, 308, 309, 311, 312, 314, 315, 317, 318, 320, 322, 323, 325, 326, 328, 329, 331, 332, 334, 335, 337, 338, 340, 341, 343, 345, 346, 348, 349, 351, 352, 354, 355, 357, 358, 360, 361, 363, 364, 366, 368, 369, 371, 372, 374, 375, 377, 378, 380, 381, 383, 384, 386, 387, 389, 391, 392, 394, 395, 397, 398, 400, 401, 403, 404, 406, 407, 409, 410, 412, 414, 415, 417, 418, 420, 421, 423, 424, 426, 427, 429, 430, 432, 433, 435, 437, 438, 440, }
{0, 9, 18, 27, 36, 46, 55, 64, 73, 82, 92, 101, 110, 119, 128, 138, 147, 156, 165, 174, 184, 193, 202, 211, 220, 230, 239, 248, 257, 266, 276, 285, 294, 303, 312, 322, 331, 340, 349, 358, 368, 377, 386, 395, 404, 414, 423, 432, }
{441, 443, 444, 446, 447, 449, 450, 452, 453, 455, 456, 458, 460, 461, 463, 464, 466, 467, 469, 470, 472, 473, 475, 476, 478, 479, 481, 483, 484, 486, 487, 489, 490, 492, 493, 495, 496, 498, 499, 501, 502, 504, 506, 507, 509, 510, 512, 513, 515, 516, 518, 519, 521, 522, 524, 525, 527, 529, 530, 532, 533, 535, 536, 538, 539, 541, 542, 544, 545, 547, 548, 550, 552, 553, 555, 556, 558, 559, 561, 562, 564, 565, 567, 568, 570, 571, 573, 575, 576, 578, 579, 581, 582, 584, 585, 587, 588, 590, 591, 593, 594, 596, 598, 599, 601, 602, 604, 605, 607, 608, 610, 611, 613, 614, 616, 617, 619, 621, 622, 624, 625, 627, 628, 630, 631, 633, 634, 636, 637, 639, 640, 642, 644, 645, 647, 648, 650, 651, 653, 654, 656, 657, 659, 660, 662, 663, 665, 667, 668, 670, 671, 673, 674, 676, 677, 679, 680, 682, 683, 685, 686, 688, 690, 691, 693, 694, 696, 697, 699, 700, 702, 703, 705, 706, 708, 709, 711, 713, 714, 716, 717, 719, 720, 722, 723, 725, 726, 728, 729, 731, 732, 734, 736, 737, 739, 740, 742, 743, 745, 746, 748, 749, 751, 752, 754, 755, 757, 759, 760, 762, 763, 765, 766, 768, 769, 771, 772, 774, 775, 777, 778, 780, 782, 783, 785, 786, 788, 789, 791, 792, 794, 795, 797, 798, 800, 801, 803, 805, 806, 808, 809, 811, 812, 814, 815, 817, 818, 820, 821, 823, 824, 826, 828, 829, 831, 832, 834, 835, 837, 838, 840, 841, 843, 844, 846, 847, 849, 851, 852, 854, 855, 857, 858, 860, 861, 863, 864, 866, 867, 869, 870, 872, 874, 875, 877, 878, 880, 881, }
{441, 450, 460, 469, 478, 487, 496, 506, 515, 524, 533, 542, 552, 561, 570, 579, 588, 598, 607, 616, 625, 634, 644, 653, 662, 671, 680, 690, 699, 708, 717, 726, 736, 745, 754, 763, 772, 782, 791, 800, 809, 818, 828, 837, 846, 855, 864, 874, }
{883, 884, 886, 887, 889, 890, 892, 893, 895, 897, 898, 900, 901, 903, 904, 906, 907, 909, 910, 912, 913, 915, 916, 918, 920, 921, 923, 924, 926, 927, 929, 930, 932, 933, 935, 936, 938, 939, 941, 943, 944, 946, 947, 949, 950, 952, 953, 955, 956, 958, 959, 961, 962, 964, 966, 967, 969, 970, 972, 973, 975, 976, 978, 979, 981, 982, 984, 985, 987, 989, 990, 992, 993, 995, 996, 998, 999, 1001, 1002, 1004, 1005, 1007, 1008, 1010, 1012, 1013, 1015, 1016, 1018, 1019, 1021, 1022, 1024, 1025, 1027, 1028, 1030, 1031, 1033, 1035, 1036, 1038, 1039, 1041, 1042, 1044, 1045, 1047, 1048, 1050, 1051, 1053, 1054, 1056, 1058, 1059, 1061, 1062, 1064, 1065, 1067, 1068, 1070, 1071, 1073, 1074, 1076, 1077, 1079, 1081, 1082, 1084, 1085, 1087, 1088, 1090, 1091, 1093, 1094, 1096, 1097, 1099, 1100, 1102, 1104, 1105, 1107, 1108, 1110, 1111, 1113, 1114, 1116, 1117, 1119, 1120, 1122, 1123, 1125, 1127, 1128, 1130, 1131, 1133, 1134, 1136, 1137, 1139, 1140, 1142, 1143, 1145, 1146, 1148, 1150, 1151, 1153, 1154, 1156, 1157, 1159, 1160, 1162, 1163, 1165, 1166, 1168, 1169, 1171, 1173, 1174, 1176, 1177, 1179, 1180, 1182, 1183, 1185, 1186, 1188, 1189, 1191, 1192, 1194, 1196, 1197, 1199, 1200, 1202, 1203, 1205, 1206, 1208, 1209, 1211, 1212, 1214, 1215, 1217, 1219, 1220, 1222, 1223, 1225, 1226, 1228, 1229, 1231, 1232, 1234, 1235, 1237, 1238, 1240, 1242, 1243, 1245, 1246, 1248, 1249, 1251, 1252, 1254, 1255, 1257, 1258, 1260, 1261, 1263, 1265, 1266, 1268, 1269, 1271, 1272, 1274, 1275, 1277, 1278, 1280, 1281, 1283, 1284, 1286, 1288, 1289, 1291, 1292, 1294, 1295, 1297, 1298, 1300, 1301, 1303, 1304, 1306, 1307, 1309, 1311, 1312, 1314, 1315, 1317, 1318, 1320, 1321, 1323, }
{883, 892, 901, 910, 920, 929, 938, 947, 956, 966, 975, 984, 993, 1002, 1012, 1021, 1030, 1039, 1048, 1058, 1067, 1076, 1085, 1094, 1104, 1113, 1122, 1131, 1140, 1150, 1159, 1168, 1177, 1186, 1196, 1205, 1214, 1223, 1232, 1242, 1251, 1260, 1269, 1278, 1288, 1297, 1306, 1315, }
{1324, 1326, 1327, 1329, 1330, 1332, 1334, 1335, 1337, 1338, 1340, 1341, 1343, 1344, 1346, 1347, 1349, 1350, 1352, 1353, 1355, 1357, 1358, 1360, 1361, 1363, 1364, 1366, 1367, 1369, 1370, 1372, 1373, 1375, 1376, 1378, 1380, 1381, 1383, 1384, 1386, 1387, 1389, 1390, 1392, 1393, 1395, 1396, 1398, 1399, 1401, 1403, 1404, 1406, 1407, 1409, 1410, 1412, 1413, 1415, 1416, 1418, 1419, 1421, 1422, 1424, 1426, 1427, 1429, 1430, 1432, 1433, 1435, 1436, 1438, 1439, 1441, 1442, 1444, 1445, 1447, 1449, 1450, 1452, 1453, 1455, 1456, 1458, 1459, 1461, 1462, 1464, 1465, 1467, 1468, 1470, }
{1324, 1327, 1330, 1334, 1337, 1340, 1343, 1346, 1349, 1352, 1355, 1358, 1361, 1364, 1367, 1370, 1373, 1376, 1380, 1383, 1386, 1389, 1392, 1395, 1398, 1401, 1404, 1407, 1410, 1413, 1416, 1419, 1422, 1426, 1429, 1432, 1435, 1438, 1441, 1444, 1447, 1450, 1453, 1456, 1459, 1462, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 48
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 48
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 48
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,48}
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,48}
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,48}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,48}
slicedInpSpikeIds.shapeToString(): {48,48}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,48}
slicedInpSpikeIds.shapeToString(): {48,48}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,48}
slicedInpSpikeIds.shapeToString(): {48,48}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {488,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{726, 725, 18, }
start_tile: 1
num_tiles_per_layer[ilay]: 726
start_tile + num_tiles_per_layer[ilay]: 727
num_occupied_tiles: 489
batchsize: 48
sparse_size: 48
dLdx.shapeToString(): {489,48,48}
replicated_numGrads.shapeToString(): {489,48}
exchangeGroupSize: 24
numExchangeGroups: 20
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
lastExchangeGroupLarger
start_id: 456, end_id: 489
exchangeGroupSizeLast: 33
exchangeBatchSizeLast: 2
start_tile: 727
num_tiles_per_layer[ilay]: 725
start_tile + num_tiles_per_layer[ilay]: 1452
num_occupied_tiles: 488
batchsize: 48
sparse_size: 48
dLdx.shapeToString(): {488,48,48}
replicated_numGrads.shapeToString(): {488,48}
exchangeGroupSize: 24
numExchangeGroups: 20
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
lastExchangeGroupLarger
start_id: 456, end_id: 488
exchangeGroupSizeLast: 32
exchangeBatchSizeLast: 2
start_tile: 1452
num_tiles_per_layer[ilay]: 18
start_tile + num_tiles_per_layer[ilay]: 1470
num_occupied_tiles: 5
batchsize: 48
sparse_size: 48
dLdx.shapeToString(): {5,48,48}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {488,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 21:06:18.075473: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[978 978 976  10]
0 489.0
1 489.0
2 488.0
3 5.0
[TileMapping(start_tile=1, end_tile=490), TileMapping(start_tile=490, end_tile=979), TileMapping(start_tile=979, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=490), TileMapping(start_tile=490, end_tile=979), TileMapping(start_tile=979, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.31330417999518295
limit=0.31330417999518295
limit=0.31362502409359
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 978) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 978) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 976) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_48_48_48_2_48_1_490_979_1467_490_979_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 48), (100 4190734     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][3
                                                                 keras_multi_lif_layer_sparse[0][7
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 4,190,734
Trainable params: 4,181,908
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_48_48_48_2_48_1_490_979_1467_490_979_1467_1472
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_48_48_48_2_48_1_490_979_1467_490_979_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.516752/52 [==============================] - 1180s 23s/step - loss: 2.5167
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.075552/52 [==============================] - 1s 14ms/step - loss: 2.0755
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 1.965952/52 [==============================] - 0s 7ms/step - loss: 1.9659
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 1.922952/52 [==============================] - 0s 7ms/step - loss: 1.9229
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 1.868452/52 [==============================] - 0s 10ms/step - loss: 1.8684
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 1.869452/52 [==============================] - 0s 7ms/step - loss: 1.8694
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 1.866952/52 [==============================] - 0s 7ms/step - loss: 1.8669
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 1.892552/52 [==============================] - 0s 7ms/step - loss: 1.8925
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 1.840452/52 [==============================] - 0s 9ms/step - loss: 1.8404
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 1.847452/52 [==============================] - 0s 7ms/step - loss: 1.8474
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 1.841752/52 [==============================] - 0s 8ms/step - loss: 1.8417
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 1.876352/52 [==============================] - 0s 7ms/step - loss: 1.8763
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 1.834252/52 [==============================] - 0s 7ms/step - loss: 1.8342
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 1.833452/52 [==============================] - 0s 8ms/step - loss: 1.8334
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 1.842852/52 [==============================] - 0s 7ms/step - loss: 1.8428
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 1.882552/52 [==============================] - 0s 7ms/step - loss: 1.8825
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 1.835252/52 [==============================] - 0s 7ms/step - loss: 1.8352
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 1.830152/52 [==============================] - 0s 7ms/step - loss: 1.8301
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 1.837852/52 [==============================] - 0s 7ms/step - loss: 1.8378
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 1.870752/52 [==============================] - 0s 7ms/step - loss: 1.8707
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 1.823852/52 [==============================] - 0s 7ms/step - loss: 1.8238
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 1.836152/52 [==============================] - 0s 7ms/step - loss: 1.8361
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 1.858552/52 [==============================] - 0s 7ms/step - loss: 1.8585
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 1.873352/52 [==============================] - 0s 7ms/step - loss: 1.8733
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 1.831152/52 [==============================] - 0s 7ms/step - loss: 1.8311

Final time:  1189.1968417167664
2022-09-12 21:06:30.673915: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.05
DENSE_SIZES:  [2312, 734, 734, 732, 732, 10]
SPARSE_SIZES:  [32, 36, 36, 36, 36, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  4
SECOND_THRESHOLD:  0.95

60000
9984
[  981 58844 37922 ... 53978 16552 12756]
2022-09-12 21:06:39.892773: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 21:06:40.763958: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 11, 2, 1
Build_allocator: 16, 3, 1
Build_allocator: 12, 2, 2
Build_allocator: 17, 3, 2
Build_allocator: 13, 2, 3
Build_allocator: 18, 3, 3
Build_allocator: 14, 2, 4
Build_allocator: 19, 3, 4
Build_allocator: 5, 1, 0
Build_allocator: 6, 1, 1
Build_allocator: 7, 1, 2
Build_allocator: 8, 1, 3
Build_allocator: 9, 1, 4
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 4, 0, 4
Build_allocator: 20, 4, 0
Build_allocator: 21, 4, 1
Build_allocator: 22, 4, 2
Build_allocator: 23, 4, 3
Build_allocator: 24, 4, 4
Build_allocator: 25, 5, 0
Build_allocator: 26, 5, 1
Build_allocator: 27, 5, 2
Build_allocator: 28, 5, 3
Build_allocator: 29, 5, 4
Build_allocator: 10, 2, 0
Build_allocator: 15, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0x104e0960
1: &init_state[ilay]0x104e0968
2: &init_state[ilay]0x104e0970
3: &init_state[ilay]0x104e0978
4: &init_state[ilay]0x104e0980
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0
3: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
3: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 368, 735, 1101, 1467, }
end_tiles: {368, 735, 1101, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{2, 2, 2, 2, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 5, 6, 8, 10, 11, 13, 15, 17, 18, 20, 22, 23, 25, 27, 28, 30, 32, 34, 35, 37, 39, 40, 42, 44, 46, 47, 49, 51, 52, 54, 56, 57, 59, 61, 63, 64, 66, 68, 69, 71, 73, 74, 76, 78, 80, 81, 83, 85, 86, 88, 90, 92, 93, 95, 97, 98, 100, 102, 103, 105, 107, 109, 110, 112, 114, 115, 117, 119, 120, 122, 124, 126, 127, 129, 131, 132, 134, 136, 138, 139, 141, 143, 144, 146, 148, 149, 151, 153, 155, 156, 158, 160, 161, 163, 165, 166, 168, 170, 172, 173, 175, 177, 178, 180, 182, 184, 185, 187, 189, 190, 192, 194, 195, 197, 199, 201, 202, 204, 206, 207, 209, 211, 212, 214, 216, 218, 219, 221, 223, 224, 226, 228, 230, 231, 233, 235, 236, 238, 240, 241, 243, 245, 247, 248, 250, 252, 253, 255, 257, 258, 260, 262, 264, 265, 267, 269, 270, 272, 274, 276, 277, 279, 281, 282, 284, 286, 287, 289, 291, 293, 294, 296, 298, 299, 301, 303, 304, 306, 308, 310, 311, 313, 315, 316, 318, 320, 322, 323, 325, }
{0, 6, 13, 20, 27, 34, 40, 47, 54, 61, 68, 74, 81, 88, 95, 102, 109, 115, 122, 129, 136, 143, 149, 156, 163, 170, 177, 184, 190, 197, 204, 211, 218, 224, 231, 238, 245, 252, 258, 265, 272, 279, 286, 293, 299, 306, 313, 320, }
{327, 328, 330, 332, 333, 335, 337, 339, 340, 342, 344, 345, 347, 349, 350, 352, 354, 356, 357, 359, 361, 362, 364, 366, 368, 369, 371, 373, 374, 376, 378, 379, 381, 383, 385, 386, 388, 390, 391, 393, 395, 396, 398, 400, 402, 403, 405, 407, 408, 410, 412, 414, 415, 417, 419, 420, 422, 424, 425, 427, 429, 431, 432, 434, 436, 437, 439, 441, 442, 444, 446, 448, 449, 451, 453, 454, 456, 458, 460, 461, 463, 465, 466, 468, 470, 471, 473, 475, 477, 478, 480, 482, 483, 485, 487, 488, 490, 492, 494, 495, 497, 499, 500, 502, 504, 506, 507, 509, 511, 512, 514, 516, 517, 519, 521, 523, 524, 526, 528, 529, 531, 533, 534, 536, 538, 540, 541, 543, 545, 546, 548, 550, 552, 553, 555, 557, 558, 560, 562, 563, 565, 567, 569, 570, 572, 574, 575, 577, 579, 580, 582, 584, 586, 587, 589, 591, 592, 594, 596, 598, 599, 601, 603, 604, 606, 608, 609, 611, 613, 615, 616, 618, 620, 621, 623, 625, 626, 628, 630, 632, 633, 635, 637, 638, 640, 642, 644, 645, 647, 649, 650, 652, }
{327, 333, 340, 347, 354, 361, 368, 374, 381, 388, 395, 402, 408, 415, 422, 429, 436, 442, 449, 456, 463, 470, 477, 483, 490, 497, 504, 511, 517, 524, 531, 538, 545, 552, 558, 565, 572, 579, 586, 592, 599, 606, 613, 620, 626, 633, 640, 647, }
{654, 655, 657, 659, 661, 662, 664, 666, 667, 669, 671, 672, 674, 676, 678, 679, 681, 683, 684, 686, 688, 690, 691, 693, 695, 696, 698, 700, 701, 703, 705, 707, 708, 710, 712, 713, 715, 717, 718, 720, 722, 724, 725, 727, 729, 730, 732, 734, 736, 737, 739, 741, 742, 744, 746, 747, 749, 751, 753, 754, 756, 758, 759, 761, 763, 764, 766, 768, 770, 771, 773, 775, 776, 778, 780, 782, 783, 785, 787, 788, 790, 792, 793, 795, 797, 799, 800, 802, 804, 805, 807, 809, 810, 812, 814, 816, 817, 819, 821, 822, 824, 826, 828, 829, 831, 833, 834, 836, 838, 839, 841, 843, 845, 846, 848, 850, 851, 853, 855, 856, 858, 860, 862, 863, 865, 867, 868, 870, 872, 874, 875, 877, 879, 880, 882, 884, 885, 887, 889, 891, 892, 894, 896, 897, 899, 901, 902, 904, 906, 908, 909, 911, 913, 914, 916, 918, 920, 921, 923, 925, 926, 928, 930, 931, 933, 935, 937, 938, 940, 942, 943, 945, 947, 948, 950, 952, 954, 955, 957, 959, 960, 962, 964, 966, 967, 969, 971, 972, 974, 976, 977, 979, }
{654, 661, 667, 674, 681, 688, 695, 701, 708, 715, 722, 729, 736, 742, 749, 756, 763, 770, 776, 783, 790, 797, 804, 810, 817, 824, 831, 838, 845, 851, 858, 865, 872, 879, 885, 892, 899, 906, 913, 920, 926, 933, 940, 947, 954, 960, 967, 974, }
{981, 983, 984, 986, 988, 989, 991, 993, 994, 996, 998, 1000, 1001, 1003, 1005, 1006, 1008, 1010, 1012, 1013, 1015, 1017, 1018, 1020, 1022, 1023, 1025, 1027, 1029, 1030, 1032, 1034, 1035, 1037, 1039, 1040, 1042, 1044, 1046, 1047, 1049, 1051, 1052, 1054, 1056, 1058, 1059, 1061, 1063, 1064, 1066, 1068, 1069, 1071, 1073, 1075, 1076, 1078, 1080, 1081, 1083, 1085, 1086, 1088, 1090, 1092, 1093, 1095, 1097, 1098, 1100, 1102, 1104, 1105, 1107, 1109, 1110, 1112, 1114, 1115, 1117, 1119, 1121, 1122, 1124, 1126, 1127, 1129, 1131, 1132, 1134, 1136, 1138, 1139, 1141, 1143, 1144, 1146, 1148, 1150, 1151, 1153, 1155, 1156, 1158, 1160, 1161, 1163, 1165, 1167, 1168, 1170, 1172, 1173, 1175, 1177, 1178, 1180, 1182, 1184, 1185, 1187, 1189, 1190, 1192, 1194, 1196, 1197, 1199, 1201, 1202, 1204, 1206, 1207, 1209, 1211, 1213, 1214, 1216, 1218, 1219, 1221, 1223, 1224, 1226, 1228, 1230, 1231, 1233, 1235, 1236, 1238, 1240, 1242, 1243, 1245, 1247, 1248, 1250, 1252, 1253, 1255, 1257, 1259, 1260, 1262, 1264, 1265, 1267, 1269, 1270, 1272, 1274, 1276, 1277, 1279, 1281, 1282, 1284, 1286, 1288, 1289, 1291, 1293, 1294, 1296, 1298, 1299, 1301, 1303, 1305, 1306, }
{981, 988, 994, 1001, 1008, 1015, 1022, 1029, 1035, 1042, 1049, 1056, 1063, 1069, 1076, 1083, 1090, 1097, 1104, 1110, 1117, 1124, 1131, 1138, 1144, 1151, 1158, 1165, 1172, 1178, 1185, 1192, 1199, 1206, 1213, 1219, 1226, 1233, 1240, 1247, 1253, 1260, 1267, 1274, 1281, 1288, 1294, 1301, }
{1308, 1310, 1311, 1313, 1315, 1316, 1318, 1320, 1322, 1323, 1325, 1327, 1328, 1330, 1332, 1334, 1335, 1337, 1339, 1340, 1342, 1344, 1345, 1347, 1349, 1351, 1352, 1354, 1356, 1357, 1359, 1361, 1362, 1364, 1366, 1368, 1369, 1371, 1373, 1374, 1376, 1378, 1380, 1381, 1383, 1385, 1386, 1388, 1390, 1391, 1393, 1395, 1397, 1398, 1400, 1402, 1403, 1405, 1407, 1408, 1410, 1412, 1414, 1415, 1417, 1419, 1420, 1422, 1424, 1426, 1427, 1429, 1431, 1432, 1434, 1436, 1437, 1439, 1441, 1443, 1444, 1446, 1448, 1449, 1451, 1453, 1454, 1456, 1458, 1460, 1461, 1463, 1465, 1466, 1468, 1470, }
{1308, 1311, 1315, 1318, 1322, 1325, 1328, 1332, 1335, 1339, 1342, 1345, 1349, 1352, 1356, 1359, 1362, 1366, 1369, 1373, 1376, 1380, 1383, 1386, 1390, 1393, 1397, 1400, 1403, 1407, 1410, 1414, 1417, 1420, 1424, 1427, 1431, 1434, 1437, 1441, 1444, 1448, 1451, 1454, 1458, 1461, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 62
sparse_size_to_use: 36
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 62
sparse_size_to_use: 36
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 61
sparse_size_to_use: 36
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 61
sparse_size_to_use: 36
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,36}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,36}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,36}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,36}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 4
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,36}
slicedInpSpikeIds.shapeToString(): {48,36}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,36}
slicedInpSpikeIds.shapeToString(): {48,36}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,36}
slicedInpSpikeIds.shapeToString(): {48,36}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

3
out_spike_ids[ilay].shapeToString(): {100,48,36}
slicedInpSpikeIds.shapeToString(): {48,36}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,2}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{485, 484, 484, 15, }
start_tile: 1
num_tiles_per_layer[ilay]: 485
start_tile + num_tiles_per_layer[ilay]: 486
num_occupied_tiles: 367
batchsize: 48
sparse_size: 36
dLdx.shapeToString(): {367,48,36}
replicated_numGrads.shapeToString(): {367,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 367
exchangeGroupSizeLast: 31
exchangeBatchSizeLast: 2
start_tile: 486
num_tiles_per_layer[ilay]: 484
start_tile + num_tiles_per_layer[ilay]: 970
num_occupied_tiles: 366
batchsize: 48
sparse_size: 36
dLdx.shapeToString(): {366,48,36}
replicated_numGrads.shapeToString(): {366,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 366
exchangeGroupSizeLast: 30
exchangeBatchSizeLast: 2
start_tile: 970
num_tiles_per_layer[ilay]: 484
start_tile + num_tiles_per_layer[ilay]: 1454
num_occupied_tiles: 366
batchsize: 48
sparse_size: 36
dLdx.shapeToString(): {366,48,36}
replicated_numGrads.shapeToString(): {366,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 366
exchangeGroupSizeLast: 30
exchangeBatchSizeLast: 2
start_tile: 1454
num_tiles_per_layer[ilay]: 15
start_tile + num_tiles_per_layer[ilay]: 1469
num_occupied_tiles: 5
batchsize: 48
sparse_size: 36
dLdx.shapeToString(): {5,48,36}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,1}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 21:20:22.984638: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[734 734 732 732  10]
0 367.0
1 367.0
2 366.0
3 366.0
4 5.0
[TileMapping(start_tile=1, end_tile=368), TileMapping(start_tile=368, end_tile=735), TileMapping(start_tile=735, end_tile=1101), TileMapping(start_tile=1101, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=368), TileMapping(start_tile=368, end_tile=735), TileMapping(start_tile=735, end_tile=1101), TileMapping(start_tile=1101, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.3616492648073473
limit=0.3616492648073473
limit=0.3621429841700741
limit=0.3621429841700741
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 734) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 734) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 732) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 732) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_5:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_36_36_36_36_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 36), (100 3325022     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][4
                                                                 keras_multi_lif_layer_sparse[0][9
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 3,325,022
Trainable params: 3,316,196
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_36_36_36_36_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_36_36_36_36_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.435852/52 [==============================] - 824s 16s/step - loss: 2.4358
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.146952/52 [==============================] - 0s 6ms/step - loss: 2.1469
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 2.182052/52 [==============================] - 0s 6ms/step - loss: 2.1820
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 2.113552/52 [==============================] - 0s 6ms/step - loss: 2.1135
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 2.163552/52 [==============================] - 0s 6ms/step - loss: 2.1635
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 2.124452/52 [==============================] - 0s 6ms/step - loss: 2.1244
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 2.128652/52 [==============================] - 0s 6ms/step - loss: 2.1286
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 2.097652/52 [==============================] - 0s 6ms/step - loss: 2.0976
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 2.131152/52 [==============================] - 0s 6ms/step - loss: 2.1311
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 2.114252/52 [==============================] - 0s 6ms/step - loss: 2.1142
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 2.124752/52 [==============================] - 0s 6ms/step - loss: 2.1247
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 2.086552/52 [==============================] - 0s 6ms/step - loss: 2.0865
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 2.130252/52 [==============================] - 0s 6ms/step - loss: 2.1302
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 2.124552/52 [==============================] - 0s 6ms/step - loss: 2.1245
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 2.120952/52 [==============================] - 0s 6ms/step - loss: 2.1209
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 2.091552/52 [==============================] - 0s 7ms/step - loss: 2.0915
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 2.138452/52 [==============================] - 0s 7ms/step - loss: 2.1384
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 2.132152/52 [==============================] - 0s 6ms/step - loss: 2.1321
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 2.117552/52 [==============================] - 0s 6ms/step - loss: 2.1175
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 2.098452/52 [==============================] - 0s 7ms/step - loss: 2.0984
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 2.054652/52 [==============================] - 0s 7ms/step - loss: 2.0546
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 1.936052/52 [==============================] - 0s 6ms/step - loss: 1.9360
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 1.955252/52 [==============================] - 0s 6ms/step - loss: 1.9552
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 1.957852/52 [==============================] - 0s 6ms/step - loss: 1.9578
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 1.953252/52 [==============================] - 0s 6ms/step - loss: 1.9532

Final time:  832.1098830699921
2022-09-12 21:20:33.871671: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.05
DENSE_SIZES:  [2312, 588, 586, 586, 586, 586, 10]
SPARSE_SIZES:  [32, 28, 28, 28, 28, 28, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  5
SECOND_THRESHOLD:  0.95

60000
9984
[35256 42390 50305 ...  1680 24721    14]
2022-09-12 21:20:43.495677: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 21:20:44.394236: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 13, 2, 1
Build_allocator: 19, 3, 1
Build_allocator: 14, 2, 2
Build_allocator: 20, 3, 2
Build_allocator: 15, 2, 3
Build_allocator: 21, 3, 3
Build_allocator: 16, 2, 4
Build_allocator: 22, 3, 4
Build_allocator: 17, 2, 5
Build_allocator: 23, 3, 5
Build_allocator: 6, 1, 0
Build_allocator: 7, 1, 1
Build_allocator: 8, 1, 2
Build_allocator: 9, 1, 3
Build_allocator: 10, 1, 4
Build_allocator: 11, 1, 5
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 4, 0, 4
Build_allocator: 5, 0, 5
Build_allocator: 24, 4, 0
Build_allocator: 25, 4, 1
Build_allocator: 26, 4, 2
Build_allocator: 27, 4, 3
Build_allocator: 28, 4, 4
Build_allocator: 29, 4, 5
Build_allocator: 30, 5, 0
Build_allocator: 31, 5, 1
Build_allocator: 32, 5, 2
Build_allocator: 33, 5, 3
Build_allocator: 34, 5, 4
Build_allocator: 35, 5, 5
Build_allocator: 12, 2, 0
Build_allocator: 18, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0xdd89800
1: &init_state[ilay]0xdd89808
2: &init_state[ilay]0xdd89810
3: &init_state[ilay]0xdd89818
4: &init_state[ilay]0xdd89820
5: &init_state[ilay]0xdd89828
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0
3: init_state[ilay]==init_state[ilay+1]: 0
4: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
3: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
4: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 295, 588, 881, 1174, 1467, }
end_tiles: {295, 588, 881, 1174, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{2, 2, 2, 2, 2, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 2, 4, 5, 6, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 25, 26, 27, 29, 30, 32, 33, 34, 36, 37, 39, 40, 41, 43, 44, 46, 47, 48, 50, 51, 52, 54, 55, 57, 58, 59, 61, 62, 64, 65, 66, 68, 69, 71, 72, 73, 75, 76, 78, 79, 80, 82, 83, 85, 86, 87, 89, 90, 92, 93, 94, 96, 97, 98, 100, 101, 103, 104, 105, 107, 108, 110, 111, 112, 114, 115, 117, 118, 119, 121, 122, 124, 125, 126, 128, 129, 131, 132, 133, 135, 136, 138, 139, 140, 142, 143, 144, 146, 147, 149, 150, 151, 153, 154, 156, 157, 158, 160, 161, 163, 164, 165, 167, 168, 170, 171, 172, 174, 175, 177, 178, 179, 181, 182, 184, 185, 186, 188, 189, 190, 192, 193, 195, 196, 197, 199, 200, 202, 203, 204, 206, 207, 209, 210, 211, 213, 214, 216, 217, 218, 220, 221, 223, 224, 225, 227, 228, 230, 231, 232, 234, 235, 236, 238, 239, 241, 242, 243, 245, 246, 248, 249, 250, 252, 253, 255, 256, 257, 259, 260, 262, 263, 264, 266, }
{0, 5, 11, 16, 22, 27, 33, 39, 44, 50, 55, 61, 66, 72, 78, 83, 89, 94, 100, 105, 111, 117, 122, 128, 133, 139, 144, 150, 156, 161, 167, 172, 178, 184, 189, 195, 200, 206, 211, 217, 223, 228, 234, 239, 245, 250, 256, 262, }
{267, 269, 270, 271, 273, 274, 276, 277, 278, 280, 281, 282, 284, 285, 287, 288, 289, 291, 292, 294, 295, 296, 298, 299, 301, 302, 303, 305, 306, 308, 309, 310, 312, 313, 315, 316, 317, 319, 320, 322, 323, 324, 326, 327, 328, 330, 331, 333, 334, 335, 337, 338, 340, 341, 342, 344, 345, 347, 348, 349, 351, 352, 354, 355, 356, 358, 359, 361, 362, 363, 365, 366, 368, 369, 370, 372, 373, 374, 376, 377, 379, 380, 381, 383, 384, 386, 387, 388, 390, 391, 393, 394, 395, 397, 398, 400, 401, 402, 404, 405, 407, 408, 409, 411, 412, 414, 415, 416, 418, 419, 420, 422, 423, 425, 426, 427, 429, 430, 432, 433, 434, 436, 437, 439, 440, 441, 443, 444, 446, 447, 448, 450, 451, 453, 454, 455, 457, 458, 460, 461, 462, 464, 465, 466, 468, 469, 471, 472, 473, 475, 476, 478, 479, 480, 482, 483, 485, 486, 487, 489, 490, 492, 493, 494, 496, 497, 499, 500, 501, 503, 504, 506, 507, 508, 510, 511, 512, 514, 515, 517, 518, 519, 521, 522, 524, 525, 526, 528, 529, 531, 532, 533, }
{267, 273, 278, 284, 289, 295, 301, 306, 312, 317, 323, 328, 334, 340, 345, 351, 356, 362, 368, 373, 379, 384, 390, 395, 401, 407, 412, 418, 423, 429, 434, 440, 446, 451, 457, 462, 468, 473, 479, 485, 490, 496, 501, 507, 512, 518, 524, 529, }
{535, 536, 538, 539, 540, 542, 543, 545, 546, 547, 549, 550, 552, 553, 554, 556, 557, 558, 560, 561, 563, 564, 565, 567, 568, 570, 571, 572, 574, 575, 577, 578, 579, 581, 582, 584, 585, 586, 588, 589, 591, 592, 593, 595, 596, 598, 599, 600, 602, 603, 604, 606, 607, 609, 610, 611, 613, 614, 616, 617, 618, 620, 621, 623, 624, 625, 627, 628, 630, 631, 632, 634, 635, 637, 638, 639, 641, 642, 644, 645, 646, 648, 649, 650, 652, 653, 655, 656, 657, 659, 660, 662, 663, 664, 666, 667, 669, 670, 671, 673, 674, 676, 677, 678, 680, 681, 683, 684, 685, 687, 688, 690, 691, 692, 694, 695, 696, 698, 699, 701, 702, 703, 705, 706, 708, 709, 710, 712, 713, 715, 716, 717, 719, 720, 722, 723, 724, 726, 727, 729, 730, 731, 733, 734, 736, 737, 738, 740, 741, 742, 744, 745, 747, 748, 749, 751, 752, 754, 755, 756, 758, 759, 761, 762, 763, 765, 766, 768, 769, 770, 772, 773, 775, 776, 777, 779, 780, 782, 783, 784, 786, 787, 788, 790, 791, 793, 794, 795, 797, 798, 800, 801, }
{535, 540, 546, 552, 557, 563, 568, 574, 579, 585, 591, 596, 602, 607, 613, 618, 624, 630, 635, 641, 646, 652, 657, 663, 669, 674, 680, 685, 691, 696, 702, 708, 713, 719, 724, 730, 736, 741, 747, 752, 758, 763, 769, 775, 780, 786, 791, 797, }
{802, 804, 805, 807, 808, 809, 811, 812, 814, 815, 816, 818, 819, 821, 822, 823, 825, 826, 828, 829, 830, 832, 833, 834, 836, 837, 839, 840, 841, 843, 844, 846, 847, 848, 850, 851, 853, 854, 855, 857, 858, 860, 861, 862, 864, 865, 867, 868, 869, 871, 872, 874, 875, 876, 878, 879, 880, 882, 883, 885, 886, 887, 889, 890, 892, 893, 894, 896, 897, 899, 900, 901, 903, 904, 906, 907, 908, 910, 911, 913, 914, 915, 917, 918, 920, 921, 922, 924, 925, 926, 928, 929, 931, 932, 933, 935, 936, 938, 939, 940, 942, 943, 945, 946, 947, 949, 950, 952, 953, 954, 956, 957, 959, 960, 961, 963, 964, 966, 967, 968, 970, 971, 972, 974, 975, 977, 978, 979, 981, 982, 984, 985, 986, 988, 989, 991, 992, 993, 995, 996, 998, 999, 1000, 1002, 1003, 1005, 1006, 1007, 1009, 1010, 1012, 1013, 1014, 1016, 1017, 1018, 1020, 1021, 1023, 1024, 1025, 1027, 1028, 1030, 1031, 1032, 1034, 1035, 1037, 1038, 1039, 1041, 1042, 1044, 1045, 1046, 1048, 1049, 1051, 1052, 1053, 1055, 1056, 1058, 1059, 1060, 1062, 1063, 1064, 1066, 1067, 1069, }
{802, 808, 814, 819, 825, 830, 836, 841, 847, 853, 858, 864, 869, 875, 880, 886, 892, 897, 903, 908, 914, 920, 925, 931, 936, 942, 947, 953, 959, 964, 970, 975, 981, 986, 992, 998, 1003, 1009, 1014, 1020, 1025, 1031, 1037, 1042, 1048, 1053, 1059, 1064, }
{1070, 1071, 1073, 1074, 1076, 1077, 1078, 1080, 1081, 1083, 1084, 1085, 1087, 1088, 1090, 1091, 1092, 1094, 1095, 1097, 1098, 1099, 1101, 1102, 1104, 1105, 1106, 1108, 1109, 1110, 1112, 1113, 1115, 1116, 1117, 1119, 1120, 1122, 1123, 1124, 1126, 1127, 1129, 1130, 1131, 1133, 1134, 1136, 1137, 1138, 1140, 1141, 1143, 1144, 1145, 1147, 1148, 1150, 1151, 1152, 1154, 1155, 1156, 1158, 1159, 1161, 1162, 1163, 1165, 1166, 1168, 1169, 1170, 1172, 1173, 1175, 1176, 1177, 1179, 1180, 1182, 1183, 1184, 1186, 1187, 1189, 1190, 1191, 1193, 1194, 1196, 1197, 1198, 1200, 1201, 1202, 1204, 1205, 1207, 1208, 1209, 1211, 1212, 1214, 1215, 1216, 1218, 1219, 1221, 1222, 1223, 1225, 1226, 1228, 1229, 1230, 1232, 1233, 1235, 1236, 1237, 1239, 1240, 1242, 1243, 1244, 1246, 1247, 1248, 1250, 1251, 1253, 1254, 1255, 1257, 1258, 1260, 1261, 1262, 1264, 1265, 1267, 1268, 1269, 1271, 1272, 1274, 1275, 1276, 1278, 1279, 1281, 1282, 1283, 1285, 1286, 1288, 1289, 1290, 1292, 1293, 1294, 1296, 1297, 1299, 1300, 1301, 1303, 1304, 1306, 1307, 1308, 1310, 1311, 1313, 1314, 1315, 1317, 1318, 1320, 1321, 1322, 1324, 1325, 1327, 1328, 1329, 1331, 1332, 1334, 1335, 1336, }
{1070, 1076, 1081, 1087, 1092, 1098, 1104, 1109, 1115, 1120, 1126, 1131, 1137, 1143, 1148, 1154, 1159, 1165, 1170, 1176, 1182, 1187, 1193, 1198, 1204, 1209, 1215, 1221, 1226, 1232, 1237, 1243, 1248, 1254, 1260, 1265, 1271, 1276, 1282, 1288, 1293, 1299, 1304, 1310, 1315, 1321, 1327, 1332, }
{1338, 1339, 1340, 1342, 1343, 1345, 1346, 1347, 1349, 1350, 1352, 1353, 1354, 1356, 1357, 1359, 1360, 1361, 1363, 1364, 1366, 1367, 1368, 1370, 1371, 1373, 1374, 1375, 1377, 1378, 1380, 1381, 1382, 1384, 1385, 1386, 1388, 1389, 1391, 1392, 1393, 1395, 1396, 1398, 1399, 1400, 1402, 1403, 1405, 1406, 1407, 1409, 1410, 1412, 1413, 1414, 1416, 1417, 1419, 1420, 1421, 1423, 1424, 1426, 1427, 1428, 1430, 1431, 1432, 1434, 1435, 1437, 1438, 1439, 1441, 1442, 1444, 1445, 1446, 1448, 1449, 1451, 1452, 1453, 1455, 1456, 1458, 1459, 1460, 1462, 1463, 1465, 1466, 1467, 1469, 1470, }
{1338, 1340, 1343, 1346, 1349, 1352, 1354, 1357, 1360, 1363, 1366, 1368, 1371, 1374, 1377, 1380, 1382, 1385, 1388, 1391, 1393, 1396, 1399, 1402, 1405, 1407, 1410, 1413, 1416, 1419, 1421, 1424, 1427, 1430, 1432, 1435, 1438, 1441, 1444, 1446, 1449, 1452, 1455, 1458, 1460, 1463, 1466, 1469, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 28
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 28
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 28
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 28
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 28
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,28}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,28}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,28}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,28}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,28}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 4
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 5
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,28}
slicedInpSpikeIds.shapeToString(): {48,28}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,28}
slicedInpSpikeIds.shapeToString(): {48,28}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,28}
slicedInpSpikeIds.shapeToString(): {48,28}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

3
out_spike_ids[ilay].shapeToString(): {100,48,28}
slicedInpSpikeIds.shapeToString(): {48,28}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

4
out_spike_ids[ilay].shapeToString(): {100,48,28}
slicedInpSpikeIds.shapeToString(): {48,28}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {294,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 5
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{364, 364, 364, 364, 14, }
start_tile: 1
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 365
num_occupied_tiles: 293
batchsize: 48
sparse_size: 28
dLdx.shapeToString(): {293,48,28}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 365
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 729
num_occupied_tiles: 293
batchsize: 48
sparse_size: 28
dLdx.shapeToString(): {293,48,28}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 729
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 1093
num_occupied_tiles: 293
batchsize: 48
sparse_size: 28
dLdx.shapeToString(): {293,48,28}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 1093
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 1457
num_occupied_tiles: 293
batchsize: 48
sparse_size: 28
dLdx.shapeToString(): {293,48,28}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 1457
num_tiles_per_layer[ilay]: 14
start_tile + num_tiles_per_layer[ilay]: 1471
num_occupied_tiles: 5
batchsize: 48
sparse_size: 28
dLdx.shapeToString(): {5,48,28}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {294,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 5
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 21:29:15.282141: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[588 586 586 586 586  10]
0 294.0
1 293.0
2 293.0
3 293.0
4 293.0
5 5.0
[TileMapping(start_tile=1, end_tile=295), TileMapping(start_tile=295, end_tile=588), TileMapping(start_tile=588, end_tile=881), TileMapping(start_tile=881, end_tile=1174), TileMapping(start_tile=1174, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=295), TileMapping(start_tile=295, end_tile=588), TileMapping(start_tile=588, end_tile=881), TileMapping(start_tile=881, end_tile=1174), TileMapping(start_tile=1174, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.40406101782088427
limit=0.4047499544129309
limit=0.4047499544129309
limit=0.4047499544129309
limit=0.4047499544129309
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 588) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 586) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 586) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 586) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_5:0' shape=(2, 586) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_6:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_28_28_28_28_28_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 28), (100 2748898     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][5
                                                                 keras_multi_lif_layer_sparse[0][1
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 2,748,898
Trainable params: 2,740,072
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_28_28_28_28_28_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_28_28_28_28_28_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.798252/52 [==============================] - 513s 10s/step - loss: 2.7982
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.181652/52 [==============================] - 0s 6ms/step - loss: 2.1816
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 2.179652/52 [==============================] - 0s 5ms/step - loss: 2.1796
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 2.118952/52 [==============================] - 0s 5ms/step - loss: 2.1189
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 2.045552/52 [==============================] - 0s 5ms/step - loss: 2.0455
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 1.967252/52 [==============================] - 0s 6ms/step - loss: 1.9672
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 1.838252/52 [==============================] - 0s 6ms/step - loss: 1.8382
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 1.794152/52 [==============================] - 0s 6ms/step - loss: 1.7941
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 1.820552/52 [==============================] - 0s 6ms/step - loss: 1.8205
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 1.793952/52 [==============================] - 0s 6ms/step - loss: 1.7939
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 1.777952/52 [==============================] - 0s 6ms/step - loss: 1.7779
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 1.841652/52 [==============================] - 0s 6ms/step - loss: 1.8416
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 1.787652/52 [==============================] - 0s 6ms/step - loss: 1.7876
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 1.849352/52 [==============================] - 0s 6ms/step - loss: 1.8493
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 1.920652/52 [==============================] - 0s 6ms/step - loss: 1.9206
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 2.007552/52 [==============================] - 0s 6ms/step - loss: 2.0075
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 2.106652/52 [==============================] - 0s 6ms/step - loss: 2.1066
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 1.976752/52 [==============================] - 0s 6ms/step - loss: 1.9767
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 2.044852/52 [==============================] - 0s 6ms/step - loss: 2.0448
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 2.126052/52 [==============================] - 0s 6ms/step - loss: 2.1260
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 2.132652/52 [==============================] - 0s 6ms/step - loss: 2.1326
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 2.127752/52 [==============================] - 0s 6ms/step - loss: 2.1277
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 1.993352/52 [==============================] - 0s 6ms/step - loss: 1.9933
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 2.000652/52 [==============================] - 0s 6ms/step - loss: 2.0006
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 1.964752/52 [==============================] - 0s 6ms/step - loss: 1.9647

Final time:  520.0241620540619
2022-09-12 21:29:25.417376: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.05
DENSE_SIZES:  [2312, 1466, 1466, 10]
SPARSE_SIZES:  [32, 72, 72, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  2
SECOND_THRESHOLD:  0.98

60000
9984
[34113 15755 47647 ... 24325 35425 34037]
2022-09-12 21:29:33.582803: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 21:29:34.404830: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 7, 2, 1
Build_allocator: 10, 3, 1
Build_allocator: 8, 2, 2
Build_allocator: 11, 3, 2
Build_allocator: 3, 1, 0
Build_allocator: 4, 1, 1
Build_allocator: 5, 1, 2
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 12, 4, 0
Build_allocator: 13, 4, 1
Build_allocator: 14, 4, 2
Build_allocator: 15, 5, 0
Build_allocator: 16, 5, 1
Build_allocator: 17, 5, 2
Build_allocator: 6, 2, 0
Build_allocator: 9, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0x6088b00
1: &init_state[ilay]0x6088b08
2: &init_state[ilay]0x6088b10
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 734, 1467, }
end_tiles: {734, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{4, 4, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 5, 6, 8, 10, 11, 13, 15, 17, 18, 20, 22, 23, 25, 27, 28, 30, 32, 34, 35, 37, 39, 40, 42, 44, 46, 47, 49, 51, 52, 54, 56, 57, 59, 61, 63, 64, 66, 68, 69, 71, 73, 74, 76, 78, 80, 81, 83, 85, 86, 88, 90, 92, 93, 95, 97, 98, 100, 102, 103, 105, 107, 109, 110, 112, 114, 115, 117, 119, 120, 122, 124, 126, 127, 129, 131, 132, 134, 136, 138, 139, 141, 143, 144, 146, 148, 149, 151, 153, 155, 156, 158, 160, 161, 163, 165, 166, 168, 170, 172, 173, 175, 177, 178, 180, 182, 184, 185, 187, 189, 190, 192, 194, 195, 197, 199, 201, 202, 204, 206, 207, 209, 211, 212, 214, 216, 218, 219, 221, 223, 224, 226, 228, 230, 231, 233, 235, 236, 238, 240, 241, 243, 245, 247, 248, 250, 252, 253, 255, 257, 258, 260, 262, 264, 265, 267, 269, 270, 272, 274, 276, 277, 279, 281, 282, 284, 286, 287, 289, 291, 293, 294, 296, 298, 299, 301, 303, 304, 306, 308, 310, 311, 313, 315, 316, 318, 320, 322, 323, 325, 327, 328, 330, 332, 333, 335, 337, 339, 340, 342, 344, 345, 347, 349, 350, 352, 354, 356, 357, 359, 361, 362, 364, 366, 368, 369, 371, 373, 374, 376, 378, 379, 381, 383, 385, 386, 388, 390, 391, 393, 395, 396, 398, 400, 402, 403, 405, 407, 408, 410, 412, 414, 415, 417, 419, 420, 422, 424, 425, 427, 429, 431, 432, 434, 436, 437, 439, 441, 442, 444, 446, 448, 449, 451, 453, 454, 456, 458, 460, 461, 463, 465, 466, 468, 470, 471, 473, 475, 477, 478, 480, 482, 483, 485, 487, 488, 490, 492, 494, 495, 497, 499, 500, 502, 504, 506, 507, 509, 511, 512, 514, 516, 517, 519, 521, 523, 524, 526, 528, 529, 531, 533, 534, 536, 538, 540, 541, 543, 545, 546, 548, 550, 552, 553, 555, 557, 558, 560, 562, 563, 565, 567, 569, 570, 572, 574, 575, 577, 579, 580, 582, 584, 586, 587, 589, 591, 592, 594, 596, 598, 599, 601, 603, 604, 606, 608, 609, 611, 613, 615, 616, 618, 620, 621, 623, 625, 626, 628, 630, 632, 633, 635, 637, 638, 640, 642, 644, 645, 647, 649, 650, 652, }
{0, 13, 27, 40, 54, 68, 81, 95, 109, 122, 136, 149, 163, 177, 190, 204, 218, 231, 245, 258, 272, 286, 299, 313, 327, 340, 354, 368, 381, 395, 408, 422, 436, 449, 463, 477, 490, 504, 517, 531, 545, 558, 572, 586, 599, 613, 626, 640, }
{654, 655, 657, 659, 661, 662, 664, 666, 667, 669, 671, 672, 674, 676, 678, 679, 681, 683, 684, 686, 688, 690, 691, 693, 695, 696, 698, 700, 701, 703, 705, 707, 708, 710, 712, 713, 715, 717, 718, 720, 722, 724, 725, 727, 729, 730, 732, 734, 736, 737, 739, 741, 742, 744, 746, 747, 749, 751, 753, 754, 756, 758, 759, 761, 763, 764, 766, 768, 770, 771, 773, 775, 776, 778, 780, 782, 783, 785, 787, 788, 790, 792, 793, 795, 797, 799, 800, 802, 804, 805, 807, 809, 810, 812, 814, 816, 817, 819, 821, 822, 824, 826, 828, 829, 831, 833, 834, 836, 838, 839, 841, 843, 845, 846, 848, 850, 851, 853, 855, 856, 858, 860, 862, 863, 865, 867, 868, 870, 872, 874, 875, 877, 879, 880, 882, 884, 885, 887, 889, 891, 892, 894, 896, 897, 899, 901, 902, 904, 906, 908, 909, 911, 913, 914, 916, 918, 920, 921, 923, 925, 926, 928, 930, 931, 933, 935, 937, 938, 940, 942, 943, 945, 947, 948, 950, 952, 954, 955, 957, 959, 960, 962, 964, 966, 967, 969, 971, 972, 974, 976, 977, 979, 981, 983, 984, 986, 988, 989, 991, 993, 994, 996, 998, 1000, 1001, 1003, 1005, 1006, 1008, 1010, 1012, 1013, 1015, 1017, 1018, 1020, 1022, 1023, 1025, 1027, 1029, 1030, 1032, 1034, 1035, 1037, 1039, 1040, 1042, 1044, 1046, 1047, 1049, 1051, 1052, 1054, 1056, 1058, 1059, 1061, 1063, 1064, 1066, 1068, 1069, 1071, 1073, 1075, 1076, 1078, 1080, 1081, 1083, 1085, 1086, 1088, 1090, 1092, 1093, 1095, 1097, 1098, 1100, 1102, 1104, 1105, 1107, 1109, 1110, 1112, 1114, 1115, 1117, 1119, 1121, 1122, 1124, 1126, 1127, 1129, 1131, 1132, 1134, 1136, 1138, 1139, 1141, 1143, 1144, 1146, 1148, 1150, 1151, 1153, 1155, 1156, 1158, 1160, 1161, 1163, 1165, 1167, 1168, 1170, 1172, 1173, 1175, 1177, 1178, 1180, 1182, 1184, 1185, 1187, 1189, 1190, 1192, 1194, 1196, 1197, 1199, 1201, 1202, 1204, 1206, 1207, 1209, 1211, 1213, 1214, 1216, 1218, 1219, 1221, 1223, 1224, 1226, 1228, 1230, 1231, 1233, 1235, 1236, 1238, 1240, 1242, 1243, 1245, 1247, 1248, 1250, 1252, 1253, 1255, 1257, 1259, 1260, 1262, 1264, 1265, 1267, 1269, 1270, 1272, 1274, 1276, 1277, 1279, 1281, 1282, 1284, 1286, 1288, 1289, 1291, 1293, 1294, 1296, 1298, 1299, 1301, 1303, 1305, 1306, }
{654, 667, 681, 695, 708, 722, 736, 749, 763, 776, 790, 804, 817, 831, 845, 858, 872, 885, 899, 913, 926, 940, 954, 967, 981, 994, 1008, 1022, 1035, 1049, 1063, 1076, 1090, 1104, 1117, 1131, 1144, 1158, 1172, 1185, 1199, 1213, 1226, 1240, 1253, 1267, 1281, 1294, }
{1308, 1310, 1311, 1313, 1315, 1316, 1318, 1320, 1322, 1323, 1325, 1327, 1328, 1330, 1332, 1334, 1335, 1337, 1339, 1340, 1342, 1344, 1345, 1347, 1349, 1351, 1352, 1354, 1356, 1357, 1359, 1361, 1362, 1364, 1366, 1368, 1369, 1371, 1373, 1374, 1376, 1378, 1380, 1381, 1383, 1385, 1386, 1388, 1390, 1391, 1393, 1395, 1397, 1398, 1400, 1402, 1403, 1405, 1407, 1408, 1410, 1412, 1414, 1415, 1417, 1419, 1420, 1422, 1424, 1426, 1427, 1429, 1431, 1432, 1434, 1436, 1437, 1439, 1441, 1443, 1444, 1446, 1448, 1449, 1451, 1453, 1454, 1456, 1458, 1460, 1461, 1463, 1465, 1466, 1468, 1470, }
{1308, 1311, 1315, 1318, 1322, 1325, 1328, 1332, 1335, 1339, 1342, 1345, 1349, 1352, 1356, 1359, 1362, 1366, 1369, 1373, 1376, 1380, 1383, 1386, 1390, 1393, 1397, 1400, 1403, 1407, 1410, 1414, 1417, 1420, 1424, 1427, 1431, 1434, 1437, 1441, 1444, 1448, 1451, 1454, 1458, 1461, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 4
num_workers: 24
max_num_states_per_worker: 62
sparse_size_to_use: 62
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 4
num_workers: 24
max_num_states_per_worker: 62
sparse_size_to_use: 62
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,4,72}
 repeated_sparse_spike_ids.shapeToString(): {48,2,4,72}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,72}
slicedInpSpikeIds.shapeToString(): {48,72}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,72}
slicedInpSpikeIds.shapeToString(): {48,72}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{1444, 26, }
start_tile: 1
num_tiles_per_layer[ilay]: 1444
start_tile + num_tiles_per_layer[ilay]: 1445
num_occupied_tiles: 733
batchsize: 48
sparse_size: 72
dLdx.shapeToString(): {733,48,72}
replicated_numGrads.shapeToString(): {733,48}
exchangeGroupSize: 24
numExchangeGroups: 30
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
start_id: 456, end_id: 480
start_id: 480, end_id: 504
start_id: 504, end_id: 528
start_id: 528, end_id: 552
start_id: 552, end_id: 576
start_id: 576, end_id: 600
start_id: 600, end_id: 624
start_id: 624, end_id: 648
start_id: 648, end_id: 672
start_id: 672, end_id: 696
lastExchangeGroupLarger
start_id: 696, end_id: 733
exchangeGroupSizeLast: 37
exchangeBatchSizeLast: 2
start_tile: 1445
num_tiles_per_layer[ilay]: 26
start_tile + num_tiles_per_layer[ilay]: 1471
num_occupied_tiles: 5
batchsize: 48
sparse_size: 72
dLdx.shapeToString(): {5,48,72}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 21:45:08.566782: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[1466 1466   10]
0 733.0
1 733.0
2 5.0
[TileMapping(start_tile=1, end_tile=734), TileMapping(start_tile=734, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=734), TileMapping(start_tile=734, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.2558990251065398
limit=0.2558990251065398
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 1466) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 1466) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_72_72_2_48_1_734_1467_734_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 72), (100 5562034     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][2
                                                                 keras_multi_lif_layer_sparse[0][5
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 5,562,034
Trainable params: 5,553,208
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_72_72_2_48_1_734_1467_734_1467_1472
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_72_72_2_48_1_734_1467_734_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.415852/52 [==============================] - 937s 18s/step - loss: 2.4158
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 1.933652/52 [==============================] - 0s 9ms/step - loss: 1.9336
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 1.793852/52 [==============================] - 0s 8ms/step - loss: 1.7938
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 1.783152/52 [==============================] - 0s 8ms/step - loss: 1.7831
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 1.804052/52 [==============================] - 0s 8ms/step - loss: 1.8040
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 1.763752/52 [==============================] - 0s 8ms/step - loss: 1.7637
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 1.524652/52 [==============================] - 0s 8ms/step - loss: 1.5246
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 1.561852/52 [==============================] - 0s 9ms/step - loss: 1.5618
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 1.480152/52 [==============================] - 0s 8ms/step - loss: 1.4801
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 1.539652/52 [==============================] - 0s 8ms/step - loss: 1.5396
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 1.418052/52 [==============================] - 0s 9ms/step - loss: 1.4180
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 1.495852/52 [==============================] - 0s 8ms/step - loss: 1.4958
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 1.410052/52 [==============================] - 1s 28ms/step - loss: 1.4100
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 1.439452/52 [==============================] - 0s 8ms/step - loss: 1.4394
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 1.391252/52 [==============================] - 0s 8ms/step - loss: 1.3912
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 1.462452/52 [==============================] - 0s 9ms/step - loss: 1.4624
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 1.389452/52 [==============================] - 0s 9ms/step - loss: 1.3894
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 1.538552/52 [==============================] - 0s 8ms/step - loss: 1.5385
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 1.428052/52 [==============================] - 0s 8ms/step - loss: 1.4280
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 1.426952/52 [==============================] - 0s 9ms/step - loss: 1.4269
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 1.515752/52 [==============================] - 0s 9ms/step - loss: 1.5157
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 1.538652/52 [==============================] - 0s 8ms/step - loss: 1.5386
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 1.423352/52 [==============================] - 0s 8ms/step - loss: 1.4233
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 1.427052/52 [==============================] - 0s 8ms/step - loss: 1.4270
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 1.407052/52 [==============================] - 0s 8ms/step - loss: 1.4070

Final time:  948.2487044334412
2022-09-12 21:45:23.802125: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.05
DENSE_SIZES:  [2312, 978, 978, 976, 10]
SPARSE_SIZES:  [32, 48, 48, 48, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  3
SECOND_THRESHOLD:  0.98

60000
9984
[31190 56261 47476 ...  5107  6451 23716]
2022-09-12 21:45:32.266508: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 21:45:33.119316: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 9, 2, 1
Build_allocator: 13, 3, 1
Build_allocator: 10, 2, 2
Build_allocator: 14, 3, 2
Build_allocator: 11, 2, 3
Build_allocator: 15, 3, 3
Build_allocator: 4, 1, 0
Build_allocator: 5, 1, 1
Build_allocator: 6, 1, 2
Build_allocator: 7, 1, 3
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 16, 4, 0
Build_allocator: 17, 4, 1
Build_allocator: 18, 4, 2
Build_allocator: 19, 4, 3
Build_allocator: 20, 5, 0
Build_allocator: 21, 5, 1
Build_allocator: 22, 5, 2
Build_allocator: 23, 5, 3
Build_allocator: 8, 2, 0
Build_allocator: 12, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0x20b02050
1: &init_state[ilay]0x20b02058
2: &init_state[ilay]0x20b02060
3: &init_state[ilay]0x20b02068
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 490, 979, 1467, }
end_tiles: {490, 979, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{3, 3, 3, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 4, 6, 7, 9, 10, 12, 13, 15, 16, 18, 19, 21, 23, 24, 26, 27, 29, 30, 32, 33, 35, 36, 38, 39, 41, 42, 44, 46, 47, 49, 50, 52, 53, 55, 56, 58, 59, 61, 62, 64, 65, 67, 69, 70, 72, 73, 75, 76, 78, 79, 81, 82, 84, 85, 87, 88, 90, 92, 93, 95, 96, 98, 99, 101, 102, 104, 105, 107, 108, 110, 111, 113, 115, 116, 118, 119, 121, 122, 124, 125, 127, 128, 130, 131, 133, 134, 136, 138, 139, 141, 142, 144, 145, 147, 148, 150, 151, 153, 154, 156, 157, 159, 161, 162, 164, 165, 167, 168, 170, 171, 173, 174, 176, 177, 179, 180, 182, 184, 185, 187, 188, 190, 191, 193, 194, 196, 197, 199, 200, 202, 203, 205, 207, 208, 210, 211, 213, 214, 216, 217, 219, 220, 222, 223, 225, 226, 228, 230, 231, 233, 234, 236, 237, 239, 240, 242, 243, 245, 246, 248, 249, 251, 253, 254, 256, 257, 259, 260, 262, 263, 265, 266, 268, 269, 271, 272, 274, 276, 277, 279, 280, 282, 283, 285, 286, 288, 289, 291, 292, 294, 295, 297, 299, 300, 302, 303, 305, 306, 308, 309, 311, 312, 314, 315, 317, 318, 320, 322, 323, 325, 326, 328, 329, 331, 332, 334, 335, 337, 338, 340, 341, 343, 345, 346, 348, 349, 351, 352, 354, 355, 357, 358, 360, 361, 363, 364, 366, 368, 369, 371, 372, 374, 375, 377, 378, 380, 381, 383, 384, 386, 387, 389, 391, 392, 394, 395, 397, 398, 400, 401, 403, 404, 406, 407, 409, 410, 412, 414, 415, 417, 418, 420, 421, 423, 424, 426, 427, 429, 430, 432, 433, 435, 437, 438, 440, }
{0, 9, 18, 27, 36, 46, 55, 64, 73, 82, 92, 101, 110, 119, 128, 138, 147, 156, 165, 174, 184, 193, 202, 211, 220, 230, 239, 248, 257, 266, 276, 285, 294, 303, 312, 322, 331, 340, 349, 358, 368, 377, 386, 395, 404, 414, 423, 432, }
{441, 443, 444, 446, 447, 449, 450, 452, 453, 455, 456, 458, 460, 461, 463, 464, 466, 467, 469, 470, 472, 473, 475, 476, 478, 479, 481, 483, 484, 486, 487, 489, 490, 492, 493, 495, 496, 498, 499, 501, 502, 504, 506, 507, 509, 510, 512, 513, 515, 516, 518, 519, 521, 522, 524, 525, 527, 529, 530, 532, 533, 535, 536, 538, 539, 541, 542, 544, 545, 547, 548, 550, 552, 553, 555, 556, 558, 559, 561, 562, 564, 565, 567, 568, 570, 571, 573, 575, 576, 578, 579, 581, 582, 584, 585, 587, 588, 590, 591, 593, 594, 596, 598, 599, 601, 602, 604, 605, 607, 608, 610, 611, 613, 614, 616, 617, 619, 621, 622, 624, 625, 627, 628, 630, 631, 633, 634, 636, 637, 639, 640, 642, 644, 645, 647, 648, 650, 651, 653, 654, 656, 657, 659, 660, 662, 663, 665, 667, 668, 670, 671, 673, 674, 676, 677, 679, 680, 682, 683, 685, 686, 688, 690, 691, 693, 694, 696, 697, 699, 700, 702, 703, 705, 706, 708, 709, 711, 713, 714, 716, 717, 719, 720, 722, 723, 725, 726, 728, 729, 731, 732, 734, 736, 737, 739, 740, 742, 743, 745, 746, 748, 749, 751, 752, 754, 755, 757, 759, 760, 762, 763, 765, 766, 768, 769, 771, 772, 774, 775, 777, 778, 780, 782, 783, 785, 786, 788, 789, 791, 792, 794, 795, 797, 798, 800, 801, 803, 805, 806, 808, 809, 811, 812, 814, 815, 817, 818, 820, 821, 823, 824, 826, 828, 829, 831, 832, 834, 835, 837, 838, 840, 841, 843, 844, 846, 847, 849, 851, 852, 854, 855, 857, 858, 860, 861, 863, 864, 866, 867, 869, 870, 872, 874, 875, 877, 878, 880, 881, }
{441, 450, 460, 469, 478, 487, 496, 506, 515, 524, 533, 542, 552, 561, 570, 579, 588, 598, 607, 616, 625, 634, 644, 653, 662, 671, 680, 690, 699, 708, 717, 726, 736, 745, 754, 763, 772, 782, 791, 800, 809, 818, 828, 837, 846, 855, 864, 874, }
{883, 884, 886, 887, 889, 890, 892, 893, 895, 897, 898, 900, 901, 903, 904, 906, 907, 909, 910, 912, 913, 915, 916, 918, 920, 921, 923, 924, 926, 927, 929, 930, 932, 933, 935, 936, 938, 939, 941, 943, 944, 946, 947, 949, 950, 952, 953, 955, 956, 958, 959, 961, 962, 964, 966, 967, 969, 970, 972, 973, 975, 976, 978, 979, 981, 982, 984, 985, 987, 989, 990, 992, 993, 995, 996, 998, 999, 1001, 1002, 1004, 1005, 1007, 1008, 1010, 1012, 1013, 1015, 1016, 1018, 1019, 1021, 1022, 1024, 1025, 1027, 1028, 1030, 1031, 1033, 1035, 1036, 1038, 1039, 1041, 1042, 1044, 1045, 1047, 1048, 1050, 1051, 1053, 1054, 1056, 1058, 1059, 1061, 1062, 1064, 1065, 1067, 1068, 1070, 1071, 1073, 1074, 1076, 1077, 1079, 1081, 1082, 1084, 1085, 1087, 1088, 1090, 1091, 1093, 1094, 1096, 1097, 1099, 1100, 1102, 1104, 1105, 1107, 1108, 1110, 1111, 1113, 1114, 1116, 1117, 1119, 1120, 1122, 1123, 1125, 1127, 1128, 1130, 1131, 1133, 1134, 1136, 1137, 1139, 1140, 1142, 1143, 1145, 1146, 1148, 1150, 1151, 1153, 1154, 1156, 1157, 1159, 1160, 1162, 1163, 1165, 1166, 1168, 1169, 1171, 1173, 1174, 1176, 1177, 1179, 1180, 1182, 1183, 1185, 1186, 1188, 1189, 1191, 1192, 1194, 1196, 1197, 1199, 1200, 1202, 1203, 1205, 1206, 1208, 1209, 1211, 1212, 1214, 1215, 1217, 1219, 1220, 1222, 1223, 1225, 1226, 1228, 1229, 1231, 1232, 1234, 1235, 1237, 1238, 1240, 1242, 1243, 1245, 1246, 1248, 1249, 1251, 1252, 1254, 1255, 1257, 1258, 1260, 1261, 1263, 1265, 1266, 1268, 1269, 1271, 1272, 1274, 1275, 1277, 1278, 1280, 1281, 1283, 1284, 1286, 1288, 1289, 1291, 1292, 1294, 1295, 1297, 1298, 1300, 1301, 1303, 1304, 1306, 1307, 1309, 1311, 1312, 1314, 1315, 1317, 1318, 1320, 1321, 1323, }
{883, 892, 901, 910, 920, 929, 938, 947, 956, 966, 975, 984, 993, 1002, 1012, 1021, 1030, 1039, 1048, 1058, 1067, 1076, 1085, 1094, 1104, 1113, 1122, 1131, 1140, 1150, 1159, 1168, 1177, 1186, 1196, 1205, 1214, 1223, 1232, 1242, 1251, 1260, 1269, 1278, 1288, 1297, 1306, 1315, }
{1324, 1326, 1327, 1329, 1330, 1332, 1334, 1335, 1337, 1338, 1340, 1341, 1343, 1344, 1346, 1347, 1349, 1350, 1352, 1353, 1355, 1357, 1358, 1360, 1361, 1363, 1364, 1366, 1367, 1369, 1370, 1372, 1373, 1375, 1376, 1378, 1380, 1381, 1383, 1384, 1386, 1387, 1389, 1390, 1392, 1393, 1395, 1396, 1398, 1399, 1401, 1403, 1404, 1406, 1407, 1409, 1410, 1412, 1413, 1415, 1416, 1418, 1419, 1421, 1422, 1424, 1426, 1427, 1429, 1430, 1432, 1433, 1435, 1436, 1438, 1439, 1441, 1442, 1444, 1445, 1447, 1449, 1450, 1452, 1453, 1455, 1456, 1458, 1459, 1461, 1462, 1464, 1465, 1467, 1468, 1470, }
{1324, 1327, 1330, 1334, 1337, 1340, 1343, 1346, 1349, 1352, 1355, 1358, 1361, 1364, 1367, 1370, 1373, 1376, 1380, 1383, 1386, 1389, 1392, 1395, 1398, 1401, 1404, 1407, 1410, 1413, 1416, 1419, 1422, 1426, 1429, 1432, 1435, 1438, 1441, 1444, 1447, 1450, 1453, 1456, 1459, 1462, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 48
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 48
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 48
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,48}
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,48}
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,48}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,48}
slicedInpSpikeIds.shapeToString(): {48,48}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,48}
slicedInpSpikeIds.shapeToString(): {48,48}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,48}
slicedInpSpikeIds.shapeToString(): {48,48}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {488,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{726, 725, 18, }
start_tile: 1
num_tiles_per_layer[ilay]: 726
start_tile + num_tiles_per_layer[ilay]: 727
num_occupied_tiles: 489
batchsize: 48
sparse_size: 48
dLdx.shapeToString(): {489,48,48}
replicated_numGrads.shapeToString(): {489,48}
exchangeGroupSize: 24
numExchangeGroups: 20
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
lastExchangeGroupLarger
start_id: 456, end_id: 489
exchangeGroupSizeLast: 33
exchangeBatchSizeLast: 2
start_tile: 727
num_tiles_per_layer[ilay]: 725
start_tile + num_tiles_per_layer[ilay]: 1452
num_occupied_tiles: 488
batchsize: 48
sparse_size: 48
dLdx.shapeToString(): {488,48,48}
replicated_numGrads.shapeToString(): {488,48}
exchangeGroupSize: 24
numExchangeGroups: 20
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
lastExchangeGroupLarger
start_id: 456, end_id: 488
exchangeGroupSizeLast: 32
exchangeBatchSizeLast: 2
start_tile: 1452
num_tiles_per_layer[ilay]: 18
start_tile + num_tiles_per_layer[ilay]: 1470
num_occupied_tiles: 5
batchsize: 48
sparse_size: 48
dLdx.shapeToString(): {5,48,48}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {488,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 22:05:10.048743: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[978 978 976  10]
0 489.0
1 489.0
2 488.0
3 5.0
[TileMapping(start_tile=1, end_tile=490), TileMapping(start_tile=490, end_tile=979), TileMapping(start_tile=979, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=490), TileMapping(start_tile=490, end_tile=979), TileMapping(start_tile=979, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.31330417999518295
limit=0.31330417999518295
limit=0.31362502409359
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 978) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 978) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 976) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_48_48_48_2_48_1_490_979_1467_490_979_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 48), (100 4190734     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][3
                                                                 keras_multi_lif_layer_sparse[0][7
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 4,190,734
Trainable params: 4,181,908
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_48_48_48_2_48_1_490_979_1467_490_979_1467_1472
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_48_48_48_2_48_1_490_979_1467_490_979_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.467852/52 [==============================] - 1179s 23s/step - loss: 2.4678
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.079952/52 [==============================] - 0s 7ms/step - loss: 2.0799
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 2.063252/52 [==============================] - 0s 7ms/step - loss: 2.0632
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 2.097652/52 [==============================] - 0s 7ms/step - loss: 2.0976
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 2.062452/52 [==============================] - 0s 7ms/step - loss: 2.0624
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 2.064852/52 [==============================] - 0s 7ms/step - loss: 2.0648
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 2.069052/52 [==============================] - 0s 7ms/step - loss: 2.0690
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 2.096652/52 [==============================] - 0s 7ms/step - loss: 2.0966
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 2.053552/52 [==============================] - 0s 7ms/step - loss: 2.0535
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 2.040652/52 [==============================] - 0s 8ms/step - loss: 2.0406
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 2.050952/52 [==============================] - 0s 8ms/step - loss: 2.0509
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 2.086052/52 [==============================] - 0s 8ms/step - loss: 2.0860
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 2.051252/52 [==============================] - 0s 7ms/step - loss: 2.0512
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 2.039652/52 [==============================] - 0s 7ms/step - loss: 2.0396
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 2.046052/52 [==============================] - 0s 7ms/step - loss: 2.0460
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 2.079852/52 [==============================] - 0s 7ms/step - loss: 2.0798
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 2.056352/52 [==============================] - 0s 7ms/step - loss: 2.0563
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 2.035652/52 [==============================] - 0s 7ms/step - loss: 2.0356
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 2.043952/52 [==============================] - 0s 7ms/step - loss: 2.0439
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 2.081352/52 [==============================] - 0s 7ms/step - loss: 2.0813
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 2.054552/52 [==============================] - 0s 7ms/step - loss: 2.0545
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 2.064952/52 [==============================] - 0s 7ms/step - loss: 2.0649
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 2.049552/52 [==============================] - 0s 7ms/step - loss: 2.0495
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 2.079752/52 [==============================] - 0s 7ms/step - loss: 2.0797
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 2.047352/52 [==============================] - 0s 7ms/step - loss: 2.0473

Final time:  1187.998547077179
2022-09-12 22:05:22.422028: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.05
DENSE_SIZES:  [2312, 734, 734, 732, 732, 10]
SPARSE_SIZES:  [32, 36, 36, 36, 36, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  4
SECOND_THRESHOLD:  0.98

60000
9984
[55478 46084 30735 ... 57973 52051 39969]
2022-09-12 22:05:31.312667: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 22:05:32.198708: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 11, 2, 1
Build_allocator: 16, 3, 1
Build_allocator: 12, 2, 2
Build_allocator: 17, 3, 2
Build_allocator: 13, 2, 3
Build_allocator: 18, 3, 3
Build_allocator: 14, 2, 4
Build_allocator: 19, 3, 4
Build_allocator: 5, 1, 0
Build_allocator: 6, 1, 1
Build_allocator: 7, 1, 2
Build_allocator: 8, 1, 3
Build_allocator: 9, 1, 4
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 4, 0, 4
Build_allocator: 20, 4, 0
Build_allocator: 21, 4, 1
Build_allocator: 22, 4, 2
Build_allocator: 23, 4, 3
Build_allocator: 24, 4, 4
Build_allocator: 25, 5, 0
Build_allocator: 26, 5, 1
Build_allocator: 27, 5, 2
Build_allocator: 28, 5, 3
Build_allocator: 29, 5, 4
Build_allocator: 10, 2, 0
Build_allocator: 15, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0x1fd82fb0
1: &init_state[ilay]0x1fd82fb8
2: &init_state[ilay]0x1fd82fc0
3: &init_state[ilay]0x1fd82fc8
4: &init_state[ilay]0x1fd82fd0
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0
3: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
3: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 368, 735, 1101, 1467, }
end_tiles: {368, 735, 1101, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{2, 2, 2, 2, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 5, 6, 8, 10, 11, 13, 15, 17, 18, 20, 22, 23, 25, 27, 28, 30, 32, 34, 35, 37, 39, 40, 42, 44, 46, 47, 49, 51, 52, 54, 56, 57, 59, 61, 63, 64, 66, 68, 69, 71, 73, 74, 76, 78, 80, 81, 83, 85, 86, 88, 90, 92, 93, 95, 97, 98, 100, 102, 103, 105, 107, 109, 110, 112, 114, 115, 117, 119, 120, 122, 124, 126, 127, 129, 131, 132, 134, 136, 138, 139, 141, 143, 144, 146, 148, 149, 151, 153, 155, 156, 158, 160, 161, 163, 165, 166, 168, 170, 172, 173, 175, 177, 178, 180, 182, 184, 185, 187, 189, 190, 192, 194, 195, 197, 199, 201, 202, 204, 206, 207, 209, 211, 212, 214, 216, 218, 219, 221, 223, 224, 226, 228, 230, 231, 233, 235, 236, 238, 240, 241, 243, 245, 247, 248, 250, 252, 253, 255, 257, 258, 260, 262, 264, 265, 267, 269, 270, 272, 274, 276, 277, 279, 281, 282, 284, 286, 287, 289, 291, 293, 294, 296, 298, 299, 301, 303, 304, 306, 308, 310, 311, 313, 315, 316, 318, 320, 322, 323, 325, }
{0, 6, 13, 20, 27, 34, 40, 47, 54, 61, 68, 74, 81, 88, 95, 102, 109, 115, 122, 129, 136, 143, 149, 156, 163, 170, 177, 184, 190, 197, 204, 211, 218, 224, 231, 238, 245, 252, 258, 265, 272, 279, 286, 293, 299, 306, 313, 320, }
{327, 328, 330, 332, 333, 335, 337, 339, 340, 342, 344, 345, 347, 349, 350, 352, 354, 356, 357, 359, 361, 362, 364, 366, 368, 369, 371, 373, 374, 376, 378, 379, 381, 383, 385, 386, 388, 390, 391, 393, 395, 396, 398, 400, 402, 403, 405, 407, 408, 410, 412, 414, 415, 417, 419, 420, 422, 424, 425, 427, 429, 431, 432, 434, 436, 437, 439, 441, 442, 444, 446, 448, 449, 451, 453, 454, 456, 458, 460, 461, 463, 465, 466, 468, 470, 471, 473, 475, 477, 478, 480, 482, 483, 485, 487, 488, 490, 492, 494, 495, 497, 499, 500, 502, 504, 506, 507, 509, 511, 512, 514, 516, 517, 519, 521, 523, 524, 526, 528, 529, 531, 533, 534, 536, 538, 540, 541, 543, 545, 546, 548, 550, 552, 553, 555, 557, 558, 560, 562, 563, 565, 567, 569, 570, 572, 574, 575, 577, 579, 580, 582, 584, 586, 587, 589, 591, 592, 594, 596, 598, 599, 601, 603, 604, 606, 608, 609, 611, 613, 615, 616, 618, 620, 621, 623, 625, 626, 628, 630, 632, 633, 635, 637, 638, 640, 642, 644, 645, 647, 649, 650, 652, }
{327, 333, 340, 347, 354, 361, 368, 374, 381, 388, 395, 402, 408, 415, 422, 429, 436, 442, 449, 456, 463, 470, 477, 483, 490, 497, 504, 511, 517, 524, 531, 538, 545, 552, 558, 565, 572, 579, 586, 592, 599, 606, 613, 620, 626, 633, 640, 647, }
{654, 655, 657, 659, 661, 662, 664, 666, 667, 669, 671, 672, 674, 676, 678, 679, 681, 683, 684, 686, 688, 690, 691, 693, 695, 696, 698, 700, 701, 703, 705, 707, 708, 710, 712, 713, 715, 717, 718, 720, 722, 724, 725, 727, 729, 730, 732, 734, 736, 737, 739, 741, 742, 744, 746, 747, 749, 751, 753, 754, 756, 758, 759, 761, 763, 764, 766, 768, 770, 771, 773, 775, 776, 778, 780, 782, 783, 785, 787, 788, 790, 792, 793, 795, 797, 799, 800, 802, 804, 805, 807, 809, 810, 812, 814, 816, 817, 819, 821, 822, 824, 826, 828, 829, 831, 833, 834, 836, 838, 839, 841, 843, 845, 846, 848, 850, 851, 853, 855, 856, 858, 860, 862, 863, 865, 867, 868, 870, 872, 874, 875, 877, 879, 880, 882, 884, 885, 887, 889, 891, 892, 894, 896, 897, 899, 901, 902, 904, 906, 908, 909, 911, 913, 914, 916, 918, 920, 921, 923, 925, 926, 928, 930, 931, 933, 935, 937, 938, 940, 942, 943, 945, 947, 948, 950, 952, 954, 955, 957, 959, 960, 962, 964, 966, 967, 969, 971, 972, 974, 976, 977, 979, }
{654, 661, 667, 674, 681, 688, 695, 701, 708, 715, 722, 729, 736, 742, 749, 756, 763, 770, 776, 783, 790, 797, 804, 810, 817, 824, 831, 838, 845, 851, 858, 865, 872, 879, 885, 892, 899, 906, 913, 920, 926, 933, 940, 947, 954, 960, 967, 974, }
{981, 983, 984, 986, 988, 989, 991, 993, 994, 996, 998, 1000, 1001, 1003, 1005, 1006, 1008, 1010, 1012, 1013, 1015, 1017, 1018, 1020, 1022, 1023, 1025, 1027, 1029, 1030, 1032, 1034, 1035, 1037, 1039, 1040, 1042, 1044, 1046, 1047, 1049, 1051, 1052, 1054, 1056, 1058, 1059, 1061, 1063, 1064, 1066, 1068, 1069, 1071, 1073, 1075, 1076, 1078, 1080, 1081, 1083, 1085, 1086, 1088, 1090, 1092, 1093, 1095, 1097, 1098, 1100, 1102, 1104, 1105, 1107, 1109, 1110, 1112, 1114, 1115, 1117, 1119, 1121, 1122, 1124, 1126, 1127, 1129, 1131, 1132, 1134, 1136, 1138, 1139, 1141, 1143, 1144, 1146, 1148, 1150, 1151, 1153, 1155, 1156, 1158, 1160, 1161, 1163, 1165, 1167, 1168, 1170, 1172, 1173, 1175, 1177, 1178, 1180, 1182, 1184, 1185, 1187, 1189, 1190, 1192, 1194, 1196, 1197, 1199, 1201, 1202, 1204, 1206, 1207, 1209, 1211, 1213, 1214, 1216, 1218, 1219, 1221, 1223, 1224, 1226, 1228, 1230, 1231, 1233, 1235, 1236, 1238, 1240, 1242, 1243, 1245, 1247, 1248, 1250, 1252, 1253, 1255, 1257, 1259, 1260, 1262, 1264, 1265, 1267, 1269, 1270, 1272, 1274, 1276, 1277, 1279, 1281, 1282, 1284, 1286, 1288, 1289, 1291, 1293, 1294, 1296, 1298, 1299, 1301, 1303, 1305, 1306, }
{981, 988, 994, 1001, 1008, 1015, 1022, 1029, 1035, 1042, 1049, 1056, 1063, 1069, 1076, 1083, 1090, 1097, 1104, 1110, 1117, 1124, 1131, 1138, 1144, 1151, 1158, 1165, 1172, 1178, 1185, 1192, 1199, 1206, 1213, 1219, 1226, 1233, 1240, 1247, 1253, 1260, 1267, 1274, 1281, 1288, 1294, 1301, }
{1308, 1310, 1311, 1313, 1315, 1316, 1318, 1320, 1322, 1323, 1325, 1327, 1328, 1330, 1332, 1334, 1335, 1337, 1339, 1340, 1342, 1344, 1345, 1347, 1349, 1351, 1352, 1354, 1356, 1357, 1359, 1361, 1362, 1364, 1366, 1368, 1369, 1371, 1373, 1374, 1376, 1378, 1380, 1381, 1383, 1385, 1386, 1388, 1390, 1391, 1393, 1395, 1397, 1398, 1400, 1402, 1403, 1405, 1407, 1408, 1410, 1412, 1414, 1415, 1417, 1419, 1420, 1422, 1424, 1426, 1427, 1429, 1431, 1432, 1434, 1436, 1437, 1439, 1441, 1443, 1444, 1446, 1448, 1449, 1451, 1453, 1454, 1456, 1458, 1460, 1461, 1463, 1465, 1466, 1468, 1470, }
{1308, 1311, 1315, 1318, 1322, 1325, 1328, 1332, 1335, 1339, 1342, 1345, 1349, 1352, 1356, 1359, 1362, 1366, 1369, 1373, 1376, 1380, 1383, 1386, 1390, 1393, 1397, 1400, 1403, 1407, 1410, 1414, 1417, 1420, 1424, 1427, 1431, 1434, 1437, 1441, 1444, 1448, 1451, 1454, 1458, 1461, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 62
sparse_size_to_use: 36
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 62
sparse_size_to_use: 36
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 61
sparse_size_to_use: 36
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 61
sparse_size_to_use: 36
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,36}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,36}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,36}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,36}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 4
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,36}
slicedInpSpikeIds.shapeToString(): {48,36}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,36}
slicedInpSpikeIds.shapeToString(): {48,36}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,36}
slicedInpSpikeIds.shapeToString(): {48,36}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

3
out_spike_ids[ilay].shapeToString(): {100,48,36}
slicedInpSpikeIds.shapeToString(): {48,36}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,2}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{485, 484, 484, 15, }
start_tile: 1
num_tiles_per_layer[ilay]: 485
start_tile + num_tiles_per_layer[ilay]: 486
num_occupied_tiles: 367
batchsize: 48
sparse_size: 36
dLdx.shapeToString(): {367,48,36}
replicated_numGrads.shapeToString(): {367,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 367
exchangeGroupSizeLast: 31
exchangeBatchSizeLast: 2
start_tile: 486
num_tiles_per_layer[ilay]: 484
start_tile + num_tiles_per_layer[ilay]: 970
num_occupied_tiles: 366
batchsize: 48
sparse_size: 36
dLdx.shapeToString(): {366,48,36}
replicated_numGrads.shapeToString(): {366,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 366
exchangeGroupSizeLast: 30
exchangeBatchSizeLast: 2
start_tile: 970
num_tiles_per_layer[ilay]: 484
start_tile + num_tiles_per_layer[ilay]: 1454
num_occupied_tiles: 366
batchsize: 48
sparse_size: 36
dLdx.shapeToString(): {366,48,36}
replicated_numGrads.shapeToString(): {366,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 366
exchangeGroupSizeLast: 30
exchangeBatchSizeLast: 2
start_tile: 1454
num_tiles_per_layer[ilay]: 15
start_tile + num_tiles_per_layer[ilay]: 1469
num_occupied_tiles: 5
batchsize: 48
sparse_size: 36
dLdx.shapeToString(): {5,48,36}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,1}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 22:19:21.887462: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[734 734 732 732  10]
0 367.0
1 367.0
2 366.0
3 366.0
4 5.0
[TileMapping(start_tile=1, end_tile=368), TileMapping(start_tile=368, end_tile=735), TileMapping(start_tile=735, end_tile=1101), TileMapping(start_tile=1101, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=368), TileMapping(start_tile=368, end_tile=735), TileMapping(start_tile=735, end_tile=1101), TileMapping(start_tile=1101, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.3616492648073473
limit=0.3616492648073473
limit=0.3621429841700741
limit=0.3621429841700741
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 734) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 734) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 732) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 732) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_5:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_36_36_36_36_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 36), (100 3325022     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][4
                                                                 keras_multi_lif_layer_sparse[0][9
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 3,325,022
Trainable params: 3,316,196
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_36_36_36_36_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_36_36_36_36_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.548852/52 [==============================] - 832s 16s/step - loss: 2.5488
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.309652/52 [==============================] - 0s 6ms/step - loss: 2.3096
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 2.293152/52 [==============================] - 0s 6ms/step - loss: 2.2931
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 2.138952/52 [==============================] - 0s 6ms/step - loss: 2.1389
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 2.107352/52 [==============================] - 0s 6ms/step - loss: 2.1073
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 2.122852/52 [==============================] - 0s 6ms/step - loss: 2.1228
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 2.263152/52 [==============================] - 0s 6ms/step - loss: 2.2631
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 2.099052/52 [==============================] - 0s 6ms/step - loss: 2.0990
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 2.076152/52 [==============================] - 0s 6ms/step - loss: 2.0761
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 2.077652/52 [==============================] - 0s 6ms/step - loss: 2.0776
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 2.084752/52 [==============================] - 0s 6ms/step - loss: 2.0847
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 2.067352/52 [==============================] - 0s 6ms/step - loss: 2.0673
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 2.084752/52 [==============================] - 0s 6ms/step - loss: 2.0847
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 2.080252/52 [==============================] - 0s 6ms/step - loss: 2.0802
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 2.072852/52 [==============================] - 0s 6ms/step - loss: 2.0728
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 2.065452/52 [==============================] - 0s 6ms/step - loss: 2.0654
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 2.091752/52 [==============================] - 0s 6ms/step - loss: 2.0917
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 2.087852/52 [==============================] - 0s 6ms/step - loss: 2.0878
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 2.136452/52 [==============================] - 0s 6ms/step - loss: 2.1364
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 2.082452/52 [==============================] - 0s 6ms/step - loss: 2.0824
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 2.099752/52 [==============================] - 0s 6ms/step - loss: 2.0997
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 2.080152/52 [==============================] - 0s 6ms/step - loss: 2.0801
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 2.054852/52 [==============================] - 0s 7ms/step - loss: 2.0548
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 1.982352/52 [==============================] - 0s 6ms/step - loss: 1.9823
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 2.085552/52 [==============================] - 0s 6ms/step - loss: 2.0855

Final time:  839.328752040863
2022-09-12 22:19:32.604144: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.05
DENSE_SIZES:  [2312, 588, 586, 586, 586, 586, 10]
SPARSE_SIZES:  [32, 28, 28, 28, 28, 28, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  5
SECOND_THRESHOLD:  0.98

60000
9984
[52705 43601 14581 ... 58575 21615 34387]
2022-09-12 22:19:42.561287: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 22:19:43.472636: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 13, 2, 1
Build_allocator: 19, 3, 1
Build_allocator: 14, 2, 2
Build_allocator: 20, 3, 2
Build_allocator: 15, 2, 3
Build_allocator: 21, 3, 3
Build_allocator: 16, 2, 4
Build_allocator: 22, 3, 4
Build_allocator: 17, 2, 5
Build_allocator: 23, 3, 5
Build_allocator: 6, 1, 0
Build_allocator: 7, 1, 1
Build_allocator: 8, 1, 2
Build_allocator: 9, 1, 3
Build_allocator: 10, 1, 4
Build_allocator: 11, 1, 5
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 4, 0, 4
Build_allocator: 5, 0, 5
Build_allocator: 24, 4, 0
Build_allocator: 25, 4, 1
Build_allocator: 26, 4, 2
Build_allocator: 27, 4, 3
Build_allocator: 28, 4, 4
Build_allocator: 29, 4, 5
Build_allocator: 30, 5, 0
Build_allocator: 31, 5, 1
Build_allocator: 32, 5, 2
Build_allocator: 33, 5, 3
Build_allocator: 34, 5, 4
Build_allocator: 35, 5, 5
Build_allocator: 12, 2, 0
Build_allocator: 18, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0xd922fc0
1: &init_state[ilay]0xd922fc8
2: &init_state[ilay]0xd922fd0
3: &init_state[ilay]0xd922fd8
4: &init_state[ilay]0xd922fe0
5: &init_state[ilay]0xd922fe8
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0
3: init_state[ilay]==init_state[ilay+1]: 0
4: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
3: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
4: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 295, 588, 881, 1174, 1467, }
end_tiles: {295, 588, 881, 1174, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{2, 2, 2, 2, 2, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 2, 4, 5, 6, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 25, 26, 27, 29, 30, 32, 33, 34, 36, 37, 39, 40, 41, 43, 44, 46, 47, 48, 50, 51, 52, 54, 55, 57, 58, 59, 61, 62, 64, 65, 66, 68, 69, 71, 72, 73, 75, 76, 78, 79, 80, 82, 83, 85, 86, 87, 89, 90, 92, 93, 94, 96, 97, 98, 100, 101, 103, 104, 105, 107, 108, 110, 111, 112, 114, 115, 117, 118, 119, 121, 122, 124, 125, 126, 128, 129, 131, 132, 133, 135, 136, 138, 139, 140, 142, 143, 144, 146, 147, 149, 150, 151, 153, 154, 156, 157, 158, 160, 161, 163, 164, 165, 167, 168, 170, 171, 172, 174, 175, 177, 178, 179, 181, 182, 184, 185, 186, 188, 189, 190, 192, 193, 195, 196, 197, 199, 200, 202, 203, 204, 206, 207, 209, 210, 211, 213, 214, 216, 217, 218, 220, 221, 223, 224, 225, 227, 228, 230, 231, 232, 234, 235, 236, 238, 239, 241, 242, 243, 245, 246, 248, 249, 250, 252, 253, 255, 256, 257, 259, 260, 262, 263, 264, 266, }
{0, 5, 11, 16, 22, 27, 33, 39, 44, 50, 55, 61, 66, 72, 78, 83, 89, 94, 100, 105, 111, 117, 122, 128, 133, 139, 144, 150, 156, 161, 167, 172, 178, 184, 189, 195, 200, 206, 211, 217, 223, 228, 234, 239, 245, 250, 256, 262, }
{267, 269, 270, 271, 273, 274, 276, 277, 278, 280, 281, 282, 284, 285, 287, 288, 289, 291, 292, 294, 295, 296, 298, 299, 301, 302, 303, 305, 306, 308, 309, 310, 312, 313, 315, 316, 317, 319, 320, 322, 323, 324, 326, 327, 328, 330, 331, 333, 334, 335, 337, 338, 340, 341, 342, 344, 345, 347, 348, 349, 351, 352, 354, 355, 356, 358, 359, 361, 362, 363, 365, 366, 368, 369, 370, 372, 373, 374, 376, 377, 379, 380, 381, 383, 384, 386, 387, 388, 390, 391, 393, 394, 395, 397, 398, 400, 401, 402, 404, 405, 407, 408, 409, 411, 412, 414, 415, 416, 418, 419, 420, 422, 423, 425, 426, 427, 429, 430, 432, 433, 434, 436, 437, 439, 440, 441, 443, 444, 446, 447, 448, 450, 451, 453, 454, 455, 457, 458, 460, 461, 462, 464, 465, 466, 468, 469, 471, 472, 473, 475, 476, 478, 479, 480, 482, 483, 485, 486, 487, 489, 490, 492, 493, 494, 496, 497, 499, 500, 501, 503, 504, 506, 507, 508, 510, 511, 512, 514, 515, 517, 518, 519, 521, 522, 524, 525, 526, 528, 529, 531, 532, 533, }
{267, 273, 278, 284, 289, 295, 301, 306, 312, 317, 323, 328, 334, 340, 345, 351, 356, 362, 368, 373, 379, 384, 390, 395, 401, 407, 412, 418, 423, 429, 434, 440, 446, 451, 457, 462, 468, 473, 479, 485, 490, 496, 501, 507, 512, 518, 524, 529, }
{535, 536, 538, 539, 540, 542, 543, 545, 546, 547, 549, 550, 552, 553, 554, 556, 557, 558, 560, 561, 563, 564, 565, 567, 568, 570, 571, 572, 574, 575, 577, 578, 579, 581, 582, 584, 585, 586, 588, 589, 591, 592, 593, 595, 596, 598, 599, 600, 602, 603, 604, 606, 607, 609, 610, 611, 613, 614, 616, 617, 618, 620, 621, 623, 624, 625, 627, 628, 630, 631, 632, 634, 635, 637, 638, 639, 641, 642, 644, 645, 646, 648, 649, 650, 652, 653, 655, 656, 657, 659, 660, 662, 663, 664, 666, 667, 669, 670, 671, 673, 674, 676, 677, 678, 680, 681, 683, 684, 685, 687, 688, 690, 691, 692, 694, 695, 696, 698, 699, 701, 702, 703, 705, 706, 708, 709, 710, 712, 713, 715, 716, 717, 719, 720, 722, 723, 724, 726, 727, 729, 730, 731, 733, 734, 736, 737, 738, 740, 741, 742, 744, 745, 747, 748, 749, 751, 752, 754, 755, 756, 758, 759, 761, 762, 763, 765, 766, 768, 769, 770, 772, 773, 775, 776, 777, 779, 780, 782, 783, 784, 786, 787, 788, 790, 791, 793, 794, 795, 797, 798, 800, 801, }
{535, 540, 546, 552, 557, 563, 568, 574, 579, 585, 591, 596, 602, 607, 613, 618, 624, 630, 635, 641, 646, 652, 657, 663, 669, 674, 680, 685, 691, 696, 702, 708, 713, 719, 724, 730, 736, 741, 747, 752, 758, 763, 769, 775, 780, 786, 791, 797, }
{802, 804, 805, 807, 808, 809, 811, 812, 814, 815, 816, 818, 819, 821, 822, 823, 825, 826, 828, 829, 830, 832, 833, 834, 836, 837, 839, 840, 841, 843, 844, 846, 847, 848, 850, 851, 853, 854, 855, 857, 858, 860, 861, 862, 864, 865, 867, 868, 869, 871, 872, 874, 875, 876, 878, 879, 880, 882, 883, 885, 886, 887, 889, 890, 892, 893, 894, 896, 897, 899, 900, 901, 903, 904, 906, 907, 908, 910, 911, 913, 914, 915, 917, 918, 920, 921, 922, 924, 925, 926, 928, 929, 931, 932, 933, 935, 936, 938, 939, 940, 942, 943, 945, 946, 947, 949, 950, 952, 953, 954, 956, 957, 959, 960, 961, 963, 964, 966, 967, 968, 970, 971, 972, 974, 975, 977, 978, 979, 981, 982, 984, 985, 986, 988, 989, 991, 992, 993, 995, 996, 998, 999, 1000, 1002, 1003, 1005, 1006, 1007, 1009, 1010, 1012, 1013, 1014, 1016, 1017, 1018, 1020, 1021, 1023, 1024, 1025, 1027, 1028, 1030, 1031, 1032, 1034, 1035, 1037, 1038, 1039, 1041, 1042, 1044, 1045, 1046, 1048, 1049, 1051, 1052, 1053, 1055, 1056, 1058, 1059, 1060, 1062, 1063, 1064, 1066, 1067, 1069, }
{802, 808, 814, 819, 825, 830, 836, 841, 847, 853, 858, 864, 869, 875, 880, 886, 892, 897, 903, 908, 914, 920, 925, 931, 936, 942, 947, 953, 959, 964, 970, 975, 981, 986, 992, 998, 1003, 1009, 1014, 1020, 1025, 1031, 1037, 1042, 1048, 1053, 1059, 1064, }
{1070, 1071, 1073, 1074, 1076, 1077, 1078, 1080, 1081, 1083, 1084, 1085, 1087, 1088, 1090, 1091, 1092, 1094, 1095, 1097, 1098, 1099, 1101, 1102, 1104, 1105, 1106, 1108, 1109, 1110, 1112, 1113, 1115, 1116, 1117, 1119, 1120, 1122, 1123, 1124, 1126, 1127, 1129, 1130, 1131, 1133, 1134, 1136, 1137, 1138, 1140, 1141, 1143, 1144, 1145, 1147, 1148, 1150, 1151, 1152, 1154, 1155, 1156, 1158, 1159, 1161, 1162, 1163, 1165, 1166, 1168, 1169, 1170, 1172, 1173, 1175, 1176, 1177, 1179, 1180, 1182, 1183, 1184, 1186, 1187, 1189, 1190, 1191, 1193, 1194, 1196, 1197, 1198, 1200, 1201, 1202, 1204, 1205, 1207, 1208, 1209, 1211, 1212, 1214, 1215, 1216, 1218, 1219, 1221, 1222, 1223, 1225, 1226, 1228, 1229, 1230, 1232, 1233, 1235, 1236, 1237, 1239, 1240, 1242, 1243, 1244, 1246, 1247, 1248, 1250, 1251, 1253, 1254, 1255, 1257, 1258, 1260, 1261, 1262, 1264, 1265, 1267, 1268, 1269, 1271, 1272, 1274, 1275, 1276, 1278, 1279, 1281, 1282, 1283, 1285, 1286, 1288, 1289, 1290, 1292, 1293, 1294, 1296, 1297, 1299, 1300, 1301, 1303, 1304, 1306, 1307, 1308, 1310, 1311, 1313, 1314, 1315, 1317, 1318, 1320, 1321, 1322, 1324, 1325, 1327, 1328, 1329, 1331, 1332, 1334, 1335, 1336, }
{1070, 1076, 1081, 1087, 1092, 1098, 1104, 1109, 1115, 1120, 1126, 1131, 1137, 1143, 1148, 1154, 1159, 1165, 1170, 1176, 1182, 1187, 1193, 1198, 1204, 1209, 1215, 1221, 1226, 1232, 1237, 1243, 1248, 1254, 1260, 1265, 1271, 1276, 1282, 1288, 1293, 1299, 1304, 1310, 1315, 1321, 1327, 1332, }
{1338, 1339, 1340, 1342, 1343, 1345, 1346, 1347, 1349, 1350, 1352, 1353, 1354, 1356, 1357, 1359, 1360, 1361, 1363, 1364, 1366, 1367, 1368, 1370, 1371, 1373, 1374, 1375, 1377, 1378, 1380, 1381, 1382, 1384, 1385, 1386, 1388, 1389, 1391, 1392, 1393, 1395, 1396, 1398, 1399, 1400, 1402, 1403, 1405, 1406, 1407, 1409, 1410, 1412, 1413, 1414, 1416, 1417, 1419, 1420, 1421, 1423, 1424, 1426, 1427, 1428, 1430, 1431, 1432, 1434, 1435, 1437, 1438, 1439, 1441, 1442, 1444, 1445, 1446, 1448, 1449, 1451, 1452, 1453, 1455, 1456, 1458, 1459, 1460, 1462, 1463, 1465, 1466, 1467, 1469, 1470, }
{1338, 1340, 1343, 1346, 1349, 1352, 1354, 1357, 1360, 1363, 1366, 1368, 1371, 1374, 1377, 1380, 1382, 1385, 1388, 1391, 1393, 1396, 1399, 1402, 1405, 1407, 1410, 1413, 1416, 1419, 1421, 1424, 1427, 1430, 1432, 1435, 1438, 1441, 1444, 1446, 1449, 1452, 1455, 1458, 1460, 1463, 1466, 1469, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 28
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 28
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 28
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 28
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 28
sparseSmallerStatesPerWorker: 1
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,28}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,28}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,28}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,28}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,28}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 4
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 5
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,28}
slicedInpSpikeIds.shapeToString(): {48,28}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,28}
slicedInpSpikeIds.shapeToString(): {48,28}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,28}
slicedInpSpikeIds.shapeToString(): {48,28}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

3
out_spike_ids[ilay].shapeToString(): {100,48,28}
slicedInpSpikeIds.shapeToString(): {48,28}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

4
out_spike_ids[ilay].shapeToString(): {100,48,28}
slicedInpSpikeIds.shapeToString(): {48,28}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {294,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 5
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{364, 364, 364, 364, 14, }
start_tile: 1
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 365
num_occupied_tiles: 293
batchsize: 48
sparse_size: 28
dLdx.shapeToString(): {293,48,28}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 365
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 729
num_occupied_tiles: 293
batchsize: 48
sparse_size: 28
dLdx.shapeToString(): {293,48,28}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 729
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 1093
num_occupied_tiles: 293
batchsize: 48
sparse_size: 28
dLdx.shapeToString(): {293,48,28}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 1093
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 1457
num_occupied_tiles: 293
batchsize: 48
sparse_size: 28
dLdx.shapeToString(): {293,48,28}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 1457
num_tiles_per_layer[ilay]: 14
start_tile + num_tiles_per_layer[ilay]: 1471
num_occupied_tiles: 5
batchsize: 48
sparse_size: 28
dLdx.shapeToString(): {5,48,28}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {294,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 5
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 22:28:13.168290: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[588 586 586 586 586  10]
0 294.0
1 293.0
2 293.0
3 293.0
4 293.0
5 5.0
[TileMapping(start_tile=1, end_tile=295), TileMapping(start_tile=295, end_tile=588), TileMapping(start_tile=588, end_tile=881), TileMapping(start_tile=881, end_tile=1174), TileMapping(start_tile=1174, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=295), TileMapping(start_tile=295, end_tile=588), TileMapping(start_tile=588, end_tile=881), TileMapping(start_tile=881, end_tile=1174), TileMapping(start_tile=1174, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.40406101782088427
limit=0.4047499544129309
limit=0.4047499544129309
limit=0.4047499544129309
limit=0.4047499544129309
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 588) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 586) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 586) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 586) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_5:0' shape=(2, 586) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_6:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_28_28_28_28_28_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 28), (100 2748898     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][5
                                                                 keras_multi_lif_layer_sparse[0][1
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 2,748,898
Trainable params: 2,740,072
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_28_28_28_28_28_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_28_28_28_28_28_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.401252/52 [==============================] - 512s 10s/step - loss: 2.4012
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.151852/52 [==============================] - 0s 6ms/step - loss: 2.1518
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 2.153052/52 [==============================] - 0s 5ms/step - loss: 2.1530
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 2.154252/52 [==============================] - 0s 5ms/step - loss: 2.1542
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 2.134052/52 [==============================] - 0s 5ms/step - loss: 2.1340
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 2.146052/52 [==============================] - 0s 6ms/step - loss: 2.1460
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 2.121852/52 [==============================] - 0s 6ms/step - loss: 2.1218
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 2.140752/52 [==============================] - 0s 6ms/step - loss: 2.1407
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 2.044352/52 [==============================] - 0s 6ms/step - loss: 2.0443
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 2.012852/52 [==============================] - 0s 6ms/step - loss: 2.0128
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 2.140852/52 [==============================] - 0s 8ms/step - loss: 2.1408
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 2.191952/52 [==============================] - 0s 6ms/step - loss: 2.1919
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 2.179852/52 [==============================] - 0s 6ms/step - loss: 2.1798
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 2.161352/52 [==============================] - 0s 6ms/step - loss: 2.1613
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 2.201652/52 [==============================] - 0s 8ms/step - loss: 2.2016
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 2.203152/52 [==============================] - 0s 6ms/step - loss: 2.2031
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 2.164752/52 [==============================] - 0s 6ms/step - loss: 2.1647
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 2.141052/52 [==============================] - 0s 6ms/step - loss: 2.1410
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 2.117152/52 [==============================] - 0s 6ms/step - loss: 2.1171
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 2.146052/52 [==============================] - 0s 6ms/step - loss: 2.1460
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 2.137952/52 [==============================] - 0s 6ms/step - loss: 2.1379
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 2.106152/52 [==============================] - 0s 6ms/step - loss: 2.1061
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 2.103352/52 [==============================] - 0s 6ms/step - loss: 2.1033
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 2.124052/52 [==============================] - 0s 6ms/step - loss: 2.1240
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 2.095152/52 [==============================] - 0s 6ms/step - loss: 2.0951

Final time:  519.0107574462891
2022-09-12 22:28:23.473295: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.1
DENSE_SIZES:  [2312, 1466, 1466, 10]
SPARSE_SIZES:  [32, 146, 146, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  2
SECOND_THRESHOLD:  0.9

60000
9984
[55293 25940 37252 ... 55190  4885 56840]
2022-09-12 22:28:31.791755: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 22:28:32.670560: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 7, 2, 1
Build_allocator: 10, 3, 1
Build_allocator: 8, 2, 2
Build_allocator: 11, 3, 2
Build_allocator: 3, 1, 0
Build_allocator: 4, 1, 1
Build_allocator: 5, 1, 2
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 12, 4, 0
Build_allocator: 13, 4, 1
Build_allocator: 14, 4, 2
Build_allocator: 15, 5, 0
Build_allocator: 16, 5, 1
Build_allocator: 17, 5, 2
Build_allocator: 6, 2, 0
Build_allocator: 9, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0x75df580
1: &init_state[ilay]0x75df588
2: &init_state[ilay]0x75df590
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 734, 1467, }
end_tiles: {734, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{4, 4, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 5, 6, 8, 10, 11, 13, 15, 17, 18, 20, 22, 23, 25, 27, 28, 30, 32, 34, 35, 37, 39, 40, 42, 44, 46, 47, 49, 51, 52, 54, 56, 57, 59, 61, 63, 64, 66, 68, 69, 71, 73, 74, 76, 78, 80, 81, 83, 85, 86, 88, 90, 92, 93, 95, 97, 98, 100, 102, 103, 105, 107, 109, 110, 112, 114, 115, 117, 119, 120, 122, 124, 126, 127, 129, 131, 132, 134, 136, 138, 139, 141, 143, 144, 146, 148, 149, 151, 153, 155, 156, 158, 160, 161, 163, 165, 166, 168, 170, 172, 173, 175, 177, 178, 180, 182, 184, 185, 187, 189, 190, 192, 194, 195, 197, 199, 201, 202, 204, 206, 207, 209, 211, 212, 214, 216, 218, 219, 221, 223, 224, 226, 228, 230, 231, 233, 235, 236, 238, 240, 241, 243, 245, 247, 248, 250, 252, 253, 255, 257, 258, 260, 262, 264, 265, 267, 269, 270, 272, 274, 276, 277, 279, 281, 282, 284, 286, 287, 289, 291, 293, 294, 296, 298, 299, 301, 303, 304, 306, 308, 310, 311, 313, 315, 316, 318, 320, 322, 323, 325, 327, 328, 330, 332, 333, 335, 337, 339, 340, 342, 344, 345, 347, 349, 350, 352, 354, 356, 357, 359, 361, 362, 364, 366, 368, 369, 371, 373, 374, 376, 378, 379, 381, 383, 385, 386, 388, 390, 391, 393, 395, 396, 398, 400, 402, 403, 405, 407, 408, 410, 412, 414, 415, 417, 419, 420, 422, 424, 425, 427, 429, 431, 432, 434, 436, 437, 439, 441, 442, 444, 446, 448, 449, 451, 453, 454, 456, 458, 460, 461, 463, 465, 466, 468, 470, 471, 473, 475, 477, 478, 480, 482, 483, 485, 487, 488, 490, 492, 494, 495, 497, 499, 500, 502, 504, 506, 507, 509, 511, 512, 514, 516, 517, 519, 521, 523, 524, 526, 528, 529, 531, 533, 534, 536, 538, 540, 541, 543, 545, 546, 548, 550, 552, 553, 555, 557, 558, 560, 562, 563, 565, 567, 569, 570, 572, 574, 575, 577, 579, 580, 582, 584, 586, 587, 589, 591, 592, 594, 596, 598, 599, 601, 603, 604, 606, 608, 609, 611, 613, 615, 616, 618, 620, 621, 623, 625, 626, 628, 630, 632, 633, 635, 637, 638, 640, 642, 644, 645, 647, 649, 650, 652, }
{0, 13, 27, 40, 54, 68, 81, 95, 109, 122, 136, 149, 163, 177, 190, 204, 218, 231, 245, 258, 272, 286, 299, 313, 327, 340, 354, 368, 381, 395, 408, 422, 436, 449, 463, 477, 490, 504, 517, 531, 545, 558, 572, 586, 599, 613, 626, 640, }
{654, 655, 657, 659, 661, 662, 664, 666, 667, 669, 671, 672, 674, 676, 678, 679, 681, 683, 684, 686, 688, 690, 691, 693, 695, 696, 698, 700, 701, 703, 705, 707, 708, 710, 712, 713, 715, 717, 718, 720, 722, 724, 725, 727, 729, 730, 732, 734, 736, 737, 739, 741, 742, 744, 746, 747, 749, 751, 753, 754, 756, 758, 759, 761, 763, 764, 766, 768, 770, 771, 773, 775, 776, 778, 780, 782, 783, 785, 787, 788, 790, 792, 793, 795, 797, 799, 800, 802, 804, 805, 807, 809, 810, 812, 814, 816, 817, 819, 821, 822, 824, 826, 828, 829, 831, 833, 834, 836, 838, 839, 841, 843, 845, 846, 848, 850, 851, 853, 855, 856, 858, 860, 862, 863, 865, 867, 868, 870, 872, 874, 875, 877, 879, 880, 882, 884, 885, 887, 889, 891, 892, 894, 896, 897, 899, 901, 902, 904, 906, 908, 909, 911, 913, 914, 916, 918, 920, 921, 923, 925, 926, 928, 930, 931, 933, 935, 937, 938, 940, 942, 943, 945, 947, 948, 950, 952, 954, 955, 957, 959, 960, 962, 964, 966, 967, 969, 971, 972, 974, 976, 977, 979, 981, 983, 984, 986, 988, 989, 991, 993, 994, 996, 998, 1000, 1001, 1003, 1005, 1006, 1008, 1010, 1012, 1013, 1015, 1017, 1018, 1020, 1022, 1023, 1025, 1027, 1029, 1030, 1032, 1034, 1035, 1037, 1039, 1040, 1042, 1044, 1046, 1047, 1049, 1051, 1052, 1054, 1056, 1058, 1059, 1061, 1063, 1064, 1066, 1068, 1069, 1071, 1073, 1075, 1076, 1078, 1080, 1081, 1083, 1085, 1086, 1088, 1090, 1092, 1093, 1095, 1097, 1098, 1100, 1102, 1104, 1105, 1107, 1109, 1110, 1112, 1114, 1115, 1117, 1119, 1121, 1122, 1124, 1126, 1127, 1129, 1131, 1132, 1134, 1136, 1138, 1139, 1141, 1143, 1144, 1146, 1148, 1150, 1151, 1153, 1155, 1156, 1158, 1160, 1161, 1163, 1165, 1167, 1168, 1170, 1172, 1173, 1175, 1177, 1178, 1180, 1182, 1184, 1185, 1187, 1189, 1190, 1192, 1194, 1196, 1197, 1199, 1201, 1202, 1204, 1206, 1207, 1209, 1211, 1213, 1214, 1216, 1218, 1219, 1221, 1223, 1224, 1226, 1228, 1230, 1231, 1233, 1235, 1236, 1238, 1240, 1242, 1243, 1245, 1247, 1248, 1250, 1252, 1253, 1255, 1257, 1259, 1260, 1262, 1264, 1265, 1267, 1269, 1270, 1272, 1274, 1276, 1277, 1279, 1281, 1282, 1284, 1286, 1288, 1289, 1291, 1293, 1294, 1296, 1298, 1299, 1301, 1303, 1305, 1306, }
{654, 667, 681, 695, 708, 722, 736, 749, 763, 776, 790, 804, 817, 831, 845, 858, 872, 885, 899, 913, 926, 940, 954, 967, 981, 994, 1008, 1022, 1035, 1049, 1063, 1076, 1090, 1104, 1117, 1131, 1144, 1158, 1172, 1185, 1199, 1213, 1226, 1240, 1253, 1267, 1281, 1294, }
{1308, 1310, 1311, 1313, 1315, 1316, 1318, 1320, 1322, 1323, 1325, 1327, 1328, 1330, 1332, 1334, 1335, 1337, 1339, 1340, 1342, 1344, 1345, 1347, 1349, 1351, 1352, 1354, 1356, 1357, 1359, 1361, 1362, 1364, 1366, 1368, 1369, 1371, 1373, 1374, 1376, 1378, 1380, 1381, 1383, 1385, 1386, 1388, 1390, 1391, 1393, 1395, 1397, 1398, 1400, 1402, 1403, 1405, 1407, 1408, 1410, 1412, 1414, 1415, 1417, 1419, 1420, 1422, 1424, 1426, 1427, 1429, 1431, 1432, 1434, 1436, 1437, 1439, 1441, 1443, 1444, 1446, 1448, 1449, 1451, 1453, 1454, 1456, 1458, 1460, 1461, 1463, 1465, 1466, 1468, 1470, }
{1308, 1311, 1315, 1318, 1322, 1325, 1328, 1332, 1335, 1339, 1342, 1345, 1349, 1352, 1356, 1359, 1362, 1366, 1369, 1373, 1376, 1380, 1383, 1386, 1390, 1393, 1397, 1400, 1403, 1407, 1410, 1414, 1417, 1420, 1424, 1427, 1431, 1434, 1437, 1441, 1444, 1448, 1451, 1454, 1458, 1461, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 4
num_workers: 24
max_num_states_per_worker: 62
sparse_size_to_use: 62
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 4
num_workers: 24
max_num_states_per_worker: 62
sparse_size_to_use: 62
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,4,146}
 repeated_sparse_spike_ids.shapeToString(): {48,2,4,146}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,146}
slicedInpSpikeIds.shapeToString(): {48,146}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,146}
slicedInpSpikeIds.shapeToString(): {48,146}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{1444, 26, }
start_tile: 1
num_tiles_per_layer[ilay]: 1444
start_tile + num_tiles_per_layer[ilay]: 1445
num_occupied_tiles: 733
batchsize: 48
sparse_size: 146
dLdx.shapeToString(): {733,48,146}
replicated_numGrads.shapeToString(): {733,48}
exchangeGroupSize: 24
numExchangeGroups: 30
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
start_id: 456, end_id: 480
start_id: 480, end_id: 504
start_id: 504, end_id: 528
start_id: 528, end_id: 552
start_id: 552, end_id: 576
start_id: 576, end_id: 600
start_id: 600, end_id: 624
start_id: 624, end_id: 648
start_id: 648, end_id: 672
start_id: 672, end_id: 696
lastExchangeGroupLarger
start_id: 696, end_id: 733
exchangeGroupSizeLast: 37
exchangeBatchSizeLast: 2
start_tile: 1445
num_tiles_per_layer[ilay]: 26
start_tile + num_tiles_per_layer[ilay]: 1471
num_occupied_tiles: 5
batchsize: 48
sparse_size: 146
dLdx.shapeToString(): {5,48,146}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 22:44:34.995854: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[1466 1466   10]
0 733.0
1 733.0
2 5.0
[TileMapping(start_tile=1, end_tile=734), TileMapping(start_tile=734, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=734), TileMapping(start_tile=734, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.2558990251065398
limit=0.2558990251065398
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 1466) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 1466) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_146_146_2_48_1_734_1467_734_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 146), (10 5562034     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][2
                                                                 keras_multi_lif_layer_sparse[0][5
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 5,562,034
Trainable params: 5,553,208
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_146_146_2_48_1_734_1467_734_1467_1472
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_146_146_2_48_1_734_1467_734_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.423452/52 [==============================] - 965s 19s/step - loss: 2.4234
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.137352/52 [==============================] - 1s 11ms/step - loss: 2.1373
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 2.105552/52 [==============================] - 1s 10ms/step - loss: 2.1055
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 2.085852/52 [==============================] - 1s 10ms/step - loss: 2.0858
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 1.876552/52 [==============================] - 1s 10ms/step - loss: 1.8765
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 1.804452/52 [==============================] - 1s 10ms/step - loss: 1.8044
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 1.795652/52 [==============================] - 1s 10ms/step - loss: 1.7956
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 1.835252/52 [==============================] - 1s 10ms/step - loss: 1.8352
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 1.619952/52 [==============================] - 1s 10ms/step - loss: 1.6199
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 1.250052/52 [==============================] - 1s 10ms/step - loss: 1.2500
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 1.273252/52 [==============================] - 1s 10ms/step - loss: 1.2732
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 1.333252/52 [==============================] - 1s 11ms/step - loss: 1.3332
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 1.280052/52 [==============================] - 1s 10ms/step - loss: 1.2800
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 1.178752/52 [==============================] - 1s 10ms/step - loss: 1.1787
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 1.195652/52 [==============================] - 1s 11ms/step - loss: 1.1956
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 1.273752/52 [==============================] - 1s 11ms/step - loss: 1.2737
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 1.356152/52 [==============================] - 1s 10ms/step - loss: 1.3561
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 1.249152/52 [==============================] - 1s 11ms/step - loss: 1.2491
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 1.200552/52 [==============================] - 1s 13ms/step - loss: 1.2005
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 1.206352/52 [==============================] - 1s 10ms/step - loss: 1.2063
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 1.233652/52 [==============================] - 1s 11ms/step - loss: 1.2336
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 1.111752/52 [==============================] - 1s 11ms/step - loss: 1.1117
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 1.157152/52 [==============================] - 1s 11ms/step - loss: 1.1571
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 1.198352/52 [==============================] - 1s 11ms/step - loss: 1.1983
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 1.147552/52 [==============================] - 1s 11ms/step - loss: 1.1475

Final time:  978.0974321365356
2022-09-12 22:44:52.082757: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.1
DENSE_SIZES:  [2312, 978, 978, 976, 10]
SPARSE_SIZES:  [32, 96, 96, 96, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  3
SECOND_THRESHOLD:  0.9

60000
9984
[29086 41580 31303 ... 15490  3452 49454]
2022-09-12 22:45:00.592677: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 22:45:01.502039: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 9, 2, 1
Build_allocator: 13, 3, 1
Build_allocator: 10, 2, 2
Build_allocator: 14, 3, 2
Build_allocator: 11, 2, 3
Build_allocator: 15, 3, 3
Build_allocator: 4, 1, 0
Build_allocator: 5, 1, 1
Build_allocator: 6, 1, 2
Build_allocator: 7, 1, 3
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 16, 4, 0
Build_allocator: 17, 4, 1
Build_allocator: 18, 4, 2
Build_allocator: 19, 4, 3
Build_allocator: 20, 5, 0
Build_allocator: 21, 5, 1
Build_allocator: 22, 5, 2
Build_allocator: 23, 5, 3
Build_allocator: 8, 2, 0
Build_allocator: 12, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0x27142b00
1: &init_state[ilay]0x27142b08
2: &init_state[ilay]0x27142b10
3: &init_state[ilay]0x27142b18
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 490, 979, 1467, }
end_tiles: {490, 979, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{3, 3, 3, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 4, 6, 7, 9, 10, 12, 13, 15, 16, 18, 19, 21, 23, 24, 26, 27, 29, 30, 32, 33, 35, 36, 38, 39, 41, 42, 44, 46, 47, 49, 50, 52, 53, 55, 56, 58, 59, 61, 62, 64, 65, 67, 69, 70, 72, 73, 75, 76, 78, 79, 81, 82, 84, 85, 87, 88, 90, 92, 93, 95, 96, 98, 99, 101, 102, 104, 105, 107, 108, 110, 111, 113, 115, 116, 118, 119, 121, 122, 124, 125, 127, 128, 130, 131, 133, 134, 136, 138, 139, 141, 142, 144, 145, 147, 148, 150, 151, 153, 154, 156, 157, 159, 161, 162, 164, 165, 167, 168, 170, 171, 173, 174, 176, 177, 179, 180, 182, 184, 185, 187, 188, 190, 191, 193, 194, 196, 197, 199, 200, 202, 203, 205, 207, 208, 210, 211, 213, 214, 216, 217, 219, 220, 222, 223, 225, 226, 228, 230, 231, 233, 234, 236, 237, 239, 240, 242, 243, 245, 246, 248, 249, 251, 253, 254, 256, 257, 259, 260, 262, 263, 265, 266, 268, 269, 271, 272, 274, 276, 277, 279, 280, 282, 283, 285, 286, 288, 289, 291, 292, 294, 295, 297, 299, 300, 302, 303, 305, 306, 308, 309, 311, 312, 314, 315, 317, 318, 320, 322, 323, 325, 326, 328, 329, 331, 332, 334, 335, 337, 338, 340, 341, 343, 345, 346, 348, 349, 351, 352, 354, 355, 357, 358, 360, 361, 363, 364, 366, 368, 369, 371, 372, 374, 375, 377, 378, 380, 381, 383, 384, 386, 387, 389, 391, 392, 394, 395, 397, 398, 400, 401, 403, 404, 406, 407, 409, 410, 412, 414, 415, 417, 418, 420, 421, 423, 424, 426, 427, 429, 430, 432, 433, 435, 437, 438, 440, }
{0, 9, 18, 27, 36, 46, 55, 64, 73, 82, 92, 101, 110, 119, 128, 138, 147, 156, 165, 174, 184, 193, 202, 211, 220, 230, 239, 248, 257, 266, 276, 285, 294, 303, 312, 322, 331, 340, 349, 358, 368, 377, 386, 395, 404, 414, 423, 432, }
{441, 443, 444, 446, 447, 449, 450, 452, 453, 455, 456, 458, 460, 461, 463, 464, 466, 467, 469, 470, 472, 473, 475, 476, 478, 479, 481, 483, 484, 486, 487, 489, 490, 492, 493, 495, 496, 498, 499, 501, 502, 504, 506, 507, 509, 510, 512, 513, 515, 516, 518, 519, 521, 522, 524, 525, 527, 529, 530, 532, 533, 535, 536, 538, 539, 541, 542, 544, 545, 547, 548, 550, 552, 553, 555, 556, 558, 559, 561, 562, 564, 565, 567, 568, 570, 571, 573, 575, 576, 578, 579, 581, 582, 584, 585, 587, 588, 590, 591, 593, 594, 596, 598, 599, 601, 602, 604, 605, 607, 608, 610, 611, 613, 614, 616, 617, 619, 621, 622, 624, 625, 627, 628, 630, 631, 633, 634, 636, 637, 639, 640, 642, 644, 645, 647, 648, 650, 651, 653, 654, 656, 657, 659, 660, 662, 663, 665, 667, 668, 670, 671, 673, 674, 676, 677, 679, 680, 682, 683, 685, 686, 688, 690, 691, 693, 694, 696, 697, 699, 700, 702, 703, 705, 706, 708, 709, 711, 713, 714, 716, 717, 719, 720, 722, 723, 725, 726, 728, 729, 731, 732, 734, 736, 737, 739, 740, 742, 743, 745, 746, 748, 749, 751, 752, 754, 755, 757, 759, 760, 762, 763, 765, 766, 768, 769, 771, 772, 774, 775, 777, 778, 780, 782, 783, 785, 786, 788, 789, 791, 792, 794, 795, 797, 798, 800, 801, 803, 805, 806, 808, 809, 811, 812, 814, 815, 817, 818, 820, 821, 823, 824, 826, 828, 829, 831, 832, 834, 835, 837, 838, 840, 841, 843, 844, 846, 847, 849, 851, 852, 854, 855, 857, 858, 860, 861, 863, 864, 866, 867, 869, 870, 872, 874, 875, 877, 878, 880, 881, }
{441, 450, 460, 469, 478, 487, 496, 506, 515, 524, 533, 542, 552, 561, 570, 579, 588, 598, 607, 616, 625, 634, 644, 653, 662, 671, 680, 690, 699, 708, 717, 726, 736, 745, 754, 763, 772, 782, 791, 800, 809, 818, 828, 837, 846, 855, 864, 874, }
{883, 884, 886, 887, 889, 890, 892, 893, 895, 897, 898, 900, 901, 903, 904, 906, 907, 909, 910, 912, 913, 915, 916, 918, 920, 921, 923, 924, 926, 927, 929, 930, 932, 933, 935, 936, 938, 939, 941, 943, 944, 946, 947, 949, 950, 952, 953, 955, 956, 958, 959, 961, 962, 964, 966, 967, 969, 970, 972, 973, 975, 976, 978, 979, 981, 982, 984, 985, 987, 989, 990, 992, 993, 995, 996, 998, 999, 1001, 1002, 1004, 1005, 1007, 1008, 1010, 1012, 1013, 1015, 1016, 1018, 1019, 1021, 1022, 1024, 1025, 1027, 1028, 1030, 1031, 1033, 1035, 1036, 1038, 1039, 1041, 1042, 1044, 1045, 1047, 1048, 1050, 1051, 1053, 1054, 1056, 1058, 1059, 1061, 1062, 1064, 1065, 1067, 1068, 1070, 1071, 1073, 1074, 1076, 1077, 1079, 1081, 1082, 1084, 1085, 1087, 1088, 1090, 1091, 1093, 1094, 1096, 1097, 1099, 1100, 1102, 1104, 1105, 1107, 1108, 1110, 1111, 1113, 1114, 1116, 1117, 1119, 1120, 1122, 1123, 1125, 1127, 1128, 1130, 1131, 1133, 1134, 1136, 1137, 1139, 1140, 1142, 1143, 1145, 1146, 1148, 1150, 1151, 1153, 1154, 1156, 1157, 1159, 1160, 1162, 1163, 1165, 1166, 1168, 1169, 1171, 1173, 1174, 1176, 1177, 1179, 1180, 1182, 1183, 1185, 1186, 1188, 1189, 1191, 1192, 1194, 1196, 1197, 1199, 1200, 1202, 1203, 1205, 1206, 1208, 1209, 1211, 1212, 1214, 1215, 1217, 1219, 1220, 1222, 1223, 1225, 1226, 1228, 1229, 1231, 1232, 1234, 1235, 1237, 1238, 1240, 1242, 1243, 1245, 1246, 1248, 1249, 1251, 1252, 1254, 1255, 1257, 1258, 1260, 1261, 1263, 1265, 1266, 1268, 1269, 1271, 1272, 1274, 1275, 1277, 1278, 1280, 1281, 1283, 1284, 1286, 1288, 1289, 1291, 1292, 1294, 1295, 1297, 1298, 1300, 1301, 1303, 1304, 1306, 1307, 1309, 1311, 1312, 1314, 1315, 1317, 1318, 1320, 1321, 1323, }
{883, 892, 901, 910, 920, 929, 938, 947, 956, 966, 975, 984, 993, 1002, 1012, 1021, 1030, 1039, 1048, 1058, 1067, 1076, 1085, 1094, 1104, 1113, 1122, 1131, 1140, 1150, 1159, 1168, 1177, 1186, 1196, 1205, 1214, 1223, 1232, 1242, 1251, 1260, 1269, 1278, 1288, 1297, 1306, 1315, }
{1324, 1326, 1327, 1329, 1330, 1332, 1334, 1335, 1337, 1338, 1340, 1341, 1343, 1344, 1346, 1347, 1349, 1350, 1352, 1353, 1355, 1357, 1358, 1360, 1361, 1363, 1364, 1366, 1367, 1369, 1370, 1372, 1373, 1375, 1376, 1378, 1380, 1381, 1383, 1384, 1386, 1387, 1389, 1390, 1392, 1393, 1395, 1396, 1398, 1399, 1401, 1403, 1404, 1406, 1407, 1409, 1410, 1412, 1413, 1415, 1416, 1418, 1419, 1421, 1422, 1424, 1426, 1427, 1429, 1430, 1432, 1433, 1435, 1436, 1438, 1439, 1441, 1442, 1444, 1445, 1447, 1449, 1450, 1452, 1453, 1455, 1456, 1458, 1459, 1461, 1462, 1464, 1465, 1467, 1468, 1470, }
{1324, 1327, 1330, 1334, 1337, 1340, 1343, 1346, 1349, 1352, 1355, 1358, 1361, 1364, 1367, 1370, 1373, 1376, 1380, 1383, 1386, 1389, 1392, 1395, 1398, 1401, 1404, 1407, 1410, 1413, 1416, 1419, 1422, 1426, 1429, 1432, 1435, 1438, 1441, 1444, 1447, 1450, 1453, 1456, 1459, 1462, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 55
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 55
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 55
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,96}
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,96}
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,96}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,96}
slicedInpSpikeIds.shapeToString(): {48,96}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,96}
slicedInpSpikeIds.shapeToString(): {48,96}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,96}
slicedInpSpikeIds.shapeToString(): {48,96}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {488,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{726, 725, 18, }
start_tile: 1
num_tiles_per_layer[ilay]: 726
start_tile + num_tiles_per_layer[ilay]: 727
num_occupied_tiles: 489
batchsize: 48
sparse_size: 96
dLdx.shapeToString(): {489,48,96}
replicated_numGrads.shapeToString(): {489,48}
exchangeGroupSize: 24
numExchangeGroups: 20
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
lastExchangeGroupLarger
start_id: 456, end_id: 489
exchangeGroupSizeLast: 33
exchangeBatchSizeLast: 2
start_tile: 727
num_tiles_per_layer[ilay]: 725
start_tile + num_tiles_per_layer[ilay]: 1452
num_occupied_tiles: 488
batchsize: 48
sparse_size: 96
dLdx.shapeToString(): {488,48,96}
replicated_numGrads.shapeToString(): {488,48}
exchangeGroupSize: 24
numExchangeGroups: 20
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
lastExchangeGroupLarger
start_id: 456, end_id: 488
exchangeGroupSizeLast: 32
exchangeBatchSizeLast: 2
start_tile: 1452
num_tiles_per_layer[ilay]: 18
start_tile + num_tiles_per_layer[ilay]: 1470
num_occupied_tiles: 5
batchsize: 48
sparse_size: 96
dLdx.shapeToString(): {5,48,96}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {488,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 23:04:18.779025: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[978 978 976  10]
0 489.0
1 489.0
2 488.0
3 5.0
[TileMapping(start_tile=1, end_tile=490), TileMapping(start_tile=490, end_tile=979), TileMapping(start_tile=979, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=490), TileMapping(start_tile=490, end_tile=979), TileMapping(start_tile=979, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.31330417999518295
limit=0.31330417999518295
limit=0.31362502409359
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 978) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 978) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 976) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_96_96_96_2_48_1_490_979_1467_490_979_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 96), (100 4190734     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][3
                                                                 keras_multi_lif_layer_sparse[0][7
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 4,190,734
Trainable params: 4,181,908
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_96_96_96_2_48_1_490_979_1467_490_979_1467_1472
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_96_96_96_2_48_1_490_979_1467_490_979_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.686952/52 [==============================] - 1160s 22s/step - loss: 2.6869
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 1.980952/52 [==============================] - 0s 9ms/step - loss: 1.9809
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 2.053552/52 [==============================] - 0s 8ms/step - loss: 2.0535
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 1.956752/52 [==============================] - 0s 8ms/step - loss: 1.9567
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 1.741452/52 [==============================] - 0s 8ms/step - loss: 1.7414
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 1.760352/52 [==============================] - 0s 8ms/step - loss: 1.7603
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 1.761352/52 [==============================] - 0s 8ms/step - loss: 1.7613
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 1.752752/52 [==============================] - 0s 9ms/step - loss: 1.7527
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 1.702352/52 [==============================] - 0s 9ms/step - loss: 1.7023
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 1.755652/52 [==============================] - 0s 9ms/step - loss: 1.7556
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 1.803252/52 [==============================] - 0s 9ms/step - loss: 1.8032
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 1.742352/52 [==============================] - 0s 9ms/step - loss: 1.7423
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 1.672952/52 [==============================] - 1s 10ms/step - loss: 1.6729
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 1.727352/52 [==============================] - 0s 9ms/step - loss: 1.7273
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 1.711352/52 [==============================] - 0s 9ms/step - loss: 1.7113
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 1.735452/52 [==============================] - 0s 9ms/step - loss: 1.7354
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 1.661952/52 [==============================] - 0s 9ms/step - loss: 1.6619
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 1.812552/52 [==============================] - 0s 9ms/step - loss: 1.8125
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 1.913252/52 [==============================] - 0s 9ms/step - loss: 1.9132
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 1.867652/52 [==============================] - 0s 9ms/step - loss: 1.8676
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 1.851352/52 [==============================] - 0s 9ms/step - loss: 1.8513
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 1.878152/52 [==============================] - 0s 9ms/step - loss: 1.8781
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 1.876152/52 [==============================] - 0s 9ms/step - loss: 1.8761
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 1.863152/52 [==============================] - 0s 9ms/step - loss: 1.8631
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 1.844152/52 [==============================] - 0s 9ms/step - loss: 1.8441

Final time:  1170.9004673957825
2022-09-12 23:04:33.543260: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.1
DENSE_SIZES:  [2312, 734, 734, 732, 732, 10]
SPARSE_SIZES:  [32, 72, 72, 72, 72, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  4
SECOND_THRESHOLD:  0.9

60000
9984
[35762 57458 10678 ... 32092 23198 45479]
2022-09-12 23:04:42.066496: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 23:04:42.915327: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 11, 2, 1
Build_allocator: 16, 3, 1
Build_allocator: 12, 2, 2
Build_allocator: 17, 3, 2
Build_allocator: 13, 2, 3
Build_allocator: 18, 3, 3
Build_allocator: 14, 2, 4
Build_allocator: 19, 3, 4
Build_allocator: 5, 1, 0
Build_allocator: 6, 1, 1
Build_allocator: 7, 1, 2
Build_allocator: 8, 1, 3
Build_allocator: 9, 1, 4
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 4, 0, 4
Build_allocator: 20, 4, 0
Build_allocator: 21, 4, 1
Build_allocator: 22, 4, 2
Build_allocator: 23, 4, 3
Build_allocator: 24, 4, 4
Build_allocator: 25, 5, 0
Build_allocator: 26, 5, 1
Build_allocator: 27, 5, 2
Build_allocator: 28, 5, 3
Build_allocator: 29, 5, 4
Build_allocator: 10, 2, 0
Build_allocator: 15, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0x209f12e0
1: &init_state[ilay]0x209f12e8
2: &init_state[ilay]0x209f12f0
3: &init_state[ilay]0x209f12f8
4: &init_state[ilay]0x209f1300
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0
3: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
3: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 368, 735, 1101, 1467, }
end_tiles: {368, 735, 1101, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{2, 2, 2, 2, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 5, 6, 8, 10, 11, 13, 15, 17, 18, 20, 22, 23, 25, 27, 28, 30, 32, 34, 35, 37, 39, 40, 42, 44, 46, 47, 49, 51, 52, 54, 56, 57, 59, 61, 63, 64, 66, 68, 69, 71, 73, 74, 76, 78, 80, 81, 83, 85, 86, 88, 90, 92, 93, 95, 97, 98, 100, 102, 103, 105, 107, 109, 110, 112, 114, 115, 117, 119, 120, 122, 124, 126, 127, 129, 131, 132, 134, 136, 138, 139, 141, 143, 144, 146, 148, 149, 151, 153, 155, 156, 158, 160, 161, 163, 165, 166, 168, 170, 172, 173, 175, 177, 178, 180, 182, 184, 185, 187, 189, 190, 192, 194, 195, 197, 199, 201, 202, 204, 206, 207, 209, 211, 212, 214, 216, 218, 219, 221, 223, 224, 226, 228, 230, 231, 233, 235, 236, 238, 240, 241, 243, 245, 247, 248, 250, 252, 253, 255, 257, 258, 260, 262, 264, 265, 267, 269, 270, 272, 274, 276, 277, 279, 281, 282, 284, 286, 287, 289, 291, 293, 294, 296, 298, 299, 301, 303, 304, 306, 308, 310, 311, 313, 315, 316, 318, 320, 322, 323, 325, }
{0, 6, 13, 20, 27, 34, 40, 47, 54, 61, 68, 74, 81, 88, 95, 102, 109, 115, 122, 129, 136, 143, 149, 156, 163, 170, 177, 184, 190, 197, 204, 211, 218, 224, 231, 238, 245, 252, 258, 265, 272, 279, 286, 293, 299, 306, 313, 320, }
{327, 328, 330, 332, 333, 335, 337, 339, 340, 342, 344, 345, 347, 349, 350, 352, 354, 356, 357, 359, 361, 362, 364, 366, 368, 369, 371, 373, 374, 376, 378, 379, 381, 383, 385, 386, 388, 390, 391, 393, 395, 396, 398, 400, 402, 403, 405, 407, 408, 410, 412, 414, 415, 417, 419, 420, 422, 424, 425, 427, 429, 431, 432, 434, 436, 437, 439, 441, 442, 444, 446, 448, 449, 451, 453, 454, 456, 458, 460, 461, 463, 465, 466, 468, 470, 471, 473, 475, 477, 478, 480, 482, 483, 485, 487, 488, 490, 492, 494, 495, 497, 499, 500, 502, 504, 506, 507, 509, 511, 512, 514, 516, 517, 519, 521, 523, 524, 526, 528, 529, 531, 533, 534, 536, 538, 540, 541, 543, 545, 546, 548, 550, 552, 553, 555, 557, 558, 560, 562, 563, 565, 567, 569, 570, 572, 574, 575, 577, 579, 580, 582, 584, 586, 587, 589, 591, 592, 594, 596, 598, 599, 601, 603, 604, 606, 608, 609, 611, 613, 615, 616, 618, 620, 621, 623, 625, 626, 628, 630, 632, 633, 635, 637, 638, 640, 642, 644, 645, 647, 649, 650, 652, }
{327, 333, 340, 347, 354, 361, 368, 374, 381, 388, 395, 402, 408, 415, 422, 429, 436, 442, 449, 456, 463, 470, 477, 483, 490, 497, 504, 511, 517, 524, 531, 538, 545, 552, 558, 565, 572, 579, 586, 592, 599, 606, 613, 620, 626, 633, 640, 647, }
{654, 655, 657, 659, 661, 662, 664, 666, 667, 669, 671, 672, 674, 676, 678, 679, 681, 683, 684, 686, 688, 690, 691, 693, 695, 696, 698, 700, 701, 703, 705, 707, 708, 710, 712, 713, 715, 717, 718, 720, 722, 724, 725, 727, 729, 730, 732, 734, 736, 737, 739, 741, 742, 744, 746, 747, 749, 751, 753, 754, 756, 758, 759, 761, 763, 764, 766, 768, 770, 771, 773, 775, 776, 778, 780, 782, 783, 785, 787, 788, 790, 792, 793, 795, 797, 799, 800, 802, 804, 805, 807, 809, 810, 812, 814, 816, 817, 819, 821, 822, 824, 826, 828, 829, 831, 833, 834, 836, 838, 839, 841, 843, 845, 846, 848, 850, 851, 853, 855, 856, 858, 860, 862, 863, 865, 867, 868, 870, 872, 874, 875, 877, 879, 880, 882, 884, 885, 887, 889, 891, 892, 894, 896, 897, 899, 901, 902, 904, 906, 908, 909, 911, 913, 914, 916, 918, 920, 921, 923, 925, 926, 928, 930, 931, 933, 935, 937, 938, 940, 942, 943, 945, 947, 948, 950, 952, 954, 955, 957, 959, 960, 962, 964, 966, 967, 969, 971, 972, 974, 976, 977, 979, }
{654, 661, 667, 674, 681, 688, 695, 701, 708, 715, 722, 729, 736, 742, 749, 756, 763, 770, 776, 783, 790, 797, 804, 810, 817, 824, 831, 838, 845, 851, 858, 865, 872, 879, 885, 892, 899, 906, 913, 920, 926, 933, 940, 947, 954, 960, 967, 974, }
{981, 983, 984, 986, 988, 989, 991, 993, 994, 996, 998, 1000, 1001, 1003, 1005, 1006, 1008, 1010, 1012, 1013, 1015, 1017, 1018, 1020, 1022, 1023, 1025, 1027, 1029, 1030, 1032, 1034, 1035, 1037, 1039, 1040, 1042, 1044, 1046, 1047, 1049, 1051, 1052, 1054, 1056, 1058, 1059, 1061, 1063, 1064, 1066, 1068, 1069, 1071, 1073, 1075, 1076, 1078, 1080, 1081, 1083, 1085, 1086, 1088, 1090, 1092, 1093, 1095, 1097, 1098, 1100, 1102, 1104, 1105, 1107, 1109, 1110, 1112, 1114, 1115, 1117, 1119, 1121, 1122, 1124, 1126, 1127, 1129, 1131, 1132, 1134, 1136, 1138, 1139, 1141, 1143, 1144, 1146, 1148, 1150, 1151, 1153, 1155, 1156, 1158, 1160, 1161, 1163, 1165, 1167, 1168, 1170, 1172, 1173, 1175, 1177, 1178, 1180, 1182, 1184, 1185, 1187, 1189, 1190, 1192, 1194, 1196, 1197, 1199, 1201, 1202, 1204, 1206, 1207, 1209, 1211, 1213, 1214, 1216, 1218, 1219, 1221, 1223, 1224, 1226, 1228, 1230, 1231, 1233, 1235, 1236, 1238, 1240, 1242, 1243, 1245, 1247, 1248, 1250, 1252, 1253, 1255, 1257, 1259, 1260, 1262, 1264, 1265, 1267, 1269, 1270, 1272, 1274, 1276, 1277, 1279, 1281, 1282, 1284, 1286, 1288, 1289, 1291, 1293, 1294, 1296, 1298, 1299, 1301, 1303, 1305, 1306, }
{981, 988, 994, 1001, 1008, 1015, 1022, 1029, 1035, 1042, 1049, 1056, 1063, 1069, 1076, 1083, 1090, 1097, 1104, 1110, 1117, 1124, 1131, 1138, 1144, 1151, 1158, 1165, 1172, 1178, 1185, 1192, 1199, 1206, 1213, 1219, 1226, 1233, 1240, 1247, 1253, 1260, 1267, 1274, 1281, 1288, 1294, 1301, }
{1308, 1310, 1311, 1313, 1315, 1316, 1318, 1320, 1322, 1323, 1325, 1327, 1328, 1330, 1332, 1334, 1335, 1337, 1339, 1340, 1342, 1344, 1345, 1347, 1349, 1351, 1352, 1354, 1356, 1357, 1359, 1361, 1362, 1364, 1366, 1368, 1369, 1371, 1373, 1374, 1376, 1378, 1380, 1381, 1383, 1385, 1386, 1388, 1390, 1391, 1393, 1395, 1397, 1398, 1400, 1402, 1403, 1405, 1407, 1408, 1410, 1412, 1414, 1415, 1417, 1419, 1420, 1422, 1424, 1426, 1427, 1429, 1431, 1432, 1434, 1436, 1437, 1439, 1441, 1443, 1444, 1446, 1448, 1449, 1451, 1453, 1454, 1456, 1458, 1460, 1461, 1463, 1465, 1466, 1468, 1470, }
{1308, 1311, 1315, 1318, 1322, 1325, 1328, 1332, 1335, 1339, 1342, 1345, 1349, 1352, 1356, 1359, 1362, 1366, 1369, 1373, 1376, 1380, 1383, 1386, 1390, 1393, 1397, 1400, 1403, 1407, 1410, 1414, 1417, 1420, 1424, 1427, 1431, 1434, 1437, 1441, 1444, 1448, 1451, 1454, 1458, 1461, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 62
sparse_size_to_use: 62
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 62
sparse_size_to_use: 62
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 61
sparse_size_to_use: 61
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 61
sparse_size_to_use: 61
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,72}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,72}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,72}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,72}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 4
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,72}
slicedInpSpikeIds.shapeToString(): {48,72}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,72}
slicedInpSpikeIds.shapeToString(): {48,72}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,72}
slicedInpSpikeIds.shapeToString(): {48,72}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

3
out_spike_ids[ilay].shapeToString(): {100,48,72}
slicedInpSpikeIds.shapeToString(): {48,72}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,2}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{485, 484, 484, 15, }
start_tile: 1
num_tiles_per_layer[ilay]: 485
start_tile + num_tiles_per_layer[ilay]: 486
num_occupied_tiles: 367
batchsize: 48
sparse_size: 72
dLdx.shapeToString(): {367,48,72}
replicated_numGrads.shapeToString(): {367,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 367
exchangeGroupSizeLast: 31
exchangeBatchSizeLast: 2
start_tile: 486
num_tiles_per_layer[ilay]: 484
start_tile + num_tiles_per_layer[ilay]: 970
num_occupied_tiles: 366
batchsize: 48
sparse_size: 72
dLdx.shapeToString(): {366,48,72}
replicated_numGrads.shapeToString(): {366,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 366
exchangeGroupSizeLast: 30
exchangeBatchSizeLast: 2
start_tile: 970
num_tiles_per_layer[ilay]: 484
start_tile + num_tiles_per_layer[ilay]: 1454
num_occupied_tiles: 366
batchsize: 48
sparse_size: 72
dLdx.shapeToString(): {366,48,72}
replicated_numGrads.shapeToString(): {366,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 366
exchangeGroupSizeLast: 30
exchangeBatchSizeLast: 2
start_tile: 1454
num_tiles_per_layer[ilay]: 15
start_tile + num_tiles_per_layer[ilay]: 1469
num_occupied_tiles: 5
batchsize: 48
sparse_size: 72
dLdx.shapeToString(): {5,48,72}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,1}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 23:15:09.900992: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[734 734 732 732  10]
0 367.0
1 367.0
2 366.0
3 366.0
4 5.0
[TileMapping(start_tile=1, end_tile=368), TileMapping(start_tile=368, end_tile=735), TileMapping(start_tile=735, end_tile=1101), TileMapping(start_tile=1101, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=368), TileMapping(start_tile=368, end_tile=735), TileMapping(start_tile=735, end_tile=1101), TileMapping(start_tile=1101, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.3616492648073473
limit=0.3616492648073473
limit=0.3621429841700741
limit=0.3621429841700741
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 734) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 734) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 732) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 732) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_5:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_72_72_72_72_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 72), (100 3325022     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][4
                                                                 keras_multi_lif_layer_sparse[0][9
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 3,325,022
Trainable params: 3,316,196
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_72_72_72_72_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_72_72_72_72_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.831752/52 [==============================] - 629s 12s/step - loss: 2.8317
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.308752/52 [==============================] - 0s 7ms/step - loss: 2.3087
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 2.307752/52 [==============================] - 0s 7ms/step - loss: 2.3077
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 2.304852/52 [==============================] - 0s 7ms/step - loss: 2.3048
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 2.204752/52 [==============================] - 0s 7ms/step - loss: 2.2047
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 2.157552/52 [==============================] - 0s 7ms/step - loss: 2.1575
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 2.182952/52 [==============================] - 0s 7ms/step - loss: 2.1829
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 2.156852/52 [==============================] - 0s 7ms/step - loss: 2.1568
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 2.148352/52 [==============================] - 0s 7ms/step - loss: 2.1483
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 1.975852/52 [==============================] - 0s 7ms/step - loss: 1.9758
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 1.944452/52 [==============================] - 0s 7ms/step - loss: 1.9444
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 1.922952/52 [==============================] - 0s 7ms/step - loss: 1.9229
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 1.910152/52 [==============================] - 0s 8ms/step - loss: 1.9101
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 1.898152/52 [==============================] - 0s 8ms/step - loss: 1.8981
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 1.883152/52 [==============================] - 0s 8ms/step - loss: 1.8831
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 1.873552/52 [==============================] - 0s 8ms/step - loss: 1.8735
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 1.870152/52 [==============================] - 0s 8ms/step - loss: 1.8701
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 1.877852/52 [==============================] - 0s 8ms/step - loss: 1.8778
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 1.870152/52 [==============================] - 0s 8ms/step - loss: 1.8701
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 1.892452/52 [==============================] - 0s 8ms/step - loss: 1.8924
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 1.854952/52 [==============================] - 0s 8ms/step - loss: 1.8549
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 1.872852/52 [==============================] - 0s 8ms/step - loss: 1.8728
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 1.855252/52 [==============================] - 0s 9ms/step - loss: 1.8552
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 1.875052/52 [==============================] - 2s 33ms/step - loss: 1.8750
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 1.881552/52 [==============================] - 1s 12ms/step - loss: 1.8815

Final time:  640.0455069541931
2022-09-12 23:15:24.097659: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.1
DENSE_SIZES:  [2312, 588, 586, 586, 586, 586, 10]
SPARSE_SIZES:  [32, 58, 58, 58, 58, 58, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  5
SECOND_THRESHOLD:  0.9

60000
9984
[45489 42820 37420 ... 10345 41894 41598]
2022-09-12 23:15:32.952255: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 23:15:33.861956: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 13, 2, 1
Build_allocator: 19, 3, 1
Build_allocator: 14, 2, 2
Build_allocator: 20, 3, 2
Build_allocator: 15, 2, 3
Build_allocator: 21, 3, 3
Build_allocator: 16, 2, 4
Build_allocator: 22, 3, 4
Build_allocator: 17, 2, 5
Build_allocator: 23, 3, 5
Build_allocator: 6, 1, 0
Build_allocator: 7, 1, 1
Build_allocator: 8, 1, 2
Build_allocator: 9, 1, 3
Build_allocator: 10, 1, 4
Build_allocator: 11, 1, 5
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 4, 0, 4
Build_allocator: 5, 0, 5
Build_allocator: 24, 4, 0
Build_allocator: 25, 4, 1
Build_allocator: 26, 4, 2
Build_allocator: 27, 4, 3
Build_allocator: 28, 4, 4
Build_allocator: 29, 4, 5
Build_allocator: 30, 5, 0
Build_allocator: 31, 5, 1
Build_allocator: 32, 5, 2
Build_allocator: 33, 5, 3
Build_allocator: 34, 5, 4
Build_allocator: 35, 5, 5
Build_allocator: 12, 2, 0
Build_allocator: 18, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0xdb0b280
1: &init_state[ilay]0xdb0b288
2: &init_state[ilay]0xdb0b290
3: &init_state[ilay]0xdb0b298
4: &init_state[ilay]0xdb0b2a0
5: &init_state[ilay]0xdb0b2a8
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0
3: init_state[ilay]==init_state[ilay+1]: 0
4: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
3: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
4: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 295, 588, 881, 1174, 1467, }
end_tiles: {295, 588, 881, 1174, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{2, 2, 2, 2, 2, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 2, 4, 5, 6, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 25, 26, 27, 29, 30, 32, 33, 34, 36, 37, 39, 40, 41, 43, 44, 46, 47, 48, 50, 51, 52, 54, 55, 57, 58, 59, 61, 62, 64, 65, 66, 68, 69, 71, 72, 73, 75, 76, 78, 79, 80, 82, 83, 85, 86, 87, 89, 90, 92, 93, 94, 96, 97, 98, 100, 101, 103, 104, 105, 107, 108, 110, 111, 112, 114, 115, 117, 118, 119, 121, 122, 124, 125, 126, 128, 129, 131, 132, 133, 135, 136, 138, 139, 140, 142, 143, 144, 146, 147, 149, 150, 151, 153, 154, 156, 157, 158, 160, 161, 163, 164, 165, 167, 168, 170, 171, 172, 174, 175, 177, 178, 179, 181, 182, 184, 185, 186, 188, 189, 190, 192, 193, 195, 196, 197, 199, 200, 202, 203, 204, 206, 207, 209, 210, 211, 213, 214, 216, 217, 218, 220, 221, 223, 224, 225, 227, 228, 230, 231, 232, 234, 235, 236, 238, 239, 241, 242, 243, 245, 246, 248, 249, 250, 252, 253, 255, 256, 257, 259, 260, 262, 263, 264, 266, }
{0, 5, 11, 16, 22, 27, 33, 39, 44, 50, 55, 61, 66, 72, 78, 83, 89, 94, 100, 105, 111, 117, 122, 128, 133, 139, 144, 150, 156, 161, 167, 172, 178, 184, 189, 195, 200, 206, 211, 217, 223, 228, 234, 239, 245, 250, 256, 262, }
{267, 269, 270, 271, 273, 274, 276, 277, 278, 280, 281, 282, 284, 285, 287, 288, 289, 291, 292, 294, 295, 296, 298, 299, 301, 302, 303, 305, 306, 308, 309, 310, 312, 313, 315, 316, 317, 319, 320, 322, 323, 324, 326, 327, 328, 330, 331, 333, 334, 335, 337, 338, 340, 341, 342, 344, 345, 347, 348, 349, 351, 352, 354, 355, 356, 358, 359, 361, 362, 363, 365, 366, 368, 369, 370, 372, 373, 374, 376, 377, 379, 380, 381, 383, 384, 386, 387, 388, 390, 391, 393, 394, 395, 397, 398, 400, 401, 402, 404, 405, 407, 408, 409, 411, 412, 414, 415, 416, 418, 419, 420, 422, 423, 425, 426, 427, 429, 430, 432, 433, 434, 436, 437, 439, 440, 441, 443, 444, 446, 447, 448, 450, 451, 453, 454, 455, 457, 458, 460, 461, 462, 464, 465, 466, 468, 469, 471, 472, 473, 475, 476, 478, 479, 480, 482, 483, 485, 486, 487, 489, 490, 492, 493, 494, 496, 497, 499, 500, 501, 503, 504, 506, 507, 508, 510, 511, 512, 514, 515, 517, 518, 519, 521, 522, 524, 525, 526, 528, 529, 531, 532, 533, }
{267, 273, 278, 284, 289, 295, 301, 306, 312, 317, 323, 328, 334, 340, 345, 351, 356, 362, 368, 373, 379, 384, 390, 395, 401, 407, 412, 418, 423, 429, 434, 440, 446, 451, 457, 462, 468, 473, 479, 485, 490, 496, 501, 507, 512, 518, 524, 529, }
{535, 536, 538, 539, 540, 542, 543, 545, 546, 547, 549, 550, 552, 553, 554, 556, 557, 558, 560, 561, 563, 564, 565, 567, 568, 570, 571, 572, 574, 575, 577, 578, 579, 581, 582, 584, 585, 586, 588, 589, 591, 592, 593, 595, 596, 598, 599, 600, 602, 603, 604, 606, 607, 609, 610, 611, 613, 614, 616, 617, 618, 620, 621, 623, 624, 625, 627, 628, 630, 631, 632, 634, 635, 637, 638, 639, 641, 642, 644, 645, 646, 648, 649, 650, 652, 653, 655, 656, 657, 659, 660, 662, 663, 664, 666, 667, 669, 670, 671, 673, 674, 676, 677, 678, 680, 681, 683, 684, 685, 687, 688, 690, 691, 692, 694, 695, 696, 698, 699, 701, 702, 703, 705, 706, 708, 709, 710, 712, 713, 715, 716, 717, 719, 720, 722, 723, 724, 726, 727, 729, 730, 731, 733, 734, 736, 737, 738, 740, 741, 742, 744, 745, 747, 748, 749, 751, 752, 754, 755, 756, 758, 759, 761, 762, 763, 765, 766, 768, 769, 770, 772, 773, 775, 776, 777, 779, 780, 782, 783, 784, 786, 787, 788, 790, 791, 793, 794, 795, 797, 798, 800, 801, }
{535, 540, 546, 552, 557, 563, 568, 574, 579, 585, 591, 596, 602, 607, 613, 618, 624, 630, 635, 641, 646, 652, 657, 663, 669, 674, 680, 685, 691, 696, 702, 708, 713, 719, 724, 730, 736, 741, 747, 752, 758, 763, 769, 775, 780, 786, 791, 797, }
{802, 804, 805, 807, 808, 809, 811, 812, 814, 815, 816, 818, 819, 821, 822, 823, 825, 826, 828, 829, 830, 832, 833, 834, 836, 837, 839, 840, 841, 843, 844, 846, 847, 848, 850, 851, 853, 854, 855, 857, 858, 860, 861, 862, 864, 865, 867, 868, 869, 871, 872, 874, 875, 876, 878, 879, 880, 882, 883, 885, 886, 887, 889, 890, 892, 893, 894, 896, 897, 899, 900, 901, 903, 904, 906, 907, 908, 910, 911, 913, 914, 915, 917, 918, 920, 921, 922, 924, 925, 926, 928, 929, 931, 932, 933, 935, 936, 938, 939, 940, 942, 943, 945, 946, 947, 949, 950, 952, 953, 954, 956, 957, 959, 960, 961, 963, 964, 966, 967, 968, 970, 971, 972, 974, 975, 977, 978, 979, 981, 982, 984, 985, 986, 988, 989, 991, 992, 993, 995, 996, 998, 999, 1000, 1002, 1003, 1005, 1006, 1007, 1009, 1010, 1012, 1013, 1014, 1016, 1017, 1018, 1020, 1021, 1023, 1024, 1025, 1027, 1028, 1030, 1031, 1032, 1034, 1035, 1037, 1038, 1039, 1041, 1042, 1044, 1045, 1046, 1048, 1049, 1051, 1052, 1053, 1055, 1056, 1058, 1059, 1060, 1062, 1063, 1064, 1066, 1067, 1069, }
{802, 808, 814, 819, 825, 830, 836, 841, 847, 853, 858, 864, 869, 875, 880, 886, 892, 897, 903, 908, 914, 920, 925, 931, 936, 942, 947, 953, 959, 964, 970, 975, 981, 986, 992, 998, 1003, 1009, 1014, 1020, 1025, 1031, 1037, 1042, 1048, 1053, 1059, 1064, }
{1070, 1071, 1073, 1074, 1076, 1077, 1078, 1080, 1081, 1083, 1084, 1085, 1087, 1088, 1090, 1091, 1092, 1094, 1095, 1097, 1098, 1099, 1101, 1102, 1104, 1105, 1106, 1108, 1109, 1110, 1112, 1113, 1115, 1116, 1117, 1119, 1120, 1122, 1123, 1124, 1126, 1127, 1129, 1130, 1131, 1133, 1134, 1136, 1137, 1138, 1140, 1141, 1143, 1144, 1145, 1147, 1148, 1150, 1151, 1152, 1154, 1155, 1156, 1158, 1159, 1161, 1162, 1163, 1165, 1166, 1168, 1169, 1170, 1172, 1173, 1175, 1176, 1177, 1179, 1180, 1182, 1183, 1184, 1186, 1187, 1189, 1190, 1191, 1193, 1194, 1196, 1197, 1198, 1200, 1201, 1202, 1204, 1205, 1207, 1208, 1209, 1211, 1212, 1214, 1215, 1216, 1218, 1219, 1221, 1222, 1223, 1225, 1226, 1228, 1229, 1230, 1232, 1233, 1235, 1236, 1237, 1239, 1240, 1242, 1243, 1244, 1246, 1247, 1248, 1250, 1251, 1253, 1254, 1255, 1257, 1258, 1260, 1261, 1262, 1264, 1265, 1267, 1268, 1269, 1271, 1272, 1274, 1275, 1276, 1278, 1279, 1281, 1282, 1283, 1285, 1286, 1288, 1289, 1290, 1292, 1293, 1294, 1296, 1297, 1299, 1300, 1301, 1303, 1304, 1306, 1307, 1308, 1310, 1311, 1313, 1314, 1315, 1317, 1318, 1320, 1321, 1322, 1324, 1325, 1327, 1328, 1329, 1331, 1332, 1334, 1335, 1336, }
{1070, 1076, 1081, 1087, 1092, 1098, 1104, 1109, 1115, 1120, 1126, 1131, 1137, 1143, 1148, 1154, 1159, 1165, 1170, 1176, 1182, 1187, 1193, 1198, 1204, 1209, 1215, 1221, 1226, 1232, 1237, 1243, 1248, 1254, 1260, 1265, 1271, 1276, 1282, 1288, 1293, 1299, 1304, 1310, 1315, 1321, 1327, 1332, }
{1338, 1339, 1340, 1342, 1343, 1345, 1346, 1347, 1349, 1350, 1352, 1353, 1354, 1356, 1357, 1359, 1360, 1361, 1363, 1364, 1366, 1367, 1368, 1370, 1371, 1373, 1374, 1375, 1377, 1378, 1380, 1381, 1382, 1384, 1385, 1386, 1388, 1389, 1391, 1392, 1393, 1395, 1396, 1398, 1399, 1400, 1402, 1403, 1405, 1406, 1407, 1409, 1410, 1412, 1413, 1414, 1416, 1417, 1419, 1420, 1421, 1423, 1424, 1426, 1427, 1428, 1430, 1431, 1432, 1434, 1435, 1437, 1438, 1439, 1441, 1442, 1444, 1445, 1446, 1448, 1449, 1451, 1452, 1453, 1455, 1456, 1458, 1459, 1460, 1462, 1463, 1465, 1466, 1467, 1469, 1470, }
{1338, 1340, 1343, 1346, 1349, 1352, 1354, 1357, 1360, 1363, 1366, 1368, 1371, 1374, 1377, 1380, 1382, 1385, 1388, 1391, 1393, 1396, 1399, 1402, 1405, 1407, 1410, 1413, 1416, 1419, 1421, 1424, 1427, 1430, 1432, 1435, 1438, 1441, 1444, 1446, 1449, 1452, 1455, 1458, 1460, 1463, 1466, 1469, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 49
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 49
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 49
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 49
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 49
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,58}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,58}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,58}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,58}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,58}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 4
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 5
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,58}
slicedInpSpikeIds.shapeToString(): {48,58}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,58}
slicedInpSpikeIds.shapeToString(): {48,58}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,58}
slicedInpSpikeIds.shapeToString(): {48,58}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

3
out_spike_ids[ilay].shapeToString(): {100,48,58}
slicedInpSpikeIds.shapeToString(): {48,58}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

4
out_spike_ids[ilay].shapeToString(): {100,48,58}
slicedInpSpikeIds.shapeToString(): {48,58}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {294,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 5
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{364, 364, 364, 364, 14, }
start_tile: 1
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 365
num_occupied_tiles: 293
batchsize: 48
sparse_size: 58
dLdx.shapeToString(): {293,48,58}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 365
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 729
num_occupied_tiles: 293
batchsize: 48
sparse_size: 58
dLdx.shapeToString(): {293,48,58}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 729
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 1093
num_occupied_tiles: 293
batchsize: 48
sparse_size: 58
dLdx.shapeToString(): {293,48,58}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 1093
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 1457
num_occupied_tiles: 293
batchsize: 48
sparse_size: 58
dLdx.shapeToString(): {293,48,58}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 1457
num_tiles_per_layer[ilay]: 14
start_tile + num_tiles_per_layer[ilay]: 1471
num_occupied_tiles: 5
batchsize: 48
sparse_size: 58
dLdx.shapeToString(): {5,48,58}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {294,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 5
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 23:25:37.972247: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[588 586 586 586 586  10]
0 294.0
1 293.0
2 293.0
3 293.0
4 293.0
5 5.0
[TileMapping(start_tile=1, end_tile=295), TileMapping(start_tile=295, end_tile=588), TileMapping(start_tile=588, end_tile=881), TileMapping(start_tile=881, end_tile=1174), TileMapping(start_tile=1174, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=295), TileMapping(start_tile=295, end_tile=588), TileMapping(start_tile=588, end_tile=881), TileMapping(start_tile=881, end_tile=1174), TileMapping(start_tile=1174, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.40406101782088427
limit=0.4047499544129309
limit=0.4047499544129309
limit=0.4047499544129309
limit=0.4047499544129309
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 588) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 586) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 586) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 586) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_5:0' shape=(2, 586) dtype=float32, numpy=
array([[1. , 1. , 1. , ..., 1. , 1. , 1. ],
       [0.9, 0.9, 0.9, ..., 0.9, 0.9, 0.9]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_6:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_58_58_58_58_58_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 58), (100 2748898     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][5
                                                                 keras_multi_lif_layer_sparse[0][1
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 2,748,898
Trainable params: 2,740,072
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_58_58_58_58_58_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_58_58_58_58_58_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
52/52 [==============================] - ETA: 0s - loss: 3.088852/52 [==============================] - 606s 12s/step - loss: 3.0888
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.320652/52 [==============================] - 0s 7ms/step - loss: 2.3206
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 2.309552/52 [==============================] - 0s 6ms/step - loss: 2.3095
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 2.229552/52 [==============================] - 0s 6ms/step - loss: 2.2295
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 2.178852/52 [==============================] - 0s 7ms/step - loss: 2.1788
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 2.141052/52 [==============================] - 0s 7ms/step - loss: 2.1410
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 2.129452/52 [==============================] - 0s 7ms/step - loss: 2.1294
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 2.107752/52 [==============================] - 0s 7ms/step - loss: 2.1077
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 2.136452/52 [==============================] - 0s 7ms/step - loss: 2.1364
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 2.103952/52 [==============================] - 0s 7ms/step - loss: 2.1039
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 2.120852/52 [==============================] - 0s 7ms/step - loss: 2.1208
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 2.104452/52 [==============================] - 0s 7ms/step - loss: 2.1044
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 2.095652/52 [==============================] - 0s 7ms/step - loss: 2.0956
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 2.094852/52 [==============================] - 0s 7ms/step - loss: 2.0948
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 2.097452/52 [==============================] - 0s 7ms/step - loss: 2.0974
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 2.083552/52 [==============================] - 0s 7ms/step - loss: 2.0835
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 2.085852/52 [==============================] - 0s 8ms/step - loss: 2.0858
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 2.064952/52 [==============================] - 0s 7ms/step - loss: 2.0649
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 2.089152/52 [==============================] - 0s 7ms/step - loss: 2.0891
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 2.085952/52 [==============================] - 0s 7ms/step - loss: 2.0859
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 2.082752/52 [==============================] - 0s 7ms/step - loss: 2.0827
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 2.061352/52 [==============================] - 0s 7ms/step - loss: 2.0613
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 2.085852/52 [==============================] - 0s 7ms/step - loss: 2.0858
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 2.083152/52 [==============================] - 0s 7ms/step - loss: 2.0831
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 2.076052/52 [==============================] - 0s 7ms/step - loss: 2.0760

Final time:  614.8699142932892
2022-09-12 23:25:49.787894: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.1
DENSE_SIZES:  [2312, 1466, 1466, 10]
SPARSE_SIZES:  [32, 146, 146, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  2
SECOND_THRESHOLD:  0.95

60000
9984
[59835 53817 23322 ... 16827 15476 55932]
2022-09-12 23:25:58.501770: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 23:25:59.347782: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 7, 2, 1
Build_allocator: 10, 3, 1
Build_allocator: 8, 2, 2
Build_allocator: 11, 3, 2
Build_allocator: 3, 1, 0
Build_allocator: 4, 1, 1
Build_allocator: 5, 1, 2
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 12, 4, 0
Build_allocator: 13, 4, 1
Build_allocator: 14, 4, 2
Build_allocator: 15, 5, 0
Build_allocator: 16, 5, 1
Build_allocator: 17, 5, 2
Build_allocator: 6, 2, 0
Build_allocator: 9, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0xb780c10
1: &init_state[ilay]0xb780c18
2: &init_state[ilay]0xb780c20
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 734, 1467, }
end_tiles: {734, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{4, 4, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 5, 6, 8, 10, 11, 13, 15, 17, 18, 20, 22, 23, 25, 27, 28, 30, 32, 34, 35, 37, 39, 40, 42, 44, 46, 47, 49, 51, 52, 54, 56, 57, 59, 61, 63, 64, 66, 68, 69, 71, 73, 74, 76, 78, 80, 81, 83, 85, 86, 88, 90, 92, 93, 95, 97, 98, 100, 102, 103, 105, 107, 109, 110, 112, 114, 115, 117, 119, 120, 122, 124, 126, 127, 129, 131, 132, 134, 136, 138, 139, 141, 143, 144, 146, 148, 149, 151, 153, 155, 156, 158, 160, 161, 163, 165, 166, 168, 170, 172, 173, 175, 177, 178, 180, 182, 184, 185, 187, 189, 190, 192, 194, 195, 197, 199, 201, 202, 204, 206, 207, 209, 211, 212, 214, 216, 218, 219, 221, 223, 224, 226, 228, 230, 231, 233, 235, 236, 238, 240, 241, 243, 245, 247, 248, 250, 252, 253, 255, 257, 258, 260, 262, 264, 265, 267, 269, 270, 272, 274, 276, 277, 279, 281, 282, 284, 286, 287, 289, 291, 293, 294, 296, 298, 299, 301, 303, 304, 306, 308, 310, 311, 313, 315, 316, 318, 320, 322, 323, 325, 327, 328, 330, 332, 333, 335, 337, 339, 340, 342, 344, 345, 347, 349, 350, 352, 354, 356, 357, 359, 361, 362, 364, 366, 368, 369, 371, 373, 374, 376, 378, 379, 381, 383, 385, 386, 388, 390, 391, 393, 395, 396, 398, 400, 402, 403, 405, 407, 408, 410, 412, 414, 415, 417, 419, 420, 422, 424, 425, 427, 429, 431, 432, 434, 436, 437, 439, 441, 442, 444, 446, 448, 449, 451, 453, 454, 456, 458, 460, 461, 463, 465, 466, 468, 470, 471, 473, 475, 477, 478, 480, 482, 483, 485, 487, 488, 490, 492, 494, 495, 497, 499, 500, 502, 504, 506, 507, 509, 511, 512, 514, 516, 517, 519, 521, 523, 524, 526, 528, 529, 531, 533, 534, 536, 538, 540, 541, 543, 545, 546, 548, 550, 552, 553, 555, 557, 558, 560, 562, 563, 565, 567, 569, 570, 572, 574, 575, 577, 579, 580, 582, 584, 586, 587, 589, 591, 592, 594, 596, 598, 599, 601, 603, 604, 606, 608, 609, 611, 613, 615, 616, 618, 620, 621, 623, 625, 626, 628, 630, 632, 633, 635, 637, 638, 640, 642, 644, 645, 647, 649, 650, 652, }
{0, 13, 27, 40, 54, 68, 81, 95, 109, 122, 136, 149, 163, 177, 190, 204, 218, 231, 245, 258, 272, 286, 299, 313, 327, 340, 354, 368, 381, 395, 408, 422, 436, 449, 463, 477, 490, 504, 517, 531, 545, 558, 572, 586, 599, 613, 626, 640, }
{654, 655, 657, 659, 661, 662, 664, 666, 667, 669, 671, 672, 674, 676, 678, 679, 681, 683, 684, 686, 688, 690, 691, 693, 695, 696, 698, 700, 701, 703, 705, 707, 708, 710, 712, 713, 715, 717, 718, 720, 722, 724, 725, 727, 729, 730, 732, 734, 736, 737, 739, 741, 742, 744, 746, 747, 749, 751, 753, 754, 756, 758, 759, 761, 763, 764, 766, 768, 770, 771, 773, 775, 776, 778, 780, 782, 783, 785, 787, 788, 790, 792, 793, 795, 797, 799, 800, 802, 804, 805, 807, 809, 810, 812, 814, 816, 817, 819, 821, 822, 824, 826, 828, 829, 831, 833, 834, 836, 838, 839, 841, 843, 845, 846, 848, 850, 851, 853, 855, 856, 858, 860, 862, 863, 865, 867, 868, 870, 872, 874, 875, 877, 879, 880, 882, 884, 885, 887, 889, 891, 892, 894, 896, 897, 899, 901, 902, 904, 906, 908, 909, 911, 913, 914, 916, 918, 920, 921, 923, 925, 926, 928, 930, 931, 933, 935, 937, 938, 940, 942, 943, 945, 947, 948, 950, 952, 954, 955, 957, 959, 960, 962, 964, 966, 967, 969, 971, 972, 974, 976, 977, 979, 981, 983, 984, 986, 988, 989, 991, 993, 994, 996, 998, 1000, 1001, 1003, 1005, 1006, 1008, 1010, 1012, 1013, 1015, 1017, 1018, 1020, 1022, 1023, 1025, 1027, 1029, 1030, 1032, 1034, 1035, 1037, 1039, 1040, 1042, 1044, 1046, 1047, 1049, 1051, 1052, 1054, 1056, 1058, 1059, 1061, 1063, 1064, 1066, 1068, 1069, 1071, 1073, 1075, 1076, 1078, 1080, 1081, 1083, 1085, 1086, 1088, 1090, 1092, 1093, 1095, 1097, 1098, 1100, 1102, 1104, 1105, 1107, 1109, 1110, 1112, 1114, 1115, 1117, 1119, 1121, 1122, 1124, 1126, 1127, 1129, 1131, 1132, 1134, 1136, 1138, 1139, 1141, 1143, 1144, 1146, 1148, 1150, 1151, 1153, 1155, 1156, 1158, 1160, 1161, 1163, 1165, 1167, 1168, 1170, 1172, 1173, 1175, 1177, 1178, 1180, 1182, 1184, 1185, 1187, 1189, 1190, 1192, 1194, 1196, 1197, 1199, 1201, 1202, 1204, 1206, 1207, 1209, 1211, 1213, 1214, 1216, 1218, 1219, 1221, 1223, 1224, 1226, 1228, 1230, 1231, 1233, 1235, 1236, 1238, 1240, 1242, 1243, 1245, 1247, 1248, 1250, 1252, 1253, 1255, 1257, 1259, 1260, 1262, 1264, 1265, 1267, 1269, 1270, 1272, 1274, 1276, 1277, 1279, 1281, 1282, 1284, 1286, 1288, 1289, 1291, 1293, 1294, 1296, 1298, 1299, 1301, 1303, 1305, 1306, }
{654, 667, 681, 695, 708, 722, 736, 749, 763, 776, 790, 804, 817, 831, 845, 858, 872, 885, 899, 913, 926, 940, 954, 967, 981, 994, 1008, 1022, 1035, 1049, 1063, 1076, 1090, 1104, 1117, 1131, 1144, 1158, 1172, 1185, 1199, 1213, 1226, 1240, 1253, 1267, 1281, 1294, }
{1308, 1310, 1311, 1313, 1315, 1316, 1318, 1320, 1322, 1323, 1325, 1327, 1328, 1330, 1332, 1334, 1335, 1337, 1339, 1340, 1342, 1344, 1345, 1347, 1349, 1351, 1352, 1354, 1356, 1357, 1359, 1361, 1362, 1364, 1366, 1368, 1369, 1371, 1373, 1374, 1376, 1378, 1380, 1381, 1383, 1385, 1386, 1388, 1390, 1391, 1393, 1395, 1397, 1398, 1400, 1402, 1403, 1405, 1407, 1408, 1410, 1412, 1414, 1415, 1417, 1419, 1420, 1422, 1424, 1426, 1427, 1429, 1431, 1432, 1434, 1436, 1437, 1439, 1441, 1443, 1444, 1446, 1448, 1449, 1451, 1453, 1454, 1456, 1458, 1460, 1461, 1463, 1465, 1466, 1468, 1470, }
{1308, 1311, 1315, 1318, 1322, 1325, 1328, 1332, 1335, 1339, 1342, 1345, 1349, 1352, 1356, 1359, 1362, 1366, 1369, 1373, 1376, 1380, 1383, 1386, 1390, 1393, 1397, 1400, 1403, 1407, 1410, 1414, 1417, 1420, 1424, 1427, 1431, 1434, 1437, 1441, 1444, 1448, 1451, 1454, 1458, 1461, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 4
num_workers: 24
max_num_states_per_worker: 62
sparse_size_to_use: 62
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 4
num_workers: 24
max_num_states_per_worker: 62
sparse_size_to_use: 62
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,4,146}
 repeated_sparse_spike_ids.shapeToString(): {48,2,4,146}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,146}
slicedInpSpikeIds.shapeToString(): {48,146}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,146}
slicedInpSpikeIds.shapeToString(): {48,146}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{1444, 26, }
start_tile: 1
num_tiles_per_layer[ilay]: 1444
start_tile + num_tiles_per_layer[ilay]: 1445
num_occupied_tiles: 733
batchsize: 48
sparse_size: 146
dLdx.shapeToString(): {733,48,146}
replicated_numGrads.shapeToString(): {733,48}
exchangeGroupSize: 24
numExchangeGroups: 30
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
start_id: 456, end_id: 480
start_id: 480, end_id: 504
start_id: 504, end_id: 528
start_id: 528, end_id: 552
start_id: 552, end_id: 576
start_id: 576, end_id: 600
start_id: 600, end_id: 624
start_id: 624, end_id: 648
start_id: 648, end_id: 672
start_id: 672, end_id: 696
lastExchangeGroupLarger
start_id: 696, end_id: 733
exchangeGroupSizeLast: 37
exchangeBatchSizeLast: 2
start_tile: 1445
num_tiles_per_layer[ilay]: 26
start_tile + num_tiles_per_layer[ilay]: 1471
num_occupied_tiles: 5
batchsize: 48
sparse_size: 146
dLdx.shapeToString(): {5,48,146}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-12 23:42:01.640378: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[1466 1466   10]
0 733.0
1 733.0
2 5.0
[TileMapping(start_tile=1, end_tile=734), TileMapping(start_tile=734, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=734), TileMapping(start_tile=734, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.2558990251065398
limit=0.2558990251065398
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 1466) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 1466) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_146_146_2_48_1_734_1467_734_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 146), (10 5562034     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][2
                                                                 keras_multi_lif_layer_sparse[0][5
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 5,562,034
Trainable params: 5,553,208
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_146_146_2_48_1_734_1467_734_1467_1472
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_146_146_2_48_1_734_1467_734_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.551752/52 [==============================] - 965s 19s/step - loss: 2.5517
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.287852/52 [==============================] - 1s 11ms/step - loss: 2.2878
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 2.168652/52 [==============================] - 1s 10ms/step - loss: 2.1686
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 2.082552/52 [==============================] - 1s 10ms/step - loss: 2.0825
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 2.112552/52 [==============================] - 1s 10ms/step - loss: 2.1125
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 2.086052/52 [==============================] - 1s 10ms/step - loss: 2.0860
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 2.070852/52 [==============================] - 1s 10ms/step - loss: 2.0708
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 2.069952/52 [==============================] - 1s 10ms/step - loss: 2.0699
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 2.099052/52 [==============================] - 1s 10ms/step - loss: 2.0990
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 2.046552/52 [==============================] - 1s 10ms/step - loss: 2.0465
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 2.070752/52 [==============================] - 1s 10ms/step - loss: 2.0707
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 2.064152/52 [==============================] - 1s 11ms/step - loss: 2.0641
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 2.067352/52 [==============================] - 1s 11ms/step - loss: 2.0673
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 2.044752/52 [==============================] - 1s 11ms/step - loss: 2.0447
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 2.065952/52 [==============================] - 1s 11ms/step - loss: 2.0659
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 2.061552/52 [==============================] - 1s 11ms/step - loss: 2.0615
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 2.074652/52 [==============================] - 1s 11ms/step - loss: 2.0746
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 2.047852/52 [==============================] - 1s 11ms/step - loss: 2.0478
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 2.067552/52 [==============================] - 1s 11ms/step - loss: 2.0675
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 2.064552/52 [==============================] - 1s 11ms/step - loss: 2.0645
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 2.066052/52 [==============================] - 1s 11ms/step - loss: 2.0660
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 2.053152/52 [==============================] - 1s 11ms/step - loss: 2.0531
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 2.036952/52 [==============================] - 1s 11ms/step - loss: 2.0369
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 2.063252/52 [==============================] - 1s 11ms/step - loss: 2.0632
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 2.059152/52 [==============================] - 1s 11ms/step - loss: 2.0591

Final time:  978.1266174316406
2022-09-12 23:42:18.754398: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.1
DENSE_SIZES:  [2312, 978, 978, 976, 10]
SPARSE_SIZES:  [32, 96, 96, 96, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  3
SECOND_THRESHOLD:  0.95

60000
9984
[41207 41797 21201 ... 35388  3004  2957]
2022-09-12 23:42:27.639632: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-12 23:42:28.449549: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 9, 2, 1
Build_allocator: 13, 3, 1
Build_allocator: 10, 2, 2
Build_allocator: 14, 3, 2
Build_allocator: 11, 2, 3
Build_allocator: 15, 3, 3
Build_allocator: 4, 1, 0
Build_allocator: 5, 1, 1
Build_allocator: 6, 1, 2
Build_allocator: 7, 1, 3
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 16, 4, 0
Build_allocator: 17, 4, 1
Build_allocator: 18, 4, 2
Build_allocator: 19, 4, 3
Build_allocator: 20, 5, 0
Build_allocator: 21, 5, 1
Build_allocator: 22, 5, 2
Build_allocator: 23, 5, 3
Build_allocator: 8, 2, 0
Build_allocator: 12, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0x2d334070
1: &init_state[ilay]0x2d334078
2: &init_state[ilay]0x2d334080
3: &init_state[ilay]0x2d334088
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 490, 979, 1467, }
end_tiles: {490, 979, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{3, 3, 3, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 4, 6, 7, 9, 10, 12, 13, 15, 16, 18, 19, 21, 23, 24, 26, 27, 29, 30, 32, 33, 35, 36, 38, 39, 41, 42, 44, 46, 47, 49, 50, 52, 53, 55, 56, 58, 59, 61, 62, 64, 65, 67, 69, 70, 72, 73, 75, 76, 78, 79, 81, 82, 84, 85, 87, 88, 90, 92, 93, 95, 96, 98, 99, 101, 102, 104, 105, 107, 108, 110, 111, 113, 115, 116, 118, 119, 121, 122, 124, 125, 127, 128, 130, 131, 133, 134, 136, 138, 139, 141, 142, 144, 145, 147, 148, 150, 151, 153, 154, 156, 157, 159, 161, 162, 164, 165, 167, 168, 170, 171, 173, 174, 176, 177, 179, 180, 182, 184, 185, 187, 188, 190, 191, 193, 194, 196, 197, 199, 200, 202, 203, 205, 207, 208, 210, 211, 213, 214, 216, 217, 219, 220, 222, 223, 225, 226, 228, 230, 231, 233, 234, 236, 237, 239, 240, 242, 243, 245, 246, 248, 249, 251, 253, 254, 256, 257, 259, 260, 262, 263, 265, 266, 268, 269, 271, 272, 274, 276, 277, 279, 280, 282, 283, 285, 286, 288, 289, 291, 292, 294, 295, 297, 299, 300, 302, 303, 305, 306, 308, 309, 311, 312, 314, 315, 317, 318, 320, 322, 323, 325, 326, 328, 329, 331, 332, 334, 335, 337, 338, 340, 341, 343, 345, 346, 348, 349, 351, 352, 354, 355, 357, 358, 360, 361, 363, 364, 366, 368, 369, 371, 372, 374, 375, 377, 378, 380, 381, 383, 384, 386, 387, 389, 391, 392, 394, 395, 397, 398, 400, 401, 403, 404, 406, 407, 409, 410, 412, 414, 415, 417, 418, 420, 421, 423, 424, 426, 427, 429, 430, 432, 433, 435, 437, 438, 440, }
{0, 9, 18, 27, 36, 46, 55, 64, 73, 82, 92, 101, 110, 119, 128, 138, 147, 156, 165, 174, 184, 193, 202, 211, 220, 230, 239, 248, 257, 266, 276, 285, 294, 303, 312, 322, 331, 340, 349, 358, 368, 377, 386, 395, 404, 414, 423, 432, }
{441, 443, 444, 446, 447, 449, 450, 452, 453, 455, 456, 458, 460, 461, 463, 464, 466, 467, 469, 470, 472, 473, 475, 476, 478, 479, 481, 483, 484, 486, 487, 489, 490, 492, 493, 495, 496, 498, 499, 501, 502, 504, 506, 507, 509, 510, 512, 513, 515, 516, 518, 519, 521, 522, 524, 525, 527, 529, 530, 532, 533, 535, 536, 538, 539, 541, 542, 544, 545, 547, 548, 550, 552, 553, 555, 556, 558, 559, 561, 562, 564, 565, 567, 568, 570, 571, 573, 575, 576, 578, 579, 581, 582, 584, 585, 587, 588, 590, 591, 593, 594, 596, 598, 599, 601, 602, 604, 605, 607, 608, 610, 611, 613, 614, 616, 617, 619, 621, 622, 624, 625, 627, 628, 630, 631, 633, 634, 636, 637, 639, 640, 642, 644, 645, 647, 648, 650, 651, 653, 654, 656, 657, 659, 660, 662, 663, 665, 667, 668, 670, 671, 673, 674, 676, 677, 679, 680, 682, 683, 685, 686, 688, 690, 691, 693, 694, 696, 697, 699, 700, 702, 703, 705, 706, 708, 709, 711, 713, 714, 716, 717, 719, 720, 722, 723, 725, 726, 728, 729, 731, 732, 734, 736, 737, 739, 740, 742, 743, 745, 746, 748, 749, 751, 752, 754, 755, 757, 759, 760, 762, 763, 765, 766, 768, 769, 771, 772, 774, 775, 777, 778, 780, 782, 783, 785, 786, 788, 789, 791, 792, 794, 795, 797, 798, 800, 801, 803, 805, 806, 808, 809, 811, 812, 814, 815, 817, 818, 820, 821, 823, 824, 826, 828, 829, 831, 832, 834, 835, 837, 838, 840, 841, 843, 844, 846, 847, 849, 851, 852, 854, 855, 857, 858, 860, 861, 863, 864, 866, 867, 869, 870, 872, 874, 875, 877, 878, 880, 881, }
{441, 450, 460, 469, 478, 487, 496, 506, 515, 524, 533, 542, 552, 561, 570, 579, 588, 598, 607, 616, 625, 634, 644, 653, 662, 671, 680, 690, 699, 708, 717, 726, 736, 745, 754, 763, 772, 782, 791, 800, 809, 818, 828, 837, 846, 855, 864, 874, }
{883, 884, 886, 887, 889, 890, 892, 893, 895, 897, 898, 900, 901, 903, 904, 906, 907, 909, 910, 912, 913, 915, 916, 918, 920, 921, 923, 924, 926, 927, 929, 930, 932, 933, 935, 936, 938, 939, 941, 943, 944, 946, 947, 949, 950, 952, 953, 955, 956, 958, 959, 961, 962, 964, 966, 967, 969, 970, 972, 973, 975, 976, 978, 979, 981, 982, 984, 985, 987, 989, 990, 992, 993, 995, 996, 998, 999, 1001, 1002, 1004, 1005, 1007, 1008, 1010, 1012, 1013, 1015, 1016, 1018, 1019, 1021, 1022, 1024, 1025, 1027, 1028, 1030, 1031, 1033, 1035, 1036, 1038, 1039, 1041, 1042, 1044, 1045, 1047, 1048, 1050, 1051, 1053, 1054, 1056, 1058, 1059, 1061, 1062, 1064, 1065, 1067, 1068, 1070, 1071, 1073, 1074, 1076, 1077, 1079, 1081, 1082, 1084, 1085, 1087, 1088, 1090, 1091, 1093, 1094, 1096, 1097, 1099, 1100, 1102, 1104, 1105, 1107, 1108, 1110, 1111, 1113, 1114, 1116, 1117, 1119, 1120, 1122, 1123, 1125, 1127, 1128, 1130, 1131, 1133, 1134, 1136, 1137, 1139, 1140, 1142, 1143, 1145, 1146, 1148, 1150, 1151, 1153, 1154, 1156, 1157, 1159, 1160, 1162, 1163, 1165, 1166, 1168, 1169, 1171, 1173, 1174, 1176, 1177, 1179, 1180, 1182, 1183, 1185, 1186, 1188, 1189, 1191, 1192, 1194, 1196, 1197, 1199, 1200, 1202, 1203, 1205, 1206, 1208, 1209, 1211, 1212, 1214, 1215, 1217, 1219, 1220, 1222, 1223, 1225, 1226, 1228, 1229, 1231, 1232, 1234, 1235, 1237, 1238, 1240, 1242, 1243, 1245, 1246, 1248, 1249, 1251, 1252, 1254, 1255, 1257, 1258, 1260, 1261, 1263, 1265, 1266, 1268, 1269, 1271, 1272, 1274, 1275, 1277, 1278, 1280, 1281, 1283, 1284, 1286, 1288, 1289, 1291, 1292, 1294, 1295, 1297, 1298, 1300, 1301, 1303, 1304, 1306, 1307, 1309, 1311, 1312, 1314, 1315, 1317, 1318, 1320, 1321, 1323, }
{883, 892, 901, 910, 920, 929, 938, 947, 956, 966, 975, 984, 993, 1002, 1012, 1021, 1030, 1039, 1048, 1058, 1067, 1076, 1085, 1094, 1104, 1113, 1122, 1131, 1140, 1150, 1159, 1168, 1177, 1186, 1196, 1205, 1214, 1223, 1232, 1242, 1251, 1260, 1269, 1278, 1288, 1297, 1306, 1315, }
{1324, 1326, 1327, 1329, 1330, 1332, 1334, 1335, 1337, 1338, 1340, 1341, 1343, 1344, 1346, 1347, 1349, 1350, 1352, 1353, 1355, 1357, 1358, 1360, 1361, 1363, 1364, 1366, 1367, 1369, 1370, 1372, 1373, 1375, 1376, 1378, 1380, 1381, 1383, 1384, 1386, 1387, 1389, 1390, 1392, 1393, 1395, 1396, 1398, 1399, 1401, 1403, 1404, 1406, 1407, 1409, 1410, 1412, 1413, 1415, 1416, 1418, 1419, 1421, 1422, 1424, 1426, 1427, 1429, 1430, 1432, 1433, 1435, 1436, 1438, 1439, 1441, 1442, 1444, 1445, 1447, 1449, 1450, 1452, 1453, 1455, 1456, 1458, 1459, 1461, 1462, 1464, 1465, 1467, 1468, 1470, }
{1324, 1327, 1330, 1334, 1337, 1340, 1343, 1346, 1349, 1352, 1355, 1358, 1361, 1364, 1367, 1370, 1373, 1376, 1380, 1383, 1386, 1389, 1392, 1395, 1398, 1401, 1404, 1407, 1410, 1413, 1416, 1419, 1422, 1426, 1429, 1432, 1435, 1438, 1441, 1444, 1447, 1450, 1453, 1456, 1459, 1462, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 55
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 55
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 55
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,96}
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,96}
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,96}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,96}
slicedInpSpikeIds.shapeToString(): {48,96}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,96}
slicedInpSpikeIds.shapeToString(): {48,96}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,96}
slicedInpSpikeIds.shapeToString(): {48,96}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {488,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{726, 725, 18, }
start_tile: 1
num_tiles_per_layer[ilay]: 726
start_tile + num_tiles_per_layer[ilay]: 727
num_occupied_tiles: 489
batchsize: 48
sparse_size: 96
dLdx.shapeToString(): {489,48,96}
replicated_numGrads.shapeToString(): {489,48}
exchangeGroupSize: 24
numExchangeGroups: 20
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
lastExchangeGroupLarger
start_id: 456, end_id: 489
exchangeGroupSizeLast: 33
exchangeBatchSizeLast: 2
start_tile: 727
num_tiles_per_layer[ilay]: 725
start_tile + num_tiles_per_layer[ilay]: 1452
num_occupied_tiles: 488
batchsize: 48
sparse_size: 96
dLdx.shapeToString(): {488,48,96}
replicated_numGrads.shapeToString(): {488,48}
exchangeGroupSize: 24
numExchangeGroups: 20
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
lastExchangeGroupLarger
start_id: 456, end_id: 488
exchangeGroupSizeLast: 32
exchangeBatchSizeLast: 2
start_tile: 1452
num_tiles_per_layer[ilay]: 18
start_tile + num_tiles_per_layer[ilay]: 1470
num_occupied_tiles: 5
batchsize: 48
sparse_size: 96
dLdx.shapeToString(): {5,48,96}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {488,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-13 00:01:46.879261: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[978 978 976  10]
0 489.0
1 489.0
2 488.0
3 5.0
[TileMapping(start_tile=1, end_tile=490), TileMapping(start_tile=490, end_tile=979), TileMapping(start_tile=979, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=490), TileMapping(start_tile=490, end_tile=979), TileMapping(start_tile=979, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.31330417999518295
limit=0.31330417999518295
limit=0.31362502409359
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 978) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 978) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 976) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_96_96_96_2_48_1_490_979_1467_490_979_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 96), (100 4190734     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][3
                                                                 keras_multi_lif_layer_sparse[0][7
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 4,190,734
Trainable params: 4,181,908
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_96_96_96_2_48_1_490_979_1467_490_979_1467_1472
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_96_96_96_2_48_1_490_979_1467_490_979_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.391752/52 [==============================] - 1161s 22s/step - loss: 2.3917
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.083552/52 [==============================] - 0s 9ms/step - loss: 2.0835
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 2.095752/52 [==============================] - 0s 8ms/step - loss: 2.0957
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 2.109152/52 [==============================] - 0s 9ms/step - loss: 2.1091
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 2.068252/52 [==============================] - 0s 8ms/step - loss: 2.0682
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 2.046252/52 [==============================] - 0s 9ms/step - loss: 2.0462
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 2.074052/52 [==============================] - 0s 9ms/step - loss: 2.0740
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 2.072752/52 [==============================] - 0s 9ms/step - loss: 2.0727
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 2.073152/52 [==============================] - 0s 9ms/step - loss: 2.0731
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 2.045252/52 [==============================] - 0s 9ms/step - loss: 2.0452
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 2.072752/52 [==============================] - 0s 9ms/step - loss: 2.0727
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 2.068352/52 [==============================] - 0s 9ms/step - loss: 2.0683
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 2.047252/52 [==============================] - 0s 9ms/step - loss: 2.0472
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 2.042352/52 [==============================] - 0s 9ms/step - loss: 2.0423
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 2.069752/52 [==============================] - 1s 14ms/step - loss: 2.0697
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 2.067752/52 [==============================] - 1s 15ms/step - loss: 2.0677
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 2.046252/52 [==============================] - 1s 11ms/step - loss: 2.0462
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 2.037352/52 [==============================] - 1s 11ms/step - loss: 2.0373
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 2.068252/52 [==============================] - 0s 9ms/step - loss: 2.0682
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 2.066952/52 [==============================] - 1s 11ms/step - loss: 2.0669
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 2.046252/52 [==============================] - 0s 10ms/step - loss: 2.0462
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 2.035452/52 [==============================] - 0s 9ms/step - loss: 2.0354
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 2.046452/52 [==============================] - 0s 9ms/step - loss: 2.0464
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 2.068852/52 [==============================] - 0s 9ms/step - loss: 2.0688
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 2.050252/52 [==============================] - 1s 10ms/step - loss: 2.0502

Final time:  1172.9404935836792
2022-09-13 00:02:02.909415: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.1
DENSE_SIZES:  [2312, 734, 734, 732, 732, 10]
SPARSE_SIZES:  [32, 72, 72, 72, 72, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  4
SECOND_THRESHOLD:  0.95

60000
9984
[58660 20273 54715 ... 26346 46287 24962]
2022-09-13 00:02:12.359494: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-13 00:02:13.244794: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 11, 2, 1
Build_allocator: 16, 3, 1
Build_allocator: 12, 2, 2
Build_allocator: 17, 3, 2
Build_allocator: 13, 2, 3
Build_allocator: 18, 3, 3
Build_allocator: 14, 2, 4
Build_allocator: 19, 3, 4
Build_allocator: 5, 1, 0
Build_allocator: 6, 1, 1
Build_allocator: 7, 1, 2
Build_allocator: 8, 1, 3
Build_allocator: 9, 1, 4
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 4, 0, 4
Build_allocator: 20, 4, 0
Build_allocator: 21, 4, 1
Build_allocator: 22, 4, 2
Build_allocator: 23, 4, 3
Build_allocator: 24, 4, 4
Build_allocator: 25, 5, 0
Build_allocator: 26, 5, 1
Build_allocator: 27, 5, 2
Build_allocator: 28, 5, 3
Build_allocator: 29, 5, 4
Build_allocator: 10, 2, 0
Build_allocator: 15, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0x208384c0
1: &init_state[ilay]0x208384c8
2: &init_state[ilay]0x208384d0
3: &init_state[ilay]0x208384d8
4: &init_state[ilay]0x208384e0
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0
3: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
3: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 368, 735, 1101, 1467, }
end_tiles: {368, 735, 1101, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{2, 2, 2, 2, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 5, 6, 8, 10, 11, 13, 15, 17, 18, 20, 22, 23, 25, 27, 28, 30, 32, 34, 35, 37, 39, 40, 42, 44, 46, 47, 49, 51, 52, 54, 56, 57, 59, 61, 63, 64, 66, 68, 69, 71, 73, 74, 76, 78, 80, 81, 83, 85, 86, 88, 90, 92, 93, 95, 97, 98, 100, 102, 103, 105, 107, 109, 110, 112, 114, 115, 117, 119, 120, 122, 124, 126, 127, 129, 131, 132, 134, 136, 138, 139, 141, 143, 144, 146, 148, 149, 151, 153, 155, 156, 158, 160, 161, 163, 165, 166, 168, 170, 172, 173, 175, 177, 178, 180, 182, 184, 185, 187, 189, 190, 192, 194, 195, 197, 199, 201, 202, 204, 206, 207, 209, 211, 212, 214, 216, 218, 219, 221, 223, 224, 226, 228, 230, 231, 233, 235, 236, 238, 240, 241, 243, 245, 247, 248, 250, 252, 253, 255, 257, 258, 260, 262, 264, 265, 267, 269, 270, 272, 274, 276, 277, 279, 281, 282, 284, 286, 287, 289, 291, 293, 294, 296, 298, 299, 301, 303, 304, 306, 308, 310, 311, 313, 315, 316, 318, 320, 322, 323, 325, }
{0, 6, 13, 20, 27, 34, 40, 47, 54, 61, 68, 74, 81, 88, 95, 102, 109, 115, 122, 129, 136, 143, 149, 156, 163, 170, 177, 184, 190, 197, 204, 211, 218, 224, 231, 238, 245, 252, 258, 265, 272, 279, 286, 293, 299, 306, 313, 320, }
{327, 328, 330, 332, 333, 335, 337, 339, 340, 342, 344, 345, 347, 349, 350, 352, 354, 356, 357, 359, 361, 362, 364, 366, 368, 369, 371, 373, 374, 376, 378, 379, 381, 383, 385, 386, 388, 390, 391, 393, 395, 396, 398, 400, 402, 403, 405, 407, 408, 410, 412, 414, 415, 417, 419, 420, 422, 424, 425, 427, 429, 431, 432, 434, 436, 437, 439, 441, 442, 444, 446, 448, 449, 451, 453, 454, 456, 458, 460, 461, 463, 465, 466, 468, 470, 471, 473, 475, 477, 478, 480, 482, 483, 485, 487, 488, 490, 492, 494, 495, 497, 499, 500, 502, 504, 506, 507, 509, 511, 512, 514, 516, 517, 519, 521, 523, 524, 526, 528, 529, 531, 533, 534, 536, 538, 540, 541, 543, 545, 546, 548, 550, 552, 553, 555, 557, 558, 560, 562, 563, 565, 567, 569, 570, 572, 574, 575, 577, 579, 580, 582, 584, 586, 587, 589, 591, 592, 594, 596, 598, 599, 601, 603, 604, 606, 608, 609, 611, 613, 615, 616, 618, 620, 621, 623, 625, 626, 628, 630, 632, 633, 635, 637, 638, 640, 642, 644, 645, 647, 649, 650, 652, }
{327, 333, 340, 347, 354, 361, 368, 374, 381, 388, 395, 402, 408, 415, 422, 429, 436, 442, 449, 456, 463, 470, 477, 483, 490, 497, 504, 511, 517, 524, 531, 538, 545, 552, 558, 565, 572, 579, 586, 592, 599, 606, 613, 620, 626, 633, 640, 647, }
{654, 655, 657, 659, 661, 662, 664, 666, 667, 669, 671, 672, 674, 676, 678, 679, 681, 683, 684, 686, 688, 690, 691, 693, 695, 696, 698, 700, 701, 703, 705, 707, 708, 710, 712, 713, 715, 717, 718, 720, 722, 724, 725, 727, 729, 730, 732, 734, 736, 737, 739, 741, 742, 744, 746, 747, 749, 751, 753, 754, 756, 758, 759, 761, 763, 764, 766, 768, 770, 771, 773, 775, 776, 778, 780, 782, 783, 785, 787, 788, 790, 792, 793, 795, 797, 799, 800, 802, 804, 805, 807, 809, 810, 812, 814, 816, 817, 819, 821, 822, 824, 826, 828, 829, 831, 833, 834, 836, 838, 839, 841, 843, 845, 846, 848, 850, 851, 853, 855, 856, 858, 860, 862, 863, 865, 867, 868, 870, 872, 874, 875, 877, 879, 880, 882, 884, 885, 887, 889, 891, 892, 894, 896, 897, 899, 901, 902, 904, 906, 908, 909, 911, 913, 914, 916, 918, 920, 921, 923, 925, 926, 928, 930, 931, 933, 935, 937, 938, 940, 942, 943, 945, 947, 948, 950, 952, 954, 955, 957, 959, 960, 962, 964, 966, 967, 969, 971, 972, 974, 976, 977, 979, }
{654, 661, 667, 674, 681, 688, 695, 701, 708, 715, 722, 729, 736, 742, 749, 756, 763, 770, 776, 783, 790, 797, 804, 810, 817, 824, 831, 838, 845, 851, 858, 865, 872, 879, 885, 892, 899, 906, 913, 920, 926, 933, 940, 947, 954, 960, 967, 974, }
{981, 983, 984, 986, 988, 989, 991, 993, 994, 996, 998, 1000, 1001, 1003, 1005, 1006, 1008, 1010, 1012, 1013, 1015, 1017, 1018, 1020, 1022, 1023, 1025, 1027, 1029, 1030, 1032, 1034, 1035, 1037, 1039, 1040, 1042, 1044, 1046, 1047, 1049, 1051, 1052, 1054, 1056, 1058, 1059, 1061, 1063, 1064, 1066, 1068, 1069, 1071, 1073, 1075, 1076, 1078, 1080, 1081, 1083, 1085, 1086, 1088, 1090, 1092, 1093, 1095, 1097, 1098, 1100, 1102, 1104, 1105, 1107, 1109, 1110, 1112, 1114, 1115, 1117, 1119, 1121, 1122, 1124, 1126, 1127, 1129, 1131, 1132, 1134, 1136, 1138, 1139, 1141, 1143, 1144, 1146, 1148, 1150, 1151, 1153, 1155, 1156, 1158, 1160, 1161, 1163, 1165, 1167, 1168, 1170, 1172, 1173, 1175, 1177, 1178, 1180, 1182, 1184, 1185, 1187, 1189, 1190, 1192, 1194, 1196, 1197, 1199, 1201, 1202, 1204, 1206, 1207, 1209, 1211, 1213, 1214, 1216, 1218, 1219, 1221, 1223, 1224, 1226, 1228, 1230, 1231, 1233, 1235, 1236, 1238, 1240, 1242, 1243, 1245, 1247, 1248, 1250, 1252, 1253, 1255, 1257, 1259, 1260, 1262, 1264, 1265, 1267, 1269, 1270, 1272, 1274, 1276, 1277, 1279, 1281, 1282, 1284, 1286, 1288, 1289, 1291, 1293, 1294, 1296, 1298, 1299, 1301, 1303, 1305, 1306, }
{981, 988, 994, 1001, 1008, 1015, 1022, 1029, 1035, 1042, 1049, 1056, 1063, 1069, 1076, 1083, 1090, 1097, 1104, 1110, 1117, 1124, 1131, 1138, 1144, 1151, 1158, 1165, 1172, 1178, 1185, 1192, 1199, 1206, 1213, 1219, 1226, 1233, 1240, 1247, 1253, 1260, 1267, 1274, 1281, 1288, 1294, 1301, }
{1308, 1310, 1311, 1313, 1315, 1316, 1318, 1320, 1322, 1323, 1325, 1327, 1328, 1330, 1332, 1334, 1335, 1337, 1339, 1340, 1342, 1344, 1345, 1347, 1349, 1351, 1352, 1354, 1356, 1357, 1359, 1361, 1362, 1364, 1366, 1368, 1369, 1371, 1373, 1374, 1376, 1378, 1380, 1381, 1383, 1385, 1386, 1388, 1390, 1391, 1393, 1395, 1397, 1398, 1400, 1402, 1403, 1405, 1407, 1408, 1410, 1412, 1414, 1415, 1417, 1419, 1420, 1422, 1424, 1426, 1427, 1429, 1431, 1432, 1434, 1436, 1437, 1439, 1441, 1443, 1444, 1446, 1448, 1449, 1451, 1453, 1454, 1456, 1458, 1460, 1461, 1463, 1465, 1466, 1468, 1470, }
{1308, 1311, 1315, 1318, 1322, 1325, 1328, 1332, 1335, 1339, 1342, 1345, 1349, 1352, 1356, 1359, 1362, 1366, 1369, 1373, 1376, 1380, 1383, 1386, 1390, 1393, 1397, 1400, 1403, 1407, 1410, 1414, 1417, 1420, 1424, 1427, 1431, 1434, 1437, 1441, 1444, 1448, 1451, 1454, 1458, 1461, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 62
sparse_size_to_use: 62
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 62
sparse_size_to_use: 62
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 61
sparse_size_to_use: 61
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 61
sparse_size_to_use: 61
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,72}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,72}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,72}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,72}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 4
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,72}
slicedInpSpikeIds.shapeToString(): {48,72}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,72}
slicedInpSpikeIds.shapeToString(): {48,72}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,72}
slicedInpSpikeIds.shapeToString(): {48,72}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

3
out_spike_ids[ilay].shapeToString(): {100,48,72}
slicedInpSpikeIds.shapeToString(): {48,72}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,2}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{485, 484, 484, 15, }
start_tile: 1
num_tiles_per_layer[ilay]: 485
start_tile + num_tiles_per_layer[ilay]: 486
num_occupied_tiles: 367
batchsize: 48
sparse_size: 72
dLdx.shapeToString(): {367,48,72}
replicated_numGrads.shapeToString(): {367,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 367
exchangeGroupSizeLast: 31
exchangeBatchSizeLast: 2
start_tile: 486
num_tiles_per_layer[ilay]: 484
start_tile + num_tiles_per_layer[ilay]: 970
num_occupied_tiles: 366
batchsize: 48
sparse_size: 72
dLdx.shapeToString(): {366,48,72}
replicated_numGrads.shapeToString(): {366,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 366
exchangeGroupSizeLast: 30
exchangeBatchSizeLast: 2
start_tile: 970
num_tiles_per_layer[ilay]: 484
start_tile + num_tiles_per_layer[ilay]: 1454
num_occupied_tiles: 366
batchsize: 48
sparse_size: 72
dLdx.shapeToString(): {366,48,72}
replicated_numGrads.shapeToString(): {366,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 366
exchangeGroupSizeLast: 30
exchangeBatchSizeLast: 2
start_tile: 1454
num_tiles_per_layer[ilay]: 15
start_tile + num_tiles_per_layer[ilay]: 1469
num_occupied_tiles: 5
batchsize: 48
sparse_size: 72
dLdx.shapeToString(): {5,48,72}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,1}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-13 00:12:32.689698: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[734 734 732 732  10]
0 367.0
1 367.0
2 366.0
3 366.0
4 5.0
[TileMapping(start_tile=1, end_tile=368), TileMapping(start_tile=368, end_tile=735), TileMapping(start_tile=735, end_tile=1101), TileMapping(start_tile=1101, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=368), TileMapping(start_tile=368, end_tile=735), TileMapping(start_tile=735, end_tile=1101), TileMapping(start_tile=1101, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.3616492648073473
limit=0.3616492648073473
limit=0.3621429841700741
limit=0.3621429841700741
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 734) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 734) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 732) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 732) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_5:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_72_72_72_72_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 72), (100 3325022     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][4
                                                                 keras_multi_lif_layer_sparse[0][9
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 3,325,022
Trainable params: 3,316,196
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_72_72_72_72_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_72_72_72_72_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.758752/52 [==============================] - 622s 12s/step - loss: 2.7587
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.157852/52 [==============================] - 0s 8ms/step - loss: 2.1578
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 2.154852/52 [==============================] - 0s 7ms/step - loss: 2.1548
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 2.079652/52 [==============================] - 0s 7ms/step - loss: 2.0796
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 2.061452/52 [==============================] - 0s 7ms/step - loss: 2.0614
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 2.054252/52 [==============================] - 0s 7ms/step - loss: 2.0542
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 2.123452/52 [==============================] - 0s 7ms/step - loss: 2.1234
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 2.036052/52 [==============================] - 0s 7ms/step - loss: 2.0360
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 1.975752/52 [==============================] - 0s 7ms/step - loss: 1.9757
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 2.073252/52 [==============================] - 0s 7ms/step - loss: 2.0732
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 2.146652/52 [==============================] - 0s 7ms/step - loss: 2.1466
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 2.139752/52 [==============================] - 0s 7ms/step - loss: 2.1397
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 2.090452/52 [==============================] - 0s 7ms/step - loss: 2.0904
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 2.035052/52 [==============================] - 0s 7ms/step - loss: 2.0350
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 1.915452/52 [==============================] - 0s 7ms/step - loss: 1.9154
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 1.795252/52 [==============================] - 0s 7ms/step - loss: 1.7952
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 1.790552/52 [==============================] - 0s 8ms/step - loss: 1.7905
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 1.834752/52 [==============================] - 0s 8ms/step - loss: 1.8347
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 1.675252/52 [==============================] - 0s 8ms/step - loss: 1.6752
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 1.909752/52 [==============================] - 0s 8ms/step - loss: 1.9097
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 1.790552/52 [==============================] - 0s 8ms/step - loss: 1.7905
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 1.823452/52 [==============================] - 0s 8ms/step - loss: 1.8234
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 1.778152/52 [==============================] - 0s 8ms/step - loss: 1.7781
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 1.842852/52 [==============================] - 0s 8ms/step - loss: 1.8428
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 2.019952/52 [==============================] - 0s 8ms/step - loss: 2.0199

Final time:  631.0315980911255
2022-09-13 00:12:45.620906: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.1
DENSE_SIZES:  [2312, 588, 586, 586, 586, 586, 10]
SPARSE_SIZES:  [32, 58, 58, 58, 58, 58, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  5
SECOND_THRESHOLD:  0.95

60000
9984
[37470 20636 51912 ... 57174 23385 39592]
2022-09-13 00:12:54.643486: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-13 00:12:55.568689: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 13, 2, 1
Build_allocator: 19, 3, 1
Build_allocator: 14, 2, 2
Build_allocator: 20, 3, 2
Build_allocator: 15, 2, 3
Build_allocator: 21, 3, 3
Build_allocator: 16, 2, 4
Build_allocator: 22, 3, 4
Build_allocator: 17, 2, 5
Build_allocator: 23, 3, 5
Build_allocator: 6, 1, 0
Build_allocator: 7, 1, 1
Build_allocator: 8, 1, 2
Build_allocator: 9, 1, 3
Build_allocator: 10, 1, 4
Build_allocator: 11, 1, 5
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 4, 0, 4
Build_allocator: 5, 0, 5
Build_allocator: 24, 4, 0
Build_allocator: 25, 4, 1
Build_allocator: 26, 4, 2
Build_allocator: 27, 4, 3
Build_allocator: 28, 4, 4
Build_allocator: 29, 4, 5
Build_allocator: 30, 5, 0
Build_allocator: 31, 5, 1
Build_allocator: 32, 5, 2
Build_allocator: 33, 5, 3
Build_allocator: 34, 5, 4
Build_allocator: 35, 5, 5
Build_allocator: 12, 2, 0
Build_allocator: 18, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0xe74f1d0
1: &init_state[ilay]0xe74f1d8
2: &init_state[ilay]0xe74f1e0
3: &init_state[ilay]0xe74f1e8
4: &init_state[ilay]0xe74f1f0
5: &init_state[ilay]0xe74f1f8
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0
3: init_state[ilay]==init_state[ilay+1]: 0
4: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
3: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
4: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 295, 588, 881, 1174, 1467, }
end_tiles: {295, 588, 881, 1174, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{2, 2, 2, 2, 2, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 2, 4, 5, 6, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 25, 26, 27, 29, 30, 32, 33, 34, 36, 37, 39, 40, 41, 43, 44, 46, 47, 48, 50, 51, 52, 54, 55, 57, 58, 59, 61, 62, 64, 65, 66, 68, 69, 71, 72, 73, 75, 76, 78, 79, 80, 82, 83, 85, 86, 87, 89, 90, 92, 93, 94, 96, 97, 98, 100, 101, 103, 104, 105, 107, 108, 110, 111, 112, 114, 115, 117, 118, 119, 121, 122, 124, 125, 126, 128, 129, 131, 132, 133, 135, 136, 138, 139, 140, 142, 143, 144, 146, 147, 149, 150, 151, 153, 154, 156, 157, 158, 160, 161, 163, 164, 165, 167, 168, 170, 171, 172, 174, 175, 177, 178, 179, 181, 182, 184, 185, 186, 188, 189, 190, 192, 193, 195, 196, 197, 199, 200, 202, 203, 204, 206, 207, 209, 210, 211, 213, 214, 216, 217, 218, 220, 221, 223, 224, 225, 227, 228, 230, 231, 232, 234, 235, 236, 238, 239, 241, 242, 243, 245, 246, 248, 249, 250, 252, 253, 255, 256, 257, 259, 260, 262, 263, 264, 266, }
{0, 5, 11, 16, 22, 27, 33, 39, 44, 50, 55, 61, 66, 72, 78, 83, 89, 94, 100, 105, 111, 117, 122, 128, 133, 139, 144, 150, 156, 161, 167, 172, 178, 184, 189, 195, 200, 206, 211, 217, 223, 228, 234, 239, 245, 250, 256, 262, }
{267, 269, 270, 271, 273, 274, 276, 277, 278, 280, 281, 282, 284, 285, 287, 288, 289, 291, 292, 294, 295, 296, 298, 299, 301, 302, 303, 305, 306, 308, 309, 310, 312, 313, 315, 316, 317, 319, 320, 322, 323, 324, 326, 327, 328, 330, 331, 333, 334, 335, 337, 338, 340, 341, 342, 344, 345, 347, 348, 349, 351, 352, 354, 355, 356, 358, 359, 361, 362, 363, 365, 366, 368, 369, 370, 372, 373, 374, 376, 377, 379, 380, 381, 383, 384, 386, 387, 388, 390, 391, 393, 394, 395, 397, 398, 400, 401, 402, 404, 405, 407, 408, 409, 411, 412, 414, 415, 416, 418, 419, 420, 422, 423, 425, 426, 427, 429, 430, 432, 433, 434, 436, 437, 439, 440, 441, 443, 444, 446, 447, 448, 450, 451, 453, 454, 455, 457, 458, 460, 461, 462, 464, 465, 466, 468, 469, 471, 472, 473, 475, 476, 478, 479, 480, 482, 483, 485, 486, 487, 489, 490, 492, 493, 494, 496, 497, 499, 500, 501, 503, 504, 506, 507, 508, 510, 511, 512, 514, 515, 517, 518, 519, 521, 522, 524, 525, 526, 528, 529, 531, 532, 533, }
{267, 273, 278, 284, 289, 295, 301, 306, 312, 317, 323, 328, 334, 340, 345, 351, 356, 362, 368, 373, 379, 384, 390, 395, 401, 407, 412, 418, 423, 429, 434, 440, 446, 451, 457, 462, 468, 473, 479, 485, 490, 496, 501, 507, 512, 518, 524, 529, }
{535, 536, 538, 539, 540, 542, 543, 545, 546, 547, 549, 550, 552, 553, 554, 556, 557, 558, 560, 561, 563, 564, 565, 567, 568, 570, 571, 572, 574, 575, 577, 578, 579, 581, 582, 584, 585, 586, 588, 589, 591, 592, 593, 595, 596, 598, 599, 600, 602, 603, 604, 606, 607, 609, 610, 611, 613, 614, 616, 617, 618, 620, 621, 623, 624, 625, 627, 628, 630, 631, 632, 634, 635, 637, 638, 639, 641, 642, 644, 645, 646, 648, 649, 650, 652, 653, 655, 656, 657, 659, 660, 662, 663, 664, 666, 667, 669, 670, 671, 673, 674, 676, 677, 678, 680, 681, 683, 684, 685, 687, 688, 690, 691, 692, 694, 695, 696, 698, 699, 701, 702, 703, 705, 706, 708, 709, 710, 712, 713, 715, 716, 717, 719, 720, 722, 723, 724, 726, 727, 729, 730, 731, 733, 734, 736, 737, 738, 740, 741, 742, 744, 745, 747, 748, 749, 751, 752, 754, 755, 756, 758, 759, 761, 762, 763, 765, 766, 768, 769, 770, 772, 773, 775, 776, 777, 779, 780, 782, 783, 784, 786, 787, 788, 790, 791, 793, 794, 795, 797, 798, 800, 801, }
{535, 540, 546, 552, 557, 563, 568, 574, 579, 585, 591, 596, 602, 607, 613, 618, 624, 630, 635, 641, 646, 652, 657, 663, 669, 674, 680, 685, 691, 696, 702, 708, 713, 719, 724, 730, 736, 741, 747, 752, 758, 763, 769, 775, 780, 786, 791, 797, }
{802, 804, 805, 807, 808, 809, 811, 812, 814, 815, 816, 818, 819, 821, 822, 823, 825, 826, 828, 829, 830, 832, 833, 834, 836, 837, 839, 840, 841, 843, 844, 846, 847, 848, 850, 851, 853, 854, 855, 857, 858, 860, 861, 862, 864, 865, 867, 868, 869, 871, 872, 874, 875, 876, 878, 879, 880, 882, 883, 885, 886, 887, 889, 890, 892, 893, 894, 896, 897, 899, 900, 901, 903, 904, 906, 907, 908, 910, 911, 913, 914, 915, 917, 918, 920, 921, 922, 924, 925, 926, 928, 929, 931, 932, 933, 935, 936, 938, 939, 940, 942, 943, 945, 946, 947, 949, 950, 952, 953, 954, 956, 957, 959, 960, 961, 963, 964, 966, 967, 968, 970, 971, 972, 974, 975, 977, 978, 979, 981, 982, 984, 985, 986, 988, 989, 991, 992, 993, 995, 996, 998, 999, 1000, 1002, 1003, 1005, 1006, 1007, 1009, 1010, 1012, 1013, 1014, 1016, 1017, 1018, 1020, 1021, 1023, 1024, 1025, 1027, 1028, 1030, 1031, 1032, 1034, 1035, 1037, 1038, 1039, 1041, 1042, 1044, 1045, 1046, 1048, 1049, 1051, 1052, 1053, 1055, 1056, 1058, 1059, 1060, 1062, 1063, 1064, 1066, 1067, 1069, }
{802, 808, 814, 819, 825, 830, 836, 841, 847, 853, 858, 864, 869, 875, 880, 886, 892, 897, 903, 908, 914, 920, 925, 931, 936, 942, 947, 953, 959, 964, 970, 975, 981, 986, 992, 998, 1003, 1009, 1014, 1020, 1025, 1031, 1037, 1042, 1048, 1053, 1059, 1064, }
{1070, 1071, 1073, 1074, 1076, 1077, 1078, 1080, 1081, 1083, 1084, 1085, 1087, 1088, 1090, 1091, 1092, 1094, 1095, 1097, 1098, 1099, 1101, 1102, 1104, 1105, 1106, 1108, 1109, 1110, 1112, 1113, 1115, 1116, 1117, 1119, 1120, 1122, 1123, 1124, 1126, 1127, 1129, 1130, 1131, 1133, 1134, 1136, 1137, 1138, 1140, 1141, 1143, 1144, 1145, 1147, 1148, 1150, 1151, 1152, 1154, 1155, 1156, 1158, 1159, 1161, 1162, 1163, 1165, 1166, 1168, 1169, 1170, 1172, 1173, 1175, 1176, 1177, 1179, 1180, 1182, 1183, 1184, 1186, 1187, 1189, 1190, 1191, 1193, 1194, 1196, 1197, 1198, 1200, 1201, 1202, 1204, 1205, 1207, 1208, 1209, 1211, 1212, 1214, 1215, 1216, 1218, 1219, 1221, 1222, 1223, 1225, 1226, 1228, 1229, 1230, 1232, 1233, 1235, 1236, 1237, 1239, 1240, 1242, 1243, 1244, 1246, 1247, 1248, 1250, 1251, 1253, 1254, 1255, 1257, 1258, 1260, 1261, 1262, 1264, 1265, 1267, 1268, 1269, 1271, 1272, 1274, 1275, 1276, 1278, 1279, 1281, 1282, 1283, 1285, 1286, 1288, 1289, 1290, 1292, 1293, 1294, 1296, 1297, 1299, 1300, 1301, 1303, 1304, 1306, 1307, 1308, 1310, 1311, 1313, 1314, 1315, 1317, 1318, 1320, 1321, 1322, 1324, 1325, 1327, 1328, 1329, 1331, 1332, 1334, 1335, 1336, }
{1070, 1076, 1081, 1087, 1092, 1098, 1104, 1109, 1115, 1120, 1126, 1131, 1137, 1143, 1148, 1154, 1159, 1165, 1170, 1176, 1182, 1187, 1193, 1198, 1204, 1209, 1215, 1221, 1226, 1232, 1237, 1243, 1248, 1254, 1260, 1265, 1271, 1276, 1282, 1288, 1293, 1299, 1304, 1310, 1315, 1321, 1327, 1332, }
{1338, 1339, 1340, 1342, 1343, 1345, 1346, 1347, 1349, 1350, 1352, 1353, 1354, 1356, 1357, 1359, 1360, 1361, 1363, 1364, 1366, 1367, 1368, 1370, 1371, 1373, 1374, 1375, 1377, 1378, 1380, 1381, 1382, 1384, 1385, 1386, 1388, 1389, 1391, 1392, 1393, 1395, 1396, 1398, 1399, 1400, 1402, 1403, 1405, 1406, 1407, 1409, 1410, 1412, 1413, 1414, 1416, 1417, 1419, 1420, 1421, 1423, 1424, 1426, 1427, 1428, 1430, 1431, 1432, 1434, 1435, 1437, 1438, 1439, 1441, 1442, 1444, 1445, 1446, 1448, 1449, 1451, 1452, 1453, 1455, 1456, 1458, 1459, 1460, 1462, 1463, 1465, 1466, 1467, 1469, 1470, }
{1338, 1340, 1343, 1346, 1349, 1352, 1354, 1357, 1360, 1363, 1366, 1368, 1371, 1374, 1377, 1380, 1382, 1385, 1388, 1391, 1393, 1396, 1399, 1402, 1405, 1407, 1410, 1413, 1416, 1419, 1421, 1424, 1427, 1430, 1432, 1435, 1438, 1441, 1444, 1446, 1449, 1452, 1455, 1458, 1460, 1463, 1466, 1469, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 49
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 49
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 49
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 49
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 49
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,58}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,58}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,58}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,58}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,58}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 4
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 5
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,58}
slicedInpSpikeIds.shapeToString(): {48,58}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,58}
slicedInpSpikeIds.shapeToString(): {48,58}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,58}
slicedInpSpikeIds.shapeToString(): {48,58}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

3
out_spike_ids[ilay].shapeToString(): {100,48,58}
slicedInpSpikeIds.shapeToString(): {48,58}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

4
out_spike_ids[ilay].shapeToString(): {100,48,58}
slicedInpSpikeIds.shapeToString(): {48,58}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {294,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 5
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{364, 364, 364, 364, 14, }
start_tile: 1
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 365
num_occupied_tiles: 293
batchsize: 48
sparse_size: 58
dLdx.shapeToString(): {293,48,58}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 365
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 729
num_occupied_tiles: 293
batchsize: 48
sparse_size: 58
dLdx.shapeToString(): {293,48,58}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 729
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 1093
num_occupied_tiles: 293
batchsize: 48
sparse_size: 58
dLdx.shapeToString(): {293,48,58}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 1093
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 1457
num_occupied_tiles: 293
batchsize: 48
sparse_size: 58
dLdx.shapeToString(): {293,48,58}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 1457
num_tiles_per_layer[ilay]: 14
start_tile + num_tiles_per_layer[ilay]: 1471
num_occupied_tiles: 5
batchsize: 48
sparse_size: 58
dLdx.shapeToString(): {5,48,58}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {294,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 5
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-13 00:23:12.947982: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[588 586 586 586 586  10]
0 294.0
1 293.0
2 293.0
3 293.0
4 293.0
5 5.0
[TileMapping(start_tile=1, end_tile=295), TileMapping(start_tile=295, end_tile=588), TileMapping(start_tile=588, end_tile=881), TileMapping(start_tile=881, end_tile=1174), TileMapping(start_tile=1174, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=295), TileMapping(start_tile=295, end_tile=588), TileMapping(start_tile=588, end_tile=881), TileMapping(start_tile=881, end_tile=1174), TileMapping(start_tile=1174, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.40406101782088427
limit=0.4047499544129309
limit=0.4047499544129309
limit=0.4047499544129309
limit=0.4047499544129309
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 588) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 586) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 586) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 586) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_5:0' shape=(2, 586) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.95, 0.95, 0.95, ..., 0.95, 0.95, 0.95]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_6:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_58_58_58_58_58_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 58), (100 2748898     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][5
                                                                 keras_multi_lif_layer_sparse[0][1
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 2,748,898
Trainable params: 2,740,072
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_58_58_58_58_58_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_58_58_58_58_58_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
52/52 [==============================] - ETA: 0s - loss: 3.231752/52 [==============================] - 619s 12s/step - loss: 3.2317
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.270952/52 [==============================] - 0s 7ms/step - loss: 2.2709
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 2.193652/52 [==============================] - 0s 6ms/step - loss: 2.1936
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 2.121752/52 [==============================] - 0s 6ms/step - loss: 2.1217
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 2.109452/52 [==============================] - 0s 6ms/step - loss: 2.1094
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 2.095752/52 [==============================] - 0s 6ms/step - loss: 2.0957
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 2.119452/52 [==============================] - 0s 6ms/step - loss: 2.1194
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 2.085552/52 [==============================] - 0s 6ms/step - loss: 2.0855
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 2.094952/52 [==============================] - 0s 7ms/step - loss: 2.0949
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 2.055652/52 [==============================] - 0s 7ms/step - loss: 2.0556
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 2.088252/52 [==============================] - 0s 7ms/step - loss: 2.0882
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 2.077452/52 [==============================] - 0s 7ms/step - loss: 2.0774
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 2.085052/52 [==============================] - 0s 7ms/step - loss: 2.0850
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 2.076352/52 [==============================] - 0s 7ms/step - loss: 2.0763
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 2.082852/52 [==============================] - 0s 7ms/step - loss: 2.0828
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 2.076052/52 [==============================] - 0s 7ms/step - loss: 2.0760
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 2.083452/52 [==============================] - 0s 7ms/step - loss: 2.0834
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 2.052252/52 [==============================] - 0s 7ms/step - loss: 2.0522
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 2.088452/52 [==============================] - 0s 7ms/step - loss: 2.0884
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 2.097352/52 [==============================] - 0s 7ms/step - loss: 2.0973
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 2.089052/52 [==============================] - 0s 7ms/step - loss: 2.0890
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 2.066352/52 [==============================] - 0s 7ms/step - loss: 2.0663
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 2.084552/52 [==============================] - 0s 7ms/step - loss: 2.0845
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 2.155752/52 [==============================] - 0s 7ms/step - loss: 2.1557
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 2.139352/52 [==============================] - 0s 7ms/step - loss: 2.1393

Final time:  627.8440027236938
2022-09-13 00:23:24.583584: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.1
DENSE_SIZES:  [2312, 1466, 1466, 10]
SPARSE_SIZES:  [32, 146, 146, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  2
SECOND_THRESHOLD:  0.98

60000
9984
[12367 54832  7810 ... 21768 34560  8290]
2022-09-13 00:23:32.898570: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-13 00:23:33.823390: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 7, 2, 1
Build_allocator: 10, 3, 1
Build_allocator: 8, 2, 2
Build_allocator: 11, 3, 2
Build_allocator: 3, 1, 0
Build_allocator: 4, 1, 1
Build_allocator: 5, 1, 2
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 12, 4, 0
Build_allocator: 13, 4, 1
Build_allocator: 14, 4, 2
Build_allocator: 15, 5, 0
Build_allocator: 16, 5, 1
Build_allocator: 17, 5, 2
Build_allocator: 6, 2, 0
Build_allocator: 9, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0xf7a9e20
1: &init_state[ilay]0xf7a9e28
2: &init_state[ilay]0xf7a9e30
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 734, 1467, }
end_tiles: {734, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{4, 4, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 5, 6, 8, 10, 11, 13, 15, 17, 18, 20, 22, 23, 25, 27, 28, 30, 32, 34, 35, 37, 39, 40, 42, 44, 46, 47, 49, 51, 52, 54, 56, 57, 59, 61, 63, 64, 66, 68, 69, 71, 73, 74, 76, 78, 80, 81, 83, 85, 86, 88, 90, 92, 93, 95, 97, 98, 100, 102, 103, 105, 107, 109, 110, 112, 114, 115, 117, 119, 120, 122, 124, 126, 127, 129, 131, 132, 134, 136, 138, 139, 141, 143, 144, 146, 148, 149, 151, 153, 155, 156, 158, 160, 161, 163, 165, 166, 168, 170, 172, 173, 175, 177, 178, 180, 182, 184, 185, 187, 189, 190, 192, 194, 195, 197, 199, 201, 202, 204, 206, 207, 209, 211, 212, 214, 216, 218, 219, 221, 223, 224, 226, 228, 230, 231, 233, 235, 236, 238, 240, 241, 243, 245, 247, 248, 250, 252, 253, 255, 257, 258, 260, 262, 264, 265, 267, 269, 270, 272, 274, 276, 277, 279, 281, 282, 284, 286, 287, 289, 291, 293, 294, 296, 298, 299, 301, 303, 304, 306, 308, 310, 311, 313, 315, 316, 318, 320, 322, 323, 325, 327, 328, 330, 332, 333, 335, 337, 339, 340, 342, 344, 345, 347, 349, 350, 352, 354, 356, 357, 359, 361, 362, 364, 366, 368, 369, 371, 373, 374, 376, 378, 379, 381, 383, 385, 386, 388, 390, 391, 393, 395, 396, 398, 400, 402, 403, 405, 407, 408, 410, 412, 414, 415, 417, 419, 420, 422, 424, 425, 427, 429, 431, 432, 434, 436, 437, 439, 441, 442, 444, 446, 448, 449, 451, 453, 454, 456, 458, 460, 461, 463, 465, 466, 468, 470, 471, 473, 475, 477, 478, 480, 482, 483, 485, 487, 488, 490, 492, 494, 495, 497, 499, 500, 502, 504, 506, 507, 509, 511, 512, 514, 516, 517, 519, 521, 523, 524, 526, 528, 529, 531, 533, 534, 536, 538, 540, 541, 543, 545, 546, 548, 550, 552, 553, 555, 557, 558, 560, 562, 563, 565, 567, 569, 570, 572, 574, 575, 577, 579, 580, 582, 584, 586, 587, 589, 591, 592, 594, 596, 598, 599, 601, 603, 604, 606, 608, 609, 611, 613, 615, 616, 618, 620, 621, 623, 625, 626, 628, 630, 632, 633, 635, 637, 638, 640, 642, 644, 645, 647, 649, 650, 652, }
{0, 13, 27, 40, 54, 68, 81, 95, 109, 122, 136, 149, 163, 177, 190, 204, 218, 231, 245, 258, 272, 286, 299, 313, 327, 340, 354, 368, 381, 395, 408, 422, 436, 449, 463, 477, 490, 504, 517, 531, 545, 558, 572, 586, 599, 613, 626, 640, }
{654, 655, 657, 659, 661, 662, 664, 666, 667, 669, 671, 672, 674, 676, 678, 679, 681, 683, 684, 686, 688, 690, 691, 693, 695, 696, 698, 700, 701, 703, 705, 707, 708, 710, 712, 713, 715, 717, 718, 720, 722, 724, 725, 727, 729, 730, 732, 734, 736, 737, 739, 741, 742, 744, 746, 747, 749, 751, 753, 754, 756, 758, 759, 761, 763, 764, 766, 768, 770, 771, 773, 775, 776, 778, 780, 782, 783, 785, 787, 788, 790, 792, 793, 795, 797, 799, 800, 802, 804, 805, 807, 809, 810, 812, 814, 816, 817, 819, 821, 822, 824, 826, 828, 829, 831, 833, 834, 836, 838, 839, 841, 843, 845, 846, 848, 850, 851, 853, 855, 856, 858, 860, 862, 863, 865, 867, 868, 870, 872, 874, 875, 877, 879, 880, 882, 884, 885, 887, 889, 891, 892, 894, 896, 897, 899, 901, 902, 904, 906, 908, 909, 911, 913, 914, 916, 918, 920, 921, 923, 925, 926, 928, 930, 931, 933, 935, 937, 938, 940, 942, 943, 945, 947, 948, 950, 952, 954, 955, 957, 959, 960, 962, 964, 966, 967, 969, 971, 972, 974, 976, 977, 979, 981, 983, 984, 986, 988, 989, 991, 993, 994, 996, 998, 1000, 1001, 1003, 1005, 1006, 1008, 1010, 1012, 1013, 1015, 1017, 1018, 1020, 1022, 1023, 1025, 1027, 1029, 1030, 1032, 1034, 1035, 1037, 1039, 1040, 1042, 1044, 1046, 1047, 1049, 1051, 1052, 1054, 1056, 1058, 1059, 1061, 1063, 1064, 1066, 1068, 1069, 1071, 1073, 1075, 1076, 1078, 1080, 1081, 1083, 1085, 1086, 1088, 1090, 1092, 1093, 1095, 1097, 1098, 1100, 1102, 1104, 1105, 1107, 1109, 1110, 1112, 1114, 1115, 1117, 1119, 1121, 1122, 1124, 1126, 1127, 1129, 1131, 1132, 1134, 1136, 1138, 1139, 1141, 1143, 1144, 1146, 1148, 1150, 1151, 1153, 1155, 1156, 1158, 1160, 1161, 1163, 1165, 1167, 1168, 1170, 1172, 1173, 1175, 1177, 1178, 1180, 1182, 1184, 1185, 1187, 1189, 1190, 1192, 1194, 1196, 1197, 1199, 1201, 1202, 1204, 1206, 1207, 1209, 1211, 1213, 1214, 1216, 1218, 1219, 1221, 1223, 1224, 1226, 1228, 1230, 1231, 1233, 1235, 1236, 1238, 1240, 1242, 1243, 1245, 1247, 1248, 1250, 1252, 1253, 1255, 1257, 1259, 1260, 1262, 1264, 1265, 1267, 1269, 1270, 1272, 1274, 1276, 1277, 1279, 1281, 1282, 1284, 1286, 1288, 1289, 1291, 1293, 1294, 1296, 1298, 1299, 1301, 1303, 1305, 1306, }
{654, 667, 681, 695, 708, 722, 736, 749, 763, 776, 790, 804, 817, 831, 845, 858, 872, 885, 899, 913, 926, 940, 954, 967, 981, 994, 1008, 1022, 1035, 1049, 1063, 1076, 1090, 1104, 1117, 1131, 1144, 1158, 1172, 1185, 1199, 1213, 1226, 1240, 1253, 1267, 1281, 1294, }
{1308, 1310, 1311, 1313, 1315, 1316, 1318, 1320, 1322, 1323, 1325, 1327, 1328, 1330, 1332, 1334, 1335, 1337, 1339, 1340, 1342, 1344, 1345, 1347, 1349, 1351, 1352, 1354, 1356, 1357, 1359, 1361, 1362, 1364, 1366, 1368, 1369, 1371, 1373, 1374, 1376, 1378, 1380, 1381, 1383, 1385, 1386, 1388, 1390, 1391, 1393, 1395, 1397, 1398, 1400, 1402, 1403, 1405, 1407, 1408, 1410, 1412, 1414, 1415, 1417, 1419, 1420, 1422, 1424, 1426, 1427, 1429, 1431, 1432, 1434, 1436, 1437, 1439, 1441, 1443, 1444, 1446, 1448, 1449, 1451, 1453, 1454, 1456, 1458, 1460, 1461, 1463, 1465, 1466, 1468, 1470, }
{1308, 1311, 1315, 1318, 1322, 1325, 1328, 1332, 1335, 1339, 1342, 1345, 1349, 1352, 1356, 1359, 1362, 1366, 1369, 1373, 1376, 1380, 1383, 1386, 1390, 1393, 1397, 1400, 1403, 1407, 1410, 1414, 1417, 1420, 1424, 1427, 1431, 1434, 1437, 1441, 1444, 1448, 1451, 1454, 1458, 1461, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 4
num_workers: 24
max_num_states_per_worker: 62
sparse_size_to_use: 62
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 4
num_workers: 24
max_num_states_per_worker: 62
sparse_size_to_use: 62
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,4,146}
 repeated_sparse_spike_ids.shapeToString(): {48,2,4,146}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,146}
slicedInpSpikeIds.shapeToString(): {48,146}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,146}
slicedInpSpikeIds.shapeToString(): {48,146}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{1444, 26, }
start_tile: 1
num_tiles_per_layer[ilay]: 1444
start_tile + num_tiles_per_layer[ilay]: 1445
num_occupied_tiles: 733
batchsize: 48
sparse_size: 146
dLdx.shapeToString(): {733,48,146}
replicated_numGrads.shapeToString(): {733,48}
exchangeGroupSize: 24
numExchangeGroups: 30
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
start_id: 456, end_id: 480
start_id: 480, end_id: 504
start_id: 504, end_id: 528
start_id: 528, end_id: 552
start_id: 552, end_id: 576
start_id: 576, end_id: 600
start_id: 600, end_id: 624
start_id: 624, end_id: 648
start_id: 648, end_id: 672
start_id: 672, end_id: 696
lastExchangeGroupLarger
start_id: 696, end_id: 733
exchangeGroupSizeLast: 37
exchangeBatchSizeLast: 2
start_tile: 1445
num_tiles_per_layer[ilay]: 26
start_tile + num_tiles_per_layer[ilay]: 1471
num_occupied_tiles: 5
batchsize: 48
sparse_size: 146
dLdx.shapeToString(): {5,48,146}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {733,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-13 00:39:33.622001: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[1466 1466   10]
0 733.0
1 733.0
2 5.0
[TileMapping(start_tile=1, end_tile=734), TileMapping(start_tile=734, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=734), TileMapping(start_tile=734, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.2558990251065398
limit=0.2558990251065398
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 1466) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 1466) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_146_146_2_48_1_734_1467_734_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 146), (10 5562034     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][2
                                                                 keras_multi_lif_layer_sparse[0][5
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 5,562,034
Trainable params: 5,553,208
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_146_146_2_48_1_734_1467_734_1467_1472
[1, 734, 1467]
[734, 1467, 1472]
2312_1466_1466_10_32_146_146_2_48_1_734_1467_734_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.403552/52 [==============================] - 963s 19s/step - loss: 2.4035
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 1.906452/52 [==============================] - 1s 11ms/step - loss: 1.9064
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 1.767652/52 [==============================] - 0s 10ms/step - loss: 1.7676
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 1.610452/52 [==============================] - 1s 10ms/step - loss: 1.6104
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 1.597352/52 [==============================] - 1s 10ms/step - loss: 1.5973
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 1.601152/52 [==============================] - 1s 10ms/step - loss: 1.6011
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 1.693052/52 [==============================] - 1s 10ms/step - loss: 1.6930
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 1.751952/52 [==============================] - 1s 11ms/step - loss: 1.7519
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 1.708252/52 [==============================] - 1s 11ms/step - loss: 1.7082
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 1.601952/52 [==============================] - 1s 11ms/step - loss: 1.6019
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 1.589452/52 [==============================] - 1s 11ms/step - loss: 1.5894
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 1.509152/52 [==============================] - 1s 10ms/step - loss: 1.5091
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 1.532052/52 [==============================] - 1s 11ms/step - loss: 1.5320
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 1.584752/52 [==============================] - 1s 11ms/step - loss: 1.5847
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 1.577952/52 [==============================] - 1s 11ms/step - loss: 1.5779
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 1.514852/52 [==============================] - 1s 11ms/step - loss: 1.5148
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 1.505152/52 [==============================] - 1s 11ms/step - loss: 1.5051
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 1.470952/52 [==============================] - 1s 11ms/step - loss: 1.4709
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 1.507152/52 [==============================] - 1s 11ms/step - loss: 1.5071
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 1.504352/52 [==============================] - 1s 11ms/step - loss: 1.5043
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 1.484852/52 [==============================] - 1s 11ms/step - loss: 1.4848
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 1.421252/52 [==============================] - 1s 11ms/step - loss: 1.4212
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 1.331352/52 [==============================] - 1s 11ms/step - loss: 1.3313
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 1.293452/52 [==============================] - 1s 11ms/step - loss: 1.2934
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 1.307752/52 [==============================] - 1s 11ms/step - loss: 1.3077

Final time:  975.965313911438
2022-09-13 00:39:51.067480: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.1
DENSE_SIZES:  [2312, 978, 978, 976, 10]
SPARSE_SIZES:  [32, 96, 96, 96, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  3
SECOND_THRESHOLD:  0.98

60000
9984
[23035 53139 11360 ... 47174 18260 42079]
2022-09-13 00:39:59.870963: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-13 00:40:00.733355: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 9, 2, 1
Build_allocator: 13, 3, 1
Build_allocator: 10, 2, 2
Build_allocator: 14, 3, 2
Build_allocator: 11, 2, 3
Build_allocator: 15, 3, 3
Build_allocator: 4, 1, 0
Build_allocator: 5, 1, 1
Build_allocator: 6, 1, 2
Build_allocator: 7, 1, 3
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 16, 4, 0
Build_allocator: 17, 4, 1
Build_allocator: 18, 4, 2
Build_allocator: 19, 4, 3
Build_allocator: 20, 5, 0
Build_allocator: 21, 5, 1
Build_allocator: 22, 5, 2
Build_allocator: 23, 5, 3
Build_allocator: 8, 2, 0
Build_allocator: 12, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0x247eaf20
1: &init_state[ilay]0x247eaf28
2: &init_state[ilay]0x247eaf30
3: &init_state[ilay]0x247eaf38
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 490, 979, 1467, }
end_tiles: {490, 979, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{3, 3, 3, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 4, 6, 7, 9, 10, 12, 13, 15, 16, 18, 19, 21, 23, 24, 26, 27, 29, 30, 32, 33, 35, 36, 38, 39, 41, 42, 44, 46, 47, 49, 50, 52, 53, 55, 56, 58, 59, 61, 62, 64, 65, 67, 69, 70, 72, 73, 75, 76, 78, 79, 81, 82, 84, 85, 87, 88, 90, 92, 93, 95, 96, 98, 99, 101, 102, 104, 105, 107, 108, 110, 111, 113, 115, 116, 118, 119, 121, 122, 124, 125, 127, 128, 130, 131, 133, 134, 136, 138, 139, 141, 142, 144, 145, 147, 148, 150, 151, 153, 154, 156, 157, 159, 161, 162, 164, 165, 167, 168, 170, 171, 173, 174, 176, 177, 179, 180, 182, 184, 185, 187, 188, 190, 191, 193, 194, 196, 197, 199, 200, 202, 203, 205, 207, 208, 210, 211, 213, 214, 216, 217, 219, 220, 222, 223, 225, 226, 228, 230, 231, 233, 234, 236, 237, 239, 240, 242, 243, 245, 246, 248, 249, 251, 253, 254, 256, 257, 259, 260, 262, 263, 265, 266, 268, 269, 271, 272, 274, 276, 277, 279, 280, 282, 283, 285, 286, 288, 289, 291, 292, 294, 295, 297, 299, 300, 302, 303, 305, 306, 308, 309, 311, 312, 314, 315, 317, 318, 320, 322, 323, 325, 326, 328, 329, 331, 332, 334, 335, 337, 338, 340, 341, 343, 345, 346, 348, 349, 351, 352, 354, 355, 357, 358, 360, 361, 363, 364, 366, 368, 369, 371, 372, 374, 375, 377, 378, 380, 381, 383, 384, 386, 387, 389, 391, 392, 394, 395, 397, 398, 400, 401, 403, 404, 406, 407, 409, 410, 412, 414, 415, 417, 418, 420, 421, 423, 424, 426, 427, 429, 430, 432, 433, 435, 437, 438, 440, }
{0, 9, 18, 27, 36, 46, 55, 64, 73, 82, 92, 101, 110, 119, 128, 138, 147, 156, 165, 174, 184, 193, 202, 211, 220, 230, 239, 248, 257, 266, 276, 285, 294, 303, 312, 322, 331, 340, 349, 358, 368, 377, 386, 395, 404, 414, 423, 432, }
{441, 443, 444, 446, 447, 449, 450, 452, 453, 455, 456, 458, 460, 461, 463, 464, 466, 467, 469, 470, 472, 473, 475, 476, 478, 479, 481, 483, 484, 486, 487, 489, 490, 492, 493, 495, 496, 498, 499, 501, 502, 504, 506, 507, 509, 510, 512, 513, 515, 516, 518, 519, 521, 522, 524, 525, 527, 529, 530, 532, 533, 535, 536, 538, 539, 541, 542, 544, 545, 547, 548, 550, 552, 553, 555, 556, 558, 559, 561, 562, 564, 565, 567, 568, 570, 571, 573, 575, 576, 578, 579, 581, 582, 584, 585, 587, 588, 590, 591, 593, 594, 596, 598, 599, 601, 602, 604, 605, 607, 608, 610, 611, 613, 614, 616, 617, 619, 621, 622, 624, 625, 627, 628, 630, 631, 633, 634, 636, 637, 639, 640, 642, 644, 645, 647, 648, 650, 651, 653, 654, 656, 657, 659, 660, 662, 663, 665, 667, 668, 670, 671, 673, 674, 676, 677, 679, 680, 682, 683, 685, 686, 688, 690, 691, 693, 694, 696, 697, 699, 700, 702, 703, 705, 706, 708, 709, 711, 713, 714, 716, 717, 719, 720, 722, 723, 725, 726, 728, 729, 731, 732, 734, 736, 737, 739, 740, 742, 743, 745, 746, 748, 749, 751, 752, 754, 755, 757, 759, 760, 762, 763, 765, 766, 768, 769, 771, 772, 774, 775, 777, 778, 780, 782, 783, 785, 786, 788, 789, 791, 792, 794, 795, 797, 798, 800, 801, 803, 805, 806, 808, 809, 811, 812, 814, 815, 817, 818, 820, 821, 823, 824, 826, 828, 829, 831, 832, 834, 835, 837, 838, 840, 841, 843, 844, 846, 847, 849, 851, 852, 854, 855, 857, 858, 860, 861, 863, 864, 866, 867, 869, 870, 872, 874, 875, 877, 878, 880, 881, }
{441, 450, 460, 469, 478, 487, 496, 506, 515, 524, 533, 542, 552, 561, 570, 579, 588, 598, 607, 616, 625, 634, 644, 653, 662, 671, 680, 690, 699, 708, 717, 726, 736, 745, 754, 763, 772, 782, 791, 800, 809, 818, 828, 837, 846, 855, 864, 874, }
{883, 884, 886, 887, 889, 890, 892, 893, 895, 897, 898, 900, 901, 903, 904, 906, 907, 909, 910, 912, 913, 915, 916, 918, 920, 921, 923, 924, 926, 927, 929, 930, 932, 933, 935, 936, 938, 939, 941, 943, 944, 946, 947, 949, 950, 952, 953, 955, 956, 958, 959, 961, 962, 964, 966, 967, 969, 970, 972, 973, 975, 976, 978, 979, 981, 982, 984, 985, 987, 989, 990, 992, 993, 995, 996, 998, 999, 1001, 1002, 1004, 1005, 1007, 1008, 1010, 1012, 1013, 1015, 1016, 1018, 1019, 1021, 1022, 1024, 1025, 1027, 1028, 1030, 1031, 1033, 1035, 1036, 1038, 1039, 1041, 1042, 1044, 1045, 1047, 1048, 1050, 1051, 1053, 1054, 1056, 1058, 1059, 1061, 1062, 1064, 1065, 1067, 1068, 1070, 1071, 1073, 1074, 1076, 1077, 1079, 1081, 1082, 1084, 1085, 1087, 1088, 1090, 1091, 1093, 1094, 1096, 1097, 1099, 1100, 1102, 1104, 1105, 1107, 1108, 1110, 1111, 1113, 1114, 1116, 1117, 1119, 1120, 1122, 1123, 1125, 1127, 1128, 1130, 1131, 1133, 1134, 1136, 1137, 1139, 1140, 1142, 1143, 1145, 1146, 1148, 1150, 1151, 1153, 1154, 1156, 1157, 1159, 1160, 1162, 1163, 1165, 1166, 1168, 1169, 1171, 1173, 1174, 1176, 1177, 1179, 1180, 1182, 1183, 1185, 1186, 1188, 1189, 1191, 1192, 1194, 1196, 1197, 1199, 1200, 1202, 1203, 1205, 1206, 1208, 1209, 1211, 1212, 1214, 1215, 1217, 1219, 1220, 1222, 1223, 1225, 1226, 1228, 1229, 1231, 1232, 1234, 1235, 1237, 1238, 1240, 1242, 1243, 1245, 1246, 1248, 1249, 1251, 1252, 1254, 1255, 1257, 1258, 1260, 1261, 1263, 1265, 1266, 1268, 1269, 1271, 1272, 1274, 1275, 1277, 1278, 1280, 1281, 1283, 1284, 1286, 1288, 1289, 1291, 1292, 1294, 1295, 1297, 1298, 1300, 1301, 1303, 1304, 1306, 1307, 1309, 1311, 1312, 1314, 1315, 1317, 1318, 1320, 1321, 1323, }
{883, 892, 901, 910, 920, 929, 938, 947, 956, 966, 975, 984, 993, 1002, 1012, 1021, 1030, 1039, 1048, 1058, 1067, 1076, 1085, 1094, 1104, 1113, 1122, 1131, 1140, 1150, 1159, 1168, 1177, 1186, 1196, 1205, 1214, 1223, 1232, 1242, 1251, 1260, 1269, 1278, 1288, 1297, 1306, 1315, }
{1324, 1326, 1327, 1329, 1330, 1332, 1334, 1335, 1337, 1338, 1340, 1341, 1343, 1344, 1346, 1347, 1349, 1350, 1352, 1353, 1355, 1357, 1358, 1360, 1361, 1363, 1364, 1366, 1367, 1369, 1370, 1372, 1373, 1375, 1376, 1378, 1380, 1381, 1383, 1384, 1386, 1387, 1389, 1390, 1392, 1393, 1395, 1396, 1398, 1399, 1401, 1403, 1404, 1406, 1407, 1409, 1410, 1412, 1413, 1415, 1416, 1418, 1419, 1421, 1422, 1424, 1426, 1427, 1429, 1430, 1432, 1433, 1435, 1436, 1438, 1439, 1441, 1442, 1444, 1445, 1447, 1449, 1450, 1452, 1453, 1455, 1456, 1458, 1459, 1461, 1462, 1464, 1465, 1467, 1468, 1470, }
{1324, 1327, 1330, 1334, 1337, 1340, 1343, 1346, 1349, 1352, 1355, 1358, 1361, 1364, 1367, 1370, 1373, 1376, 1380, 1383, 1386, 1389, 1392, 1395, 1398, 1401, 1404, 1407, 1410, 1413, 1416, 1419, 1422, 1426, 1429, 1432, 1435, 1438, 1441, 1444, 1447, 1450, 1453, 1456, 1459, 1462, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 55
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 55
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 3
num_workers: 18
max_num_states_per_worker: 55
sparse_size_to_use: 55
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 55
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54
numStatesThisWorker: 54

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,96}
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,96}
 repeated_sparse_spike_ids.shapeToString(): {48,2,3,96}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,96}
slicedInpSpikeIds.shapeToString(): {48,96}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,96}
slicedInpSpikeIds.shapeToString(): {48,96}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,96}
slicedInpSpikeIds.shapeToString(): {48,96}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {488,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{726, 725, 18, }
start_tile: 1
num_tiles_per_layer[ilay]: 726
start_tile + num_tiles_per_layer[ilay]: 727
num_occupied_tiles: 489
batchsize: 48
sparse_size: 96
dLdx.shapeToString(): {489,48,96}
replicated_numGrads.shapeToString(): {489,48}
exchangeGroupSize: 24
numExchangeGroups: 20
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
lastExchangeGroupLarger
start_id: 456, end_id: 489
exchangeGroupSizeLast: 33
exchangeBatchSizeLast: 2
start_tile: 727
num_tiles_per_layer[ilay]: 725
start_tile + num_tiles_per_layer[ilay]: 1452
num_occupied_tiles: 488
batchsize: 48
sparse_size: 96
dLdx.shapeToString(): {488,48,96}
replicated_numGrads.shapeToString(): {488,48}
exchangeGroupSize: 24
numExchangeGroups: 20
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
start_id: 336, end_id: 360
start_id: 360, end_id: 384
start_id: 384, end_id: 408
start_id: 408, end_id: 432
start_id: 432, end_id: 456
lastExchangeGroupLarger
start_id: 456, end_id: 488
exchangeGroupSizeLast: 32
exchangeBatchSizeLast: 2
start_tile: 1452
num_tiles_per_layer[ilay]: 18
start_tile + num_tiles_per_layer[ilay]: 1470
num_occupied_tiles: 5
batchsize: 48
sparse_size: 96
dLdx.shapeToString(): {5,48,96}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {489,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {488,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-13 00:59:52.838983: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[978 978 976  10]
0 489.0
1 489.0
2 488.0
3 5.0
[TileMapping(start_tile=1, end_tile=490), TileMapping(start_tile=490, end_tile=979), TileMapping(start_tile=979, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=490), TileMapping(start_tile=490, end_tile=979), TileMapping(start_tile=979, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.31330417999518295
limit=0.31330417999518295
limit=0.31362502409359
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 978) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 978) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 976) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_96_96_96_2_48_1_490_979_1467_490_979_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 96), (100 4190734     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][3
                                                                 keras_multi_lif_layer_sparse[0][7
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 4,190,734
Trainable params: 4,181,908
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_96_96_96_2_48_1_490_979_1467_490_979_1467_1472
[1, 490, 979, 1467]
[490, 979, 1467, 1472]
2312_978_978_976_10_32_96_96_96_2_48_1_490_979_1467_490_979_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.773452/52 [==============================] - 1194s 23s/step - loss: 2.7734
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.283352/52 [==============================] - 0s 9ms/step - loss: 2.2833
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 2.056852/52 [==============================] - 0s 8ms/step - loss: 2.0568
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 2.022252/52 [==============================] - 0s 8ms/step - loss: 2.0222
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 2.051852/52 [==============================] - 0s 8ms/step - loss: 2.0518
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 1.942352/52 [==============================] - 0s 8ms/step - loss: 1.9423
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 1.957952/52 [==============================] - 0s 9ms/step - loss: 1.9579
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 1.983452/52 [==============================] - 0s 9ms/step - loss: 1.9834
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 1.906052/52 [==============================] - 0s 9ms/step - loss: 1.9060
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 1.897452/52 [==============================] - 0s 9ms/step - loss: 1.8974
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 1.871552/52 [==============================] - 0s 9ms/step - loss: 1.8715
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 1.936852/52 [==============================] - 0s 9ms/step - loss: 1.9368
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 1.880052/52 [==============================] - 0s 9ms/step - loss: 1.8800
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 1.884652/52 [==============================] - 0s 9ms/step - loss: 1.8846
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 1.867852/52 [==============================] - 0s 9ms/step - loss: 1.8678
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 1.912352/52 [==============================] - 0s 9ms/step - loss: 1.9123
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 1.881052/52 [==============================] - 0s 9ms/step - loss: 1.8810
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 1.884452/52 [==============================] - 0s 9ms/step - loss: 1.8844
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 1.867352/52 [==============================] - 0s 9ms/step - loss: 1.8673
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 1.900852/52 [==============================] - 0s 9ms/step - loss: 1.9008
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 1.870252/52 [==============================] - 0s 9ms/step - loss: 1.8702
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 1.866752/52 [==============================] - 0s 9ms/step - loss: 1.8667
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 2.162952/52 [==============================] - 0s 9ms/step - loss: 2.1629
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 2.301652/52 [==============================] - 0s 9ms/step - loss: 2.3016
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 2.298352/52 [==============================] - 0s 9ms/step - loss: 2.2983

Final time:  1205.4492535591125
2022-09-13 01:00:07.498528: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.1
DENSE_SIZES:  [2312, 734, 734, 732, 732, 10]
SPARSE_SIZES:  [32, 72, 72, 72, 72, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  4
SECOND_THRESHOLD:  0.98

60000
9984
[15918 21592 13375 ...  4899 33964 31989]
2022-09-13 01:00:16.097018: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-13 01:00:17.038731: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 11, 2, 1
Build_allocator: 16, 3, 1
Build_allocator: 12, 2, 2
Build_allocator: 17, 3, 2
Build_allocator: 13, 2, 3
Build_allocator: 18, 3, 3
Build_allocator: 14, 2, 4
Build_allocator: 19, 3, 4
Build_allocator: 5, 1, 0
Build_allocator: 6, 1, 1
Build_allocator: 7, 1, 2
Build_allocator: 8, 1, 3
Build_allocator: 9, 1, 4
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 4, 0, 4
Build_allocator: 20, 4, 0
Build_allocator: 21, 4, 1
Build_allocator: 22, 4, 2
Build_allocator: 23, 4, 3
Build_allocator: 24, 4, 4
Build_allocator: 25, 5, 0
Build_allocator: 26, 5, 1
Build_allocator: 27, 5, 2
Build_allocator: 28, 5, 3
Build_allocator: 29, 5, 4
Build_allocator: 10, 2, 0
Build_allocator: 15, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0x241ca580
1: &init_state[ilay]0x241ca588
2: &init_state[ilay]0x241ca590
3: &init_state[ilay]0x241ca598
4: &init_state[ilay]0x241ca5a0
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0
3: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
3: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 368, 735, 1101, 1467, }
end_tiles: {368, 735, 1101, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{2, 2, 2, 2, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 3, 5, 6, 8, 10, 11, 13, 15, 17, 18, 20, 22, 23, 25, 27, 28, 30, 32, 34, 35, 37, 39, 40, 42, 44, 46, 47, 49, 51, 52, 54, 56, 57, 59, 61, 63, 64, 66, 68, 69, 71, 73, 74, 76, 78, 80, 81, 83, 85, 86, 88, 90, 92, 93, 95, 97, 98, 100, 102, 103, 105, 107, 109, 110, 112, 114, 115, 117, 119, 120, 122, 124, 126, 127, 129, 131, 132, 134, 136, 138, 139, 141, 143, 144, 146, 148, 149, 151, 153, 155, 156, 158, 160, 161, 163, 165, 166, 168, 170, 172, 173, 175, 177, 178, 180, 182, 184, 185, 187, 189, 190, 192, 194, 195, 197, 199, 201, 202, 204, 206, 207, 209, 211, 212, 214, 216, 218, 219, 221, 223, 224, 226, 228, 230, 231, 233, 235, 236, 238, 240, 241, 243, 245, 247, 248, 250, 252, 253, 255, 257, 258, 260, 262, 264, 265, 267, 269, 270, 272, 274, 276, 277, 279, 281, 282, 284, 286, 287, 289, 291, 293, 294, 296, 298, 299, 301, 303, 304, 306, 308, 310, 311, 313, 315, 316, 318, 320, 322, 323, 325, }
{0, 6, 13, 20, 27, 34, 40, 47, 54, 61, 68, 74, 81, 88, 95, 102, 109, 115, 122, 129, 136, 143, 149, 156, 163, 170, 177, 184, 190, 197, 204, 211, 218, 224, 231, 238, 245, 252, 258, 265, 272, 279, 286, 293, 299, 306, 313, 320, }
{327, 328, 330, 332, 333, 335, 337, 339, 340, 342, 344, 345, 347, 349, 350, 352, 354, 356, 357, 359, 361, 362, 364, 366, 368, 369, 371, 373, 374, 376, 378, 379, 381, 383, 385, 386, 388, 390, 391, 393, 395, 396, 398, 400, 402, 403, 405, 407, 408, 410, 412, 414, 415, 417, 419, 420, 422, 424, 425, 427, 429, 431, 432, 434, 436, 437, 439, 441, 442, 444, 446, 448, 449, 451, 453, 454, 456, 458, 460, 461, 463, 465, 466, 468, 470, 471, 473, 475, 477, 478, 480, 482, 483, 485, 487, 488, 490, 492, 494, 495, 497, 499, 500, 502, 504, 506, 507, 509, 511, 512, 514, 516, 517, 519, 521, 523, 524, 526, 528, 529, 531, 533, 534, 536, 538, 540, 541, 543, 545, 546, 548, 550, 552, 553, 555, 557, 558, 560, 562, 563, 565, 567, 569, 570, 572, 574, 575, 577, 579, 580, 582, 584, 586, 587, 589, 591, 592, 594, 596, 598, 599, 601, 603, 604, 606, 608, 609, 611, 613, 615, 616, 618, 620, 621, 623, 625, 626, 628, 630, 632, 633, 635, 637, 638, 640, 642, 644, 645, 647, 649, 650, 652, }
{327, 333, 340, 347, 354, 361, 368, 374, 381, 388, 395, 402, 408, 415, 422, 429, 436, 442, 449, 456, 463, 470, 477, 483, 490, 497, 504, 511, 517, 524, 531, 538, 545, 552, 558, 565, 572, 579, 586, 592, 599, 606, 613, 620, 626, 633, 640, 647, }
{654, 655, 657, 659, 661, 662, 664, 666, 667, 669, 671, 672, 674, 676, 678, 679, 681, 683, 684, 686, 688, 690, 691, 693, 695, 696, 698, 700, 701, 703, 705, 707, 708, 710, 712, 713, 715, 717, 718, 720, 722, 724, 725, 727, 729, 730, 732, 734, 736, 737, 739, 741, 742, 744, 746, 747, 749, 751, 753, 754, 756, 758, 759, 761, 763, 764, 766, 768, 770, 771, 773, 775, 776, 778, 780, 782, 783, 785, 787, 788, 790, 792, 793, 795, 797, 799, 800, 802, 804, 805, 807, 809, 810, 812, 814, 816, 817, 819, 821, 822, 824, 826, 828, 829, 831, 833, 834, 836, 838, 839, 841, 843, 845, 846, 848, 850, 851, 853, 855, 856, 858, 860, 862, 863, 865, 867, 868, 870, 872, 874, 875, 877, 879, 880, 882, 884, 885, 887, 889, 891, 892, 894, 896, 897, 899, 901, 902, 904, 906, 908, 909, 911, 913, 914, 916, 918, 920, 921, 923, 925, 926, 928, 930, 931, 933, 935, 937, 938, 940, 942, 943, 945, 947, 948, 950, 952, 954, 955, 957, 959, 960, 962, 964, 966, 967, 969, 971, 972, 974, 976, 977, 979, }
{654, 661, 667, 674, 681, 688, 695, 701, 708, 715, 722, 729, 736, 742, 749, 756, 763, 770, 776, 783, 790, 797, 804, 810, 817, 824, 831, 838, 845, 851, 858, 865, 872, 879, 885, 892, 899, 906, 913, 920, 926, 933, 940, 947, 954, 960, 967, 974, }
{981, 983, 984, 986, 988, 989, 991, 993, 994, 996, 998, 1000, 1001, 1003, 1005, 1006, 1008, 1010, 1012, 1013, 1015, 1017, 1018, 1020, 1022, 1023, 1025, 1027, 1029, 1030, 1032, 1034, 1035, 1037, 1039, 1040, 1042, 1044, 1046, 1047, 1049, 1051, 1052, 1054, 1056, 1058, 1059, 1061, 1063, 1064, 1066, 1068, 1069, 1071, 1073, 1075, 1076, 1078, 1080, 1081, 1083, 1085, 1086, 1088, 1090, 1092, 1093, 1095, 1097, 1098, 1100, 1102, 1104, 1105, 1107, 1109, 1110, 1112, 1114, 1115, 1117, 1119, 1121, 1122, 1124, 1126, 1127, 1129, 1131, 1132, 1134, 1136, 1138, 1139, 1141, 1143, 1144, 1146, 1148, 1150, 1151, 1153, 1155, 1156, 1158, 1160, 1161, 1163, 1165, 1167, 1168, 1170, 1172, 1173, 1175, 1177, 1178, 1180, 1182, 1184, 1185, 1187, 1189, 1190, 1192, 1194, 1196, 1197, 1199, 1201, 1202, 1204, 1206, 1207, 1209, 1211, 1213, 1214, 1216, 1218, 1219, 1221, 1223, 1224, 1226, 1228, 1230, 1231, 1233, 1235, 1236, 1238, 1240, 1242, 1243, 1245, 1247, 1248, 1250, 1252, 1253, 1255, 1257, 1259, 1260, 1262, 1264, 1265, 1267, 1269, 1270, 1272, 1274, 1276, 1277, 1279, 1281, 1282, 1284, 1286, 1288, 1289, 1291, 1293, 1294, 1296, 1298, 1299, 1301, 1303, 1305, 1306, }
{981, 988, 994, 1001, 1008, 1015, 1022, 1029, 1035, 1042, 1049, 1056, 1063, 1069, 1076, 1083, 1090, 1097, 1104, 1110, 1117, 1124, 1131, 1138, 1144, 1151, 1158, 1165, 1172, 1178, 1185, 1192, 1199, 1206, 1213, 1219, 1226, 1233, 1240, 1247, 1253, 1260, 1267, 1274, 1281, 1288, 1294, 1301, }
{1308, 1310, 1311, 1313, 1315, 1316, 1318, 1320, 1322, 1323, 1325, 1327, 1328, 1330, 1332, 1334, 1335, 1337, 1339, 1340, 1342, 1344, 1345, 1347, 1349, 1351, 1352, 1354, 1356, 1357, 1359, 1361, 1362, 1364, 1366, 1368, 1369, 1371, 1373, 1374, 1376, 1378, 1380, 1381, 1383, 1385, 1386, 1388, 1390, 1391, 1393, 1395, 1397, 1398, 1400, 1402, 1403, 1405, 1407, 1408, 1410, 1412, 1414, 1415, 1417, 1419, 1420, 1422, 1424, 1426, 1427, 1429, 1431, 1432, 1434, 1436, 1437, 1439, 1441, 1443, 1444, 1446, 1448, 1449, 1451, 1453, 1454, 1456, 1458, 1460, 1461, 1463, 1465, 1466, 1468, 1470, }
{1308, 1311, 1315, 1318, 1322, 1325, 1328, 1332, 1335, 1339, 1342, 1345, 1349, 1352, 1356, 1359, 1362, 1366, 1369, 1373, 1376, 1380, 1383, 1386, 1390, 1393, 1397, 1400, 1403, 1407, 1410, 1414, 1417, 1420, 1424, 1427, 1431, 1434, 1437, 1441, 1444, 1448, 1451, 1454, 1458, 1461, 1465, 1468, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 62
sparse_size_to_use: 62
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 62
sparse_size_to_use: 62
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 62
numStatesThisWorker: 62
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 61
sparse_size_to_use: 61
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 61
sparse_size_to_use: 61
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61
numStatesThisWorker: 61

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,72}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,72}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,72}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,72}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 4
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,72}
slicedInpSpikeIds.shapeToString(): {48,72}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,72}
slicedInpSpikeIds.shapeToString(): {48,72}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,72}
slicedInpSpikeIds.shapeToString(): {48,72}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

3
out_spike_ids[ilay].shapeToString(): {100,48,72}
slicedInpSpikeIds.shapeToString(): {48,72}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,2}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{485, 484, 484, 15, }
start_tile: 1
num_tiles_per_layer[ilay]: 485
start_tile + num_tiles_per_layer[ilay]: 486
num_occupied_tiles: 367
batchsize: 48
sparse_size: 72
dLdx.shapeToString(): {367,48,72}
replicated_numGrads.shapeToString(): {367,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 367
exchangeGroupSizeLast: 31
exchangeBatchSizeLast: 2
start_tile: 486
num_tiles_per_layer[ilay]: 484
start_tile + num_tiles_per_layer[ilay]: 970
num_occupied_tiles: 366
batchsize: 48
sparse_size: 72
dLdx.shapeToString(): {366,48,72}
replicated_numGrads.shapeToString(): {366,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 366
exchangeGroupSizeLast: 30
exchangeBatchSizeLast: 2
start_tile: 970
num_tiles_per_layer[ilay]: 484
start_tile + num_tiles_per_layer[ilay]: 1454
num_occupied_tiles: 366
batchsize: 48
sparse_size: 72
dLdx.shapeToString(): {366,48,72}
replicated_numGrads.shapeToString(): {366,48}
exchangeGroupSize: 24
numExchangeGroups: 15
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
start_id: 264, end_id: 288
start_id: 288, end_id: 312
start_id: 312, end_id: 336
lastExchangeGroupLarger
start_id: 336, end_id: 366
exchangeGroupSizeLast: 30
exchangeBatchSizeLast: 2
start_tile: 1454
num_tiles_per_layer[ilay]: 15
start_tile + num_tiles_per_layer[ilay]: 1469
num_occupied_tiles: 5
batchsize: 48
sparse_size: 72
dLdx.shapeToString(): {5,48,72}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {367,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {366,48,1}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-13 01:11:03.887044: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[734 734 732 732  10]
0 367.0
1 367.0
2 366.0
3 366.0
4 5.0
[TileMapping(start_tile=1, end_tile=368), TileMapping(start_tile=368, end_tile=735), TileMapping(start_tile=735, end_tile=1101), TileMapping(start_tile=1101, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=368), TileMapping(start_tile=368, end_tile=735), TileMapping(start_tile=735, end_tile=1101), TileMapping(start_tile=1101, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.3616492648073473
limit=0.3616492648073473
limit=0.3621429841700741
limit=0.3621429841700741
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 734) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 734) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 732) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 732) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_5:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_72_72_72_72_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 72), (100 3325022     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][4
                                                                 keras_multi_lif_layer_sparse[0][9
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 3,325,022
Trainable params: 3,316,196
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_72_72_72_72_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
[1, 368, 735, 1101, 1467]
[368, 735, 1101, 1467, 1472]
2312_734_734_732_732_10_32_72_72_72_72_2_48_1_368_735_1101_1467_368_735_1101_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.886752/52 [==============================] - 649s 12s/step - loss: 2.8867
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.306952/52 [==============================] - 0s 7ms/step - loss: 2.3069
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 2.306652/52 [==============================] - 0s 7ms/step - loss: 2.3066
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 2.304552/52 [==============================] - 0s 7ms/step - loss: 2.3045
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 2.290252/52 [==============================] - 0s 7ms/step - loss: 2.2902
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 2.221252/52 [==============================] - 0s 7ms/step - loss: 2.2212
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 2.294152/52 [==============================] - 0s 7ms/step - loss: 2.2941
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 2.286052/52 [==============================] - 0s 7ms/step - loss: 2.2860
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 2.150952/52 [==============================] - 0s 7ms/step - loss: 2.1509
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 2.122152/52 [==============================] - 0s 8ms/step - loss: 2.1221
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 2.089952/52 [==============================] - 0s 8ms/step - loss: 2.0899
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 2.093752/52 [==============================] - 0s 8ms/step - loss: 2.0937
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 2.109352/52 [==============================] - 0s 8ms/step - loss: 2.1093
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 2.100752/52 [==============================] - 0s 8ms/step - loss: 2.1007
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 2.067552/52 [==============================] - 0s 8ms/step - loss: 2.0675
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 2.062452/52 [==============================] - 0s 8ms/step - loss: 2.0624
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 2.125952/52 [==============================] - 0s 8ms/step - loss: 2.1259
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 2.104952/52 [==============================] - 0s 8ms/step - loss: 2.1049
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 2.065452/52 [==============================] - 0s 8ms/step - loss: 2.0654
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 2.067252/52 [==============================] - 0s 8ms/step - loss: 2.0672
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 2.084552/52 [==============================] - 0s 8ms/step - loss: 2.0845
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 2.112152/52 [==============================] - 0s 8ms/step - loss: 2.1121
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 2.135152/52 [==============================] - 0s 8ms/step - loss: 2.1351
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 2.116052/52 [==============================] - 0s 8ms/step - loss: 2.1160
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 2.109752/52 [==============================] - 0s 8ms/step - loss: 2.1097

Final time:  658.6899991035461
2022-09-13 01:11:16.745075: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 2.6.0 (e0ab3b4f12) Poplar package: a313c81b39
#################################################################################################
MAX_ACTIVITY:  0.1
DENSE_SIZES:  [2312, 588, 586, 586, 586, 586, 10]
SPARSE_SIZES:  [32, 58, 58, 58, 58, 58, 2]
BATCHSIZE:  48
NUM_SAMPLES_TRAIN:  9984
SEQ_LEN:  100

PROFILE_RUN:  False
USE_IPU:  True
IMPL_METHOD:  sparse_layer
TRANSPOSE_WEIGHTS:  True
LEARNING_RATE:  0.001
NUM_HIDDEN_LAYERS:  5
SECOND_THRESHOLD:  0.98

60000
9984
[41223 10300 47891 ... 43527  6252 25935]
2022-09-13 01:11:25.606201: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1618] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:530: FunctionalExtension.__init__ (from tensorflow.python.ipu.keras.extensions.functional_extensions) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:328: OutfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:329: InfeedManager.__init__ (from tensorflow.python.ipu.keras.extensions.keras_data_feed_manager) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
WARNING:tensorflow:From /nethome/janf/.local/lib/python3.8/site-packages/tensorflow/python/ipu/keras/extensions/keras_extension_base.py:1217: IPUDataHandler.__init__ (from tensorflow.python.ipu.keras.extensions.data_adapter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras` instead of `tf.python.keras`.
2022-09-13 01:11:26.507076: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Build_allocator: 13, 2, 1
Build_allocator: 19, 3, 1
Build_allocator: 14, 2, 2
Build_allocator: 20, 3, 2
Build_allocator: 15, 2, 3
Build_allocator: 21, 3, 3
Build_allocator: 16, 2, 4
Build_allocator: 22, 3, 4
Build_allocator: 17, 2, 5
Build_allocator: 23, 3, 5
Build_allocator: 6, 1, 0
Build_allocator: 7, 1, 1
Build_allocator: 8, 1, 2
Build_allocator: 9, 1, 3
Build_allocator: 10, 1, 4
Build_allocator: 11, 1, 5
Build_allocator: 0, 0, 0
Build_allocator: 1, 0, 1
Build_allocator: 2, 0, 2
Build_allocator: 3, 0, 3
Build_allocator: 4, 0, 4
Build_allocator: 5, 0, 5
Build_allocator: 24, 4, 0
Build_allocator: 25, 4, 1
Build_allocator: 26, 4, 2
Build_allocator: 27, 4, 3
Build_allocator: 28, 4, 4
Build_allocator: 29, 4, 5
Build_allocator: 30, 5, 0
Build_allocator: 31, 5, 1
Build_allocator: 32, 5, 2
Build_allocator: 33, 5, 3
Build_allocator: 34, 5, 4
Build_allocator: 35, 5, 5
Build_allocator: 12, 2, 0
Build_allocator: 18, 3, 0
RUN Build

tile_map_init_state0[0].size(): 0
tile_map_init_state0[1].size(): 48
tile_map_init_state1[0].size(): 0
tile_map_init_state1[1].size(): 0
0: &init_state[ilay]0xe23ebe0
1: &init_state[ilay]0xe23ebe8
2: &init_state[ilay]0xe23ebf0
3: &init_state[ilay]0xe23ebf8
4: &init_state[ilay]0xe23ec00
5: &init_state[ilay]0xe23ec08
0: init_state[ilay]==init_state[ilay+1]: 0
1: init_state[ilay]==init_state[ilay+1]: 0
2: init_state[ilay]==init_state[ilay+1]: 0
3: init_state[ilay]==init_state[ilay+1]: 0
4: init_state[ilay]==init_state[ilay+1]: 0

inp_spike_ids_fptype
0: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
1: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
2: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
3: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0
4: inp_spike_ids_fptype[ilay]==inp_spike_ids_fptype[ilay+1]: 0

custom_lif_multi_layer_vec_transpose NUM_TILES: 1472
start_tiles: {1, 295, 588, 881, 1174, 1467, }
end_tiles: {295, 588, 881, 1174, 1467, 1472, }
target.getTargetType(): IPU
target.getTargetSystemString(): UNKNOWN
DONE

START genBatchedLIFOutSpikes2ThreshsNeuronSpikes

DONE 00100
{2, 2, 2, 2, 2, 1, }

DONE determine_layerwise_tile_spread_factor
{0, 1, 2, 4, 5, 6, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 25, 26, 27, 29, 30, 32, 33, 34, 36, 37, 39, 40, 41, 43, 44, 46, 47, 48, 50, 51, 52, 54, 55, 57, 58, 59, 61, 62, 64, 65, 66, 68, 69, 71, 72, 73, 75, 76, 78, 79, 80, 82, 83, 85, 86, 87, 89, 90, 92, 93, 94, 96, 97, 98, 100, 101, 103, 104, 105, 107, 108, 110, 111, 112, 114, 115, 117, 118, 119, 121, 122, 124, 125, 126, 128, 129, 131, 132, 133, 135, 136, 138, 139, 140, 142, 143, 144, 146, 147, 149, 150, 151, 153, 154, 156, 157, 158, 160, 161, 163, 164, 165, 167, 168, 170, 171, 172, 174, 175, 177, 178, 179, 181, 182, 184, 185, 186, 188, 189, 190, 192, 193, 195, 196, 197, 199, 200, 202, 203, 204, 206, 207, 209, 210, 211, 213, 214, 216, 217, 218, 220, 221, 223, 224, 225, 227, 228, 230, 231, 232, 234, 235, 236, 238, 239, 241, 242, 243, 245, 246, 248, 249, 250, 252, 253, 255, 256, 257, 259, 260, 262, 263, 264, 266, }
{0, 5, 11, 16, 22, 27, 33, 39, 44, 50, 55, 61, 66, 72, 78, 83, 89, 94, 100, 105, 111, 117, 122, 128, 133, 139, 144, 150, 156, 161, 167, 172, 178, 184, 189, 195, 200, 206, 211, 217, 223, 228, 234, 239, 245, 250, 256, 262, }
{267, 269, 270, 271, 273, 274, 276, 277, 278, 280, 281, 282, 284, 285, 287, 288, 289, 291, 292, 294, 295, 296, 298, 299, 301, 302, 303, 305, 306, 308, 309, 310, 312, 313, 315, 316, 317, 319, 320, 322, 323, 324, 326, 327, 328, 330, 331, 333, 334, 335, 337, 338, 340, 341, 342, 344, 345, 347, 348, 349, 351, 352, 354, 355, 356, 358, 359, 361, 362, 363, 365, 366, 368, 369, 370, 372, 373, 374, 376, 377, 379, 380, 381, 383, 384, 386, 387, 388, 390, 391, 393, 394, 395, 397, 398, 400, 401, 402, 404, 405, 407, 408, 409, 411, 412, 414, 415, 416, 418, 419, 420, 422, 423, 425, 426, 427, 429, 430, 432, 433, 434, 436, 437, 439, 440, 441, 443, 444, 446, 447, 448, 450, 451, 453, 454, 455, 457, 458, 460, 461, 462, 464, 465, 466, 468, 469, 471, 472, 473, 475, 476, 478, 479, 480, 482, 483, 485, 486, 487, 489, 490, 492, 493, 494, 496, 497, 499, 500, 501, 503, 504, 506, 507, 508, 510, 511, 512, 514, 515, 517, 518, 519, 521, 522, 524, 525, 526, 528, 529, 531, 532, 533, }
{267, 273, 278, 284, 289, 295, 301, 306, 312, 317, 323, 328, 334, 340, 345, 351, 356, 362, 368, 373, 379, 384, 390, 395, 401, 407, 412, 418, 423, 429, 434, 440, 446, 451, 457, 462, 468, 473, 479, 485, 490, 496, 501, 507, 512, 518, 524, 529, }
{535, 536, 538, 539, 540, 542, 543, 545, 546, 547, 549, 550, 552, 553, 554, 556, 557, 558, 560, 561, 563, 564, 565, 567, 568, 570, 571, 572, 574, 575, 577, 578, 579, 581, 582, 584, 585, 586, 588, 589, 591, 592, 593, 595, 596, 598, 599, 600, 602, 603, 604, 606, 607, 609, 610, 611, 613, 614, 616, 617, 618, 620, 621, 623, 624, 625, 627, 628, 630, 631, 632, 634, 635, 637, 638, 639, 641, 642, 644, 645, 646, 648, 649, 650, 652, 653, 655, 656, 657, 659, 660, 662, 663, 664, 666, 667, 669, 670, 671, 673, 674, 676, 677, 678, 680, 681, 683, 684, 685, 687, 688, 690, 691, 692, 694, 695, 696, 698, 699, 701, 702, 703, 705, 706, 708, 709, 710, 712, 713, 715, 716, 717, 719, 720, 722, 723, 724, 726, 727, 729, 730, 731, 733, 734, 736, 737, 738, 740, 741, 742, 744, 745, 747, 748, 749, 751, 752, 754, 755, 756, 758, 759, 761, 762, 763, 765, 766, 768, 769, 770, 772, 773, 775, 776, 777, 779, 780, 782, 783, 784, 786, 787, 788, 790, 791, 793, 794, 795, 797, 798, 800, 801, }
{535, 540, 546, 552, 557, 563, 568, 574, 579, 585, 591, 596, 602, 607, 613, 618, 624, 630, 635, 641, 646, 652, 657, 663, 669, 674, 680, 685, 691, 696, 702, 708, 713, 719, 724, 730, 736, 741, 747, 752, 758, 763, 769, 775, 780, 786, 791, 797, }
{802, 804, 805, 807, 808, 809, 811, 812, 814, 815, 816, 818, 819, 821, 822, 823, 825, 826, 828, 829, 830, 832, 833, 834, 836, 837, 839, 840, 841, 843, 844, 846, 847, 848, 850, 851, 853, 854, 855, 857, 858, 860, 861, 862, 864, 865, 867, 868, 869, 871, 872, 874, 875, 876, 878, 879, 880, 882, 883, 885, 886, 887, 889, 890, 892, 893, 894, 896, 897, 899, 900, 901, 903, 904, 906, 907, 908, 910, 911, 913, 914, 915, 917, 918, 920, 921, 922, 924, 925, 926, 928, 929, 931, 932, 933, 935, 936, 938, 939, 940, 942, 943, 945, 946, 947, 949, 950, 952, 953, 954, 956, 957, 959, 960, 961, 963, 964, 966, 967, 968, 970, 971, 972, 974, 975, 977, 978, 979, 981, 982, 984, 985, 986, 988, 989, 991, 992, 993, 995, 996, 998, 999, 1000, 1002, 1003, 1005, 1006, 1007, 1009, 1010, 1012, 1013, 1014, 1016, 1017, 1018, 1020, 1021, 1023, 1024, 1025, 1027, 1028, 1030, 1031, 1032, 1034, 1035, 1037, 1038, 1039, 1041, 1042, 1044, 1045, 1046, 1048, 1049, 1051, 1052, 1053, 1055, 1056, 1058, 1059, 1060, 1062, 1063, 1064, 1066, 1067, 1069, }
{802, 808, 814, 819, 825, 830, 836, 841, 847, 853, 858, 864, 869, 875, 880, 886, 892, 897, 903, 908, 914, 920, 925, 931, 936, 942, 947, 953, 959, 964, 970, 975, 981, 986, 992, 998, 1003, 1009, 1014, 1020, 1025, 1031, 1037, 1042, 1048, 1053, 1059, 1064, }
{1070, 1071, 1073, 1074, 1076, 1077, 1078, 1080, 1081, 1083, 1084, 1085, 1087, 1088, 1090, 1091, 1092, 1094, 1095, 1097, 1098, 1099, 1101, 1102, 1104, 1105, 1106, 1108, 1109, 1110, 1112, 1113, 1115, 1116, 1117, 1119, 1120, 1122, 1123, 1124, 1126, 1127, 1129, 1130, 1131, 1133, 1134, 1136, 1137, 1138, 1140, 1141, 1143, 1144, 1145, 1147, 1148, 1150, 1151, 1152, 1154, 1155, 1156, 1158, 1159, 1161, 1162, 1163, 1165, 1166, 1168, 1169, 1170, 1172, 1173, 1175, 1176, 1177, 1179, 1180, 1182, 1183, 1184, 1186, 1187, 1189, 1190, 1191, 1193, 1194, 1196, 1197, 1198, 1200, 1201, 1202, 1204, 1205, 1207, 1208, 1209, 1211, 1212, 1214, 1215, 1216, 1218, 1219, 1221, 1222, 1223, 1225, 1226, 1228, 1229, 1230, 1232, 1233, 1235, 1236, 1237, 1239, 1240, 1242, 1243, 1244, 1246, 1247, 1248, 1250, 1251, 1253, 1254, 1255, 1257, 1258, 1260, 1261, 1262, 1264, 1265, 1267, 1268, 1269, 1271, 1272, 1274, 1275, 1276, 1278, 1279, 1281, 1282, 1283, 1285, 1286, 1288, 1289, 1290, 1292, 1293, 1294, 1296, 1297, 1299, 1300, 1301, 1303, 1304, 1306, 1307, 1308, 1310, 1311, 1313, 1314, 1315, 1317, 1318, 1320, 1321, 1322, 1324, 1325, 1327, 1328, 1329, 1331, 1332, 1334, 1335, 1336, }
{1070, 1076, 1081, 1087, 1092, 1098, 1104, 1109, 1115, 1120, 1126, 1131, 1137, 1143, 1148, 1154, 1159, 1165, 1170, 1176, 1182, 1187, 1193, 1198, 1204, 1209, 1215, 1221, 1226, 1232, 1237, 1243, 1248, 1254, 1260, 1265, 1271, 1276, 1282, 1288, 1293, 1299, 1304, 1310, 1315, 1321, 1327, 1332, }
{1338, 1339, 1340, 1342, 1343, 1345, 1346, 1347, 1349, 1350, 1352, 1353, 1354, 1356, 1357, 1359, 1360, 1361, 1363, 1364, 1366, 1367, 1368, 1370, 1371, 1373, 1374, 1375, 1377, 1378, 1380, 1381, 1382, 1384, 1385, 1386, 1388, 1389, 1391, 1392, 1393, 1395, 1396, 1398, 1399, 1400, 1402, 1403, 1405, 1406, 1407, 1409, 1410, 1412, 1413, 1414, 1416, 1417, 1419, 1420, 1421, 1423, 1424, 1426, 1427, 1428, 1430, 1431, 1432, 1434, 1435, 1437, 1438, 1439, 1441, 1442, 1444, 1445, 1446, 1448, 1449, 1451, 1452, 1453, 1455, 1456, 1458, 1459, 1460, 1462, 1463, 1465, 1466, 1467, 1469, 1470, }
{1338, 1340, 1343, 1346, 1349, 1352, 1354, 1357, 1360, 1363, 1366, 1368, 1371, 1374, 1377, 1380, 1382, 1385, 1388, 1391, 1393, 1396, 1399, 1402, 1405, 1407, 1410, 1413, 1416, 1419, 1421, 1424, 1427, 1430, 1432, 1435, 1438, 1441, 1444, 1446, 1449, 1452, 1455, 1458, 1460, 1463, 1466, 1469, }

DONE determine_tile_mapping

DONE setup

DONE gen_dense_spikes

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 49
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 49
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 49
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 49
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 2
num_workers: 12
max_num_states_per_worker: 49
sparse_size_to_use: 49
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 49
numStatesThisWorker: 48
numStatesThisWorker: 48

dense_to_sparse_spikes_multi_tile_spread
num_tile_spread_fac: 1
num_workers: 6
max_num_states_per_worker: 2
sparse_size_to_use: 2
sparseSmallerStatesPerWorker: 0
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 2
numStatesThisWorker: 1
numStatesThisWorker: 1

DONE dense_to_sparse_spikes
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,58}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,58}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,58}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,58}
 repeated_sparse_spike_ids.shapeToString(): {48,2,2,58}
 repeated_sparse_spike_ids.shapeToString(): {48,2,1,2}

DONE combine_repeated_sparse_spikes_multi_thresh

DONE FORWARD

xparse2dense NUM_TILES: 1472
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}
num_out_spikes_int[i].spike_ids.shapeToString(): {100,48,2}

ilay: 0
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 1
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 2
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 3
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 4
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

ilay: 5
init_state[ilay][0].isContiguous().slice(0, 2, 0): 1
dLdstate[ilay][0].isContiguous().slice(0, 2, 0): 1

0
out_spike_ids[ilay].shapeToString(): {100,48,58}
slicedInpSpikeIds.shapeToString(): {48,58}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

1
out_spike_ids[ilay].shapeToString(): {100,48,58}
slicedInpSpikeIds.shapeToString(): {48,58}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

2
out_spike_ids[ilay].shapeToString(): {100,48,58}
slicedInpSpikeIds.shapeToString(): {48,58}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

3
out_spike_ids[ilay].shapeToString(): {100,48,58}
slicedInpSpikeIds.shapeToString(): {48,58}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

4
out_spike_ids[ilay].shapeToString(): {100,48,58}
slicedInpSpikeIds.shapeToString(): {48,58}
num_out_spikes[ilay].shapeToString(): {100,48,2}
slicedNumInpSpikes.shapeToString(): {48,2}

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {294,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,2}
ilay: 5
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,2}
{364, 364, 364, 364, 14, }
start_tile: 1
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 365
num_occupied_tiles: 293
batchsize: 48
sparse_size: 58
dLdx.shapeToString(): {293,48,58}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 365
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 729
num_occupied_tiles: 293
batchsize: 48
sparse_size: 58
dLdx.shapeToString(): {293,48,58}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 729
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 1093
num_occupied_tiles: 293
batchsize: 48
sparse_size: 58
dLdx.shapeToString(): {293,48,58}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 1093
num_tiles_per_layer[ilay]: 364
start_tile + num_tiles_per_layer[ilay]: 1457
num_occupied_tiles: 293
batchsize: 48
sparse_size: 58
dLdx.shapeToString(): {293,48,58}
replicated_numGrads.shapeToString(): {293,48}
exchangeGroupSize: 24
numExchangeGroups: 12
exchangeBatchSize: 2
lastExchangeGroupLarger: 1
start_id: 0, end_id: 24
start_id: 24, end_id: 48
start_id: 48, end_id: 72
start_id: 72, end_id: 96
start_id: 96, end_id: 120
start_id: 120, end_id: 144
start_id: 144, end_id: 168
start_id: 168, end_id: 192
start_id: 192, end_id: 216
start_id: 216, end_id: 240
start_id: 240, end_id: 264
lastExchangeGroupLarger
start_id: 264, end_id: 293
exchangeGroupSizeLast: 29
exchangeBatchSizeLast: 2
start_tile: 1457
num_tiles_per_layer[ilay]: 14
start_tile + num_tiles_per_layer[ilay]: 1471
num_occupied_tiles: 5
batchsize: 48
sparse_size: 58
dLdx.shapeToString(): {5,48,58}
replicated_numGrads.shapeToString(): {5,48}
exchangeGroupSize: 5
numExchangeGroups: 1
exchangeBatchSize: 10
lastExchangeGroupLarger: 0
start_id: 0, end_id: 5

get_relative_layer_id_on_ipu

num_ipus1

tile_map_dLdState[0].size(): 0
tile_map_dLdState[1].size(): 48
tile_map_iter_dLdState[0].size(): 0
tile_map_iter_dLdState[1].size(): 48
ilay: 0
fwdInpSpikes[ilay].num_spikes.shapeToString(): {294,48,1}
ilay: 1
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 2
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 3
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 4
fwdInpSpikes[ilay].num_spikes.shapeToString(): {293,48,1}
ilay: 5
fwdInpSpikes[ilay].num_spikes.shapeToString(): {5,48,1}
2022-09-13 01:21:37.041368: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
0

neurons_per_tile=2
num_neurons=[588 586 586 586 586  10]
0 294.0
1 293.0
2 293.0
3 293.0
4 293.0
5 5.0
[TileMapping(start_tile=1, end_tile=295), TileMapping(start_tile=295, end_tile=588), TileMapping(start_tile=588, end_tile=881), TileMapping(start_tile=881, end_tile=1174), TileMapping(start_tile=1174, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)]
tile_mapping_found=True
ListWrapper([TileMapping(start_tile=1, end_tile=295), TileMapping(start_tile=295, end_tile=588), TileMapping(start_tile=588, end_tile=881), TileMapping(start_tile=881, end_tile=1174), TileMapping(start_tile=1174, end_tile=1467), TileMapping(start_tile=1467, end_tile=1472)])
limit=0.2037706832433973
limit=0.40406101782088427
limit=0.4047499544129309
limit=0.4047499544129309
limit=0.4047499544129309
limit=0.4047499544129309
ListWrapper([<tf.Variable 'keras_multi_lif_layer_sparse/thresholds_1:0' shape=(2, 588) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_2:0' shape=(2, 586) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_3:0' shape=(2, 586) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_4:0' shape=(2, 586) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_5:0' shape=(2, 586) dtype=float32, numpy=
array([[1.  , 1.  , 1.  , ..., 1.  , 1.  , 1.  ],
       [0.98, 0.98, 0.98, ..., 0.98, 0.98, 0.98]], dtype=float32)>, <tf.Variable 'keras_multi_lif_layer_sparse/thresholds_6:0' shape=(2, 10) dtype=float32, numpy=
array([[   1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,    1.,
           1.],
       [-100., -100., -100., -100., -100., -100., -100., -100., -100.,
        -100.]], dtype=float32)>])
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_58_58_58_58_58_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
inp_spike_ids (InputLayer)      [(48, 100, 32)]      0                                            
__________________________________________________________________________________________________
num_inp_spikes (InputLayer)     [(48, 100, 1)]       0                                            
__________________________________________________________________________________________________
tf.compat.v1.transpose (TFOpLam (100, 48, 32)        0           inp_spike_ids[0][0]              
__________________________________________________________________________________________________
tf.compat.v1.transpose_1 (TFOpL (100, 48, 1)         0           num_inp_spikes[0][0]             
__________________________________________________________________________________________________
keras_multi_lif_layer_sparse (K [(100, 48, 58), (100 2748898     tf.compat.v1.transpose[0][0]     
                                                                 tf.compat.v1.transpose_1[0][0]   
__________________________________________________________________________________________________
tf.ipu_user_op (TFOpLambda)     [(100, 48, 10)]      0           keras_multi_lif_layer_sparse[0][5
                                                                 keras_multi_lif_layer_sparse[0][1
__________________________________________________________________________________________________
tf.compat.v1.transpose_2 (TFOpL (48, 100, 10)        0           tf.ipu_user_op[0][0]             
==================================================================================================
Total params: 2,748,898
Trainable params: 2,740,072
Non-trainable params: 8,826
__________________________________________________________________________________________________

Training
Epoch 1/25
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_58_58_58_58_58_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
[1, 295, 588, 881, 1174, 1467]
[295, 588, 881, 1174, 1467, 1472]
2312_588_586_586_586_586_10_32_58_58_58_58_58_2_48_1_295_588_881_1174_1467_295_588_881_1174_1467_1472
52/52 [==============================] - ETA: 0s - loss: 2.976252/52 [==============================] - 613s 12s/step - loss: 2.9762
Epoch 2/25
52/52 [==============================] - ETA: 0s - loss: 2.186652/52 [==============================] - 0s 7ms/step - loss: 2.1866
Epoch 3/25
52/52 [==============================] - ETA: 0s - loss: 2.157952/52 [==============================] - 0s 7ms/step - loss: 2.1579
Epoch 4/25
52/52 [==============================] - ETA: 0s - loss: 2.149952/52 [==============================] - 0s 7ms/step - loss: 2.1499
Epoch 5/25
52/52 [==============================] - ETA: 0s - loss: 2.117552/52 [==============================] - 0s 7ms/step - loss: 2.1175
Epoch 6/25
52/52 [==============================] - ETA: 0s - loss: 2.094852/52 [==============================] - 0s 7ms/step - loss: 2.0948
Epoch 7/25
52/52 [==============================] - ETA: 0s - loss: 2.111152/52 [==============================] - 0s 7ms/step - loss: 2.1111
Epoch 8/25
52/52 [==============================] - ETA: 0s - loss: 2.125952/52 [==============================] - 0s 7ms/step - loss: 2.1259
Epoch 9/25
52/52 [==============================] - ETA: 0s - loss: 2.095352/52 [==============================] - 0s 7ms/step - loss: 2.0953
Epoch 10/25
52/52 [==============================] - ETA: 0s - loss: 2.081852/52 [==============================] - 0s 7ms/step - loss: 2.0818
Epoch 11/25
52/52 [==============================] - ETA: 0s - loss: 2.094252/52 [==============================] - 0s 7ms/step - loss: 2.0942
Epoch 12/25
52/52 [==============================] - ETA: 0s - loss: 2.108952/52 [==============================] - 0s 7ms/step - loss: 2.1089
Epoch 13/25
52/52 [==============================] - ETA: 0s - loss: 2.120952/52 [==============================] - 0s 7ms/step - loss: 2.1209
Epoch 14/25
52/52 [==============================] - ETA: 0s - loss: 2.081652/52 [==============================] - 0s 7ms/step - loss: 2.0816
Epoch 15/25
52/52 [==============================] - ETA: 0s - loss: 2.087552/52 [==============================] - 0s 7ms/step - loss: 2.0875
Epoch 16/25
52/52 [==============================] - ETA: 0s - loss: 2.100552/52 [==============================] - 0s 7ms/step - loss: 2.1005
Epoch 17/25
52/52 [==============================] - ETA: 0s - loss: 2.107852/52 [==============================] - 0s 7ms/step - loss: 2.1078
Epoch 18/25
52/52 [==============================] - ETA: 0s - loss: 2.078352/52 [==============================] - 0s 7ms/step - loss: 2.0783
Epoch 19/25
52/52 [==============================] - ETA: 0s - loss: 2.096652/52 [==============================] - 0s 7ms/step - loss: 2.0966
Epoch 20/25
52/52 [==============================] - ETA: 0s - loss: 2.104952/52 [==============================] - 0s 7ms/step - loss: 2.1049
Epoch 21/25
52/52 [==============================] - ETA: 0s - loss: 2.119152/52 [==============================] - 0s 7ms/step - loss: 2.1191
Epoch 22/25
52/52 [==============================] - ETA: 0s - loss: 2.156652/52 [==============================] - 0s 7ms/step - loss: 2.1566
Epoch 23/25
52/52 [==============================] - ETA: 0s - loss: 2.092952/52 [==============================] - 0s 7ms/step - loss: 2.0929
Epoch 24/25
52/52 [==============================] - ETA: 0s - loss: 2.106552/52 [==============================] - 0s 7ms/step - loss: 2.1065
Epoch 25/25
52/52 [==============================] - ETA: 0s - loss: 2.093152/52 [==============================] - 0s 7ms/step - loss: 2.0931

Final time:  621.3862721920013
